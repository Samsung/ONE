<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "https://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en-US">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=11"/>
<meta name="generator" content="Doxygen 1.9.5"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<title>ONE - On-device Neural Engine: compute/cker/include/cker/operation/LSTM.h Source File</title>
<link href="tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="dynsections.js"></script>
<link href="navtree.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="resize.js"></script>
<script type="text/javascript" src="navtreedata.js"></script>
<script type="text/javascript" src="navtree.js"></script>
<link href="search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="search/searchdata.js"></script>
<script type="text/javascript" src="search/search.js"></script>
<link href="doxygen.css" rel="stylesheet" type="text/css" />
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr id="projectrow">
  <td id="projectalign">
   <div id="projectname">ONE - On-device Neural Engine
   </div>
  </td>
 </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.9.5 -->
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:d3d9a9a6595521f9666a5e94cc830dab83b65699&amp;dn=expat.txt MIT */
var searchBox = new SearchBox("searchBox", "search/",'.html');
/* @license-end */
</script>
<script type="text/javascript" src="menudata.js"></script>
<script type="text/javascript" src="menu.js"></script>
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:d3d9a9a6595521f9666a5e94cc830dab83b65699&amp;dn=expat.txt MIT */
$(function() {
  initMenu('',true,false,'search.php','Search');
  $(document).ready(function() { init_search(); });
});
/* @license-end */
</script>
<div id="main-nav"></div>
</div><!-- top -->
<div id="side-nav" class="ui-resizable side-nav-resizable">
  <div id="nav-tree">
    <div id="nav-tree-contents">
      <div id="nav-sync" class="sync"></div>
    </div>
  </div>
  <div id="splitbar" style="-moz-user-select:none;" 
       class="ui-resizable-handle">
  </div>
</div>
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:d3d9a9a6595521f9666a5e94cc830dab83b65699&amp;dn=expat.txt MIT */
$(document).ready(function(){initNavTree('compute_2cker_2include_2cker_2operation_2_l_s_t_m_8h_source.html',''); initResizable(); });
/* @license-end */
</script>
<div id="doc-content">
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<div id="MSearchResults">
<div class="SRPage">
<div id="SRIndex">
<div id="SRResults"></div>
<div class="SRStatus" id="Loading">Loading...</div>
<div class="SRStatus" id="Searching">Searching...</div>
<div class="SRStatus" id="NoMatches">No Matches</div>
</div>
</div>
</div>
</div>

<div class="header">
  <div class="headertitle"><div class="title">LSTM.h</div></div>
</div><!--header-->
<div class="contents">
<a href="compute_2cker_2include_2cker_2operation_2_l_s_t_m_8h.html">Go to the documentation of this file.</a><div class="fragment"><div class="line"><a id="l00001" name="l00001"></a><span class="lineno">    1</span><span class="comment">/*</span></div>
<div class="line"><a id="l00002" name="l00002"></a><span class="lineno">    2</span><span class="comment"> * Copyright (c) 2020 Samsung Electronics Co., Ltd. All Rights Reserved</span></div>
<div class="line"><a id="l00003" name="l00003"></a><span class="lineno">    3</span><span class="comment"> * Copyright 2018 The TensorFlow Authors. All Rights Reserved.</span></div>
<div class="line"><a id="l00004" name="l00004"></a><span class="lineno">    4</span><span class="comment"> *</span></div>
<div class="line"><a id="l00005" name="l00005"></a><span class="lineno">    5</span><span class="comment"> * Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);</span></div>
<div class="line"><a id="l00006" name="l00006"></a><span class="lineno">    6</span><span class="comment"> * you may not use this file except in compliance with the License.</span></div>
<div class="line"><a id="l00007" name="l00007"></a><span class="lineno">    7</span><span class="comment"> * You may obtain a copy of the License at</span></div>
<div class="line"><a id="l00008" name="l00008"></a><span class="lineno">    8</span><span class="comment"> *</span></div>
<div class="line"><a id="l00009" name="l00009"></a><span class="lineno">    9</span><span class="comment"> *      http://www.apache.org/licenses/LICENSE-2.0</span></div>
<div class="line"><a id="l00010" name="l00010"></a><span class="lineno">   10</span><span class="comment"> *</span></div>
<div class="line"><a id="l00011" name="l00011"></a><span class="lineno">   11</span><span class="comment"> * Unless required by applicable law or agreed to in writing, software</span></div>
<div class="line"><a id="l00012" name="l00012"></a><span class="lineno">   12</span><span class="comment"> * distributed under the License is distributed on an &quot;AS IS&quot; BASIS,</span></div>
<div class="line"><a id="l00013" name="l00013"></a><span class="lineno">   13</span><span class="comment"> * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.</span></div>
<div class="line"><a id="l00014" name="l00014"></a><span class="lineno">   14</span><span class="comment"> * See the License for the specific language governing permissions and</span></div>
<div class="line"><a id="l00015" name="l00015"></a><span class="lineno">   15</span><span class="comment"> * limitations under the License.</span></div>
<div class="line"><a id="l00016" name="l00016"></a><span class="lineno">   16</span><span class="comment"> */</span></div>
<div class="line"><a id="l00017" name="l00017"></a><span class="lineno">   17</span> </div>
<div class="line"><a id="l00018" name="l00018"></a><span class="lineno">   18</span><span class="preprocessor">#ifndef __NNFW_CKER_UNIDIRECTIONALSEQUENCELSTM_H__</span></div>
<div class="line"><a id="l00019" name="l00019"></a><span class="lineno">   19</span><span class="preprocessor">#define __NNFW_CKER_UNIDIRECTIONALSEQUENCELSTM_H__</span></div>
<div class="line"><a id="l00020" name="l00020"></a><span class="lineno">   20</span> </div>
<div class="line"><a id="l00021" name="l00021"></a><span class="lineno">   21</span><span class="preprocessor">#include &quot;<a class="code" href="compute_2cker_2include_2cker_2_tensor_utils_8h.html">cker/TensorUtils.h</a>&quot;</span></div>
<div class="line"><a id="l00022" name="l00022"></a><span class="lineno">   22</span><span class="preprocessor">#include &quot;<a class="code" href="compute_2cker_2include_2cker_2_types_8h.html">cker/Types.h</a>&quot;</span></div>
<div class="line"><a id="l00023" name="l00023"></a><span class="lineno">   23</span> </div>
<div class="line"><a id="l00024" name="l00024"></a><span class="lineno">   24</span><span class="keyword">namespace </span><a class="code hl_namespace" href="namespacennfw.html">nnfw</a></div>
<div class="line"><a id="l00025" name="l00025"></a><span class="lineno">   25</span>{</div>
<div class="line"><a id="l00026" name="l00026"></a><span class="lineno">   26</span><span class="keyword">namespace </span>cker</div>
<div class="line"><a id="l00027" name="l00027"></a><span class="lineno">   27</span>{</div>
<div class="line"><a id="l00028" name="l00028"></a><span class="lineno">   28</span> </div>
<div class="line"><a id="l00029" name="l00029"></a><span class="lineno">   29</span><span class="comment">// LINT.IfChange</span></div>
<div class="line"><a id="l00030" name="l00030"></a><span class="lineno">   30</span><span class="comment">// Calculates a single LSTM gate.</span></div>
<div class="line"><a id="l00031" name="l00031"></a><span class="lineno">   31</span><span class="comment">//</span></div>
<div class="line"><a id="l00032" name="l00032"></a><span class="lineno">   32</span><span class="comment">// Implements the following formula: (* is matrix multiply)</span></div>
<div class="line"><a id="l00033" name="l00033"></a><span class="lineno">   33</span><span class="comment">//   gate = activate(W_input    * input + W_aux       * aux_input   +</span></div>
<div class="line"><a id="l00034" name="l00034"></a><span class="lineno">   34</span><span class="comment">//                   W_peephole * cell  + W_recurrent * prev_output + bias)</span></div>
<div class="line"><a id="l00035" name="l00035"></a><span class="lineno">   35</span><span class="comment">// with layer norm:</span></div>
<div class="line"><a id="l00036" name="l00036"></a><span class="lineno">   36</span><span class="comment">//   gate = activate(W_norm * normalize(...) + bias) // not adding bias inside</span></div>
<div class="line"><a id="l00037" name="l00037"></a><span class="lineno">   37</span><span class="comment">//</span></div>
<div class="line"><a id="l00038" name="l00038"></a><span class="lineno">   38</span><span class="comment">// Activation is sigmoid except for the &quot;cell&quot; gate (configurable, usually tanh)</span></div>
<div class="line"><a id="l00039" name="l00039"></a><span class="lineno">   39</span><span class="comment">//</span></div>
<div class="line"><a id="l00040" name="l00040"></a><span class="lineno">   40</span><span class="comment">// Parameters:</span></div>
<div class="line"><a id="l00041" name="l00041"></a><span class="lineno">   41</span><span class="comment">// Input vectors (to LSTM):    | Size:                | Optional?</span></div>
<div class="line"><a id="l00042" name="l00042"></a><span class="lineno">   42</span><span class="comment">//   input                     | n_input              |</span></div>
<div class="line"><a id="l00043" name="l00043"></a><span class="lineno">   43</span><span class="comment">//   aux_input                 | n_aux_input          | y (bidir LSTM)</span></div>
<div class="line"><a id="l00044" name="l00044"></a><span class="lineno">   44</span><span class="comment">// Input vectors (persistent states):</span></div>
<div class="line"><a id="l00045" name="l00045"></a><span class="lineno">   45</span><span class="comment">//   output_state              | n_output             |</span></div>
<div class="line"><a id="l00046" name="l00046"></a><span class="lineno">   46</span><span class="comment">//   cell_state                | n_cell               |</span></div>
<div class="line"><a id="l00047" name="l00047"></a><span class="lineno">   47</span><span class="comment">// &#39;Constant&#39; inputs:</span></div>
<div class="line"><a id="l00048" name="l00048"></a><span class="lineno">   48</span><span class="comment">//   input_to_gate_weights     | n_cell * n_input     |</span></div>
<div class="line"><a id="l00049" name="l00049"></a><span class="lineno">   49</span><span class="comment">//   aux_input_to_gate_weights | n_cell * n_aux_input | y (bidir LSTM)</span></div>
<div class="line"><a id="l00050" name="l00050"></a><span class="lineno">   50</span><span class="comment">//   recurrent_to_gate_weights | n_cell * n_output    |</span></div>
<div class="line"><a id="l00051" name="l00051"></a><span class="lineno">   51</span><span class="comment">//   cell_to_gate_weights      | n_cell               | y (peephole)</span></div>
<div class="line"><a id="l00052" name="l00052"></a><span class="lineno">   52</span><span class="comment">//   gate_bias                 | n_cell               |</span></div>
<div class="line"><a id="l00053" name="l00053"></a><span class="lineno">   53</span><span class="comment">//   layer_norm_coefficients   | n_cell               | y (layer norm)</span></div>
<div class="line"><a id="l00054" name="l00054"></a><span class="lineno">   54</span><span class="comment">// Output vector:</span></div>
<div class="line"><a id="l00055" name="l00055"></a><span class="lineno">   55</span><span class="comment">//   gate                      | n_cell               |</span></div>
<div class="line"><a id="l00056" name="l00056"></a><span class="lineno">   56</span><span class="comment">// Scalar parameters:</span></div>
<div class="line"><a id="l00057" name="l00057"></a><span class="lineno">   57</span><span class="comment">//   n_batch                                    - batch size / number of vectors</span></div>
<div class="line"><a id="l00058" name="l00058"></a><span class="lineno">   58</span><span class="comment">//   n_input, n_aux_input, n_output, n_cell     - size of vectors.</span></div>
<div class="line"><a id="l00059" name="l00059"></a><span class="lineno">   59</span><span class="comment">//   activation                                 - activation to use.</span></div>
<div class="line"><a id="l00060" name="l00060"></a><span class="lineno">   60</span><span class="comment">//   is_input_all_zeros, is_aux_input_all_zeros - if input vectors are all zero.</span></div>
<div class="line"><a id="l00061" name="l00061"></a><span class="lineno">   61</span><span class="comment">//   use_layer_norm                             - if doing layer norm LSTM.</span></div>
<div class="line"><a id="l00062" name="l00062"></a><span class="lineno"><a class="line" href="namespacennfw_1_1cker.html#ae60dea0f6844a22a3ed03e20d8a00e78">   62</a></span><span class="keyword">inline</span> <span class="keywordtype">void</span> <a class="code hl_function" href="namespacennfw_1_1cker.html#ae60dea0f6844a22a3ed03e20d8a00e78">CalculateLstmGateFloat</a>(<span class="keyword">const</span> <span class="keywordtype">float</span> *input, <span class="keyword">const</span> <span class="keywordtype">float</span> *input_to_gate_weights,</div>
<div class="line"><a id="l00063" name="l00063"></a><span class="lineno">   63</span>                                   <span class="keyword">const</span> <span class="keywordtype">float</span> *aux_input, <span class="keyword">const</span> <span class="keywordtype">float</span> *aux_input_to_gate_weights,</div>
<div class="line"><a id="l00064" name="l00064"></a><span class="lineno">   64</span>                                   <span class="keyword">const</span> <span class="keywordtype">float</span> *output_state,</div>
<div class="line"><a id="l00065" name="l00065"></a><span class="lineno">   65</span>                                   <span class="keyword">const</span> <span class="keywordtype">float</span> *recurrent_to_gate_weights, <span class="keyword">const</span> <span class="keywordtype">float</span> *cell_state,</div>
<div class="line"><a id="l00066" name="l00066"></a><span class="lineno">   66</span>                                   <span class="keyword">const</span> <span class="keywordtype">float</span> *cell_to_gate_weights,</div>
<div class="line"><a id="l00067" name="l00067"></a><span class="lineno">   67</span>                                   <span class="keyword">const</span> <span class="keywordtype">float</span> *layer_norm_coefficients, <span class="keyword">const</span> <span class="keywordtype">float</span> *gate_bias,</div>
<div class="line"><a id="l00068" name="l00068"></a><span class="lineno">   68</span>                                   <span class="keyword">const</span> <span class="keywordtype">int</span> n_batch, <span class="keyword">const</span> <span class="keywordtype">int</span> n_input, <span class="keyword">const</span> <span class="keywordtype">int</span> n_aux_input,</div>
<div class="line"><a id="l00069" name="l00069"></a><span class="lineno">   69</span>                                   <span class="keyword">const</span> <span class="keywordtype">int</span> n_output, <span class="keyword">const</span> <span class="keywordtype">int</span> n_cell,</div>
<div class="line"><a id="l00070" name="l00070"></a><span class="lineno">   70</span>                                   <span class="keyword">const</span> <a class="code hl_enumeration" href="namespacennfw_1_1cker.html#af09a114337895158c72fde6226b568a2">FusedActivationFunctionType</a> activation, <span class="keywordtype">float</span> *gate,</div>
<div class="line"><a id="l00071" name="l00071"></a><span class="lineno">   71</span>                                   <span class="keyword">const</span> <span class="keywordtype">bool</span> is_input_all_zeros, <span class="keyword">const</span> <span class="keywordtype">bool</span> is_aux_input_all_zeros)</div>
<div class="line"><a id="l00072" name="l00072"></a><span class="lineno">   72</span>{</div>
<div class="line"><a id="l00073" name="l00073"></a><span class="lineno">   73</span>  <span class="keyword">const</span> <span class="keywordtype">bool</span> use_peephole = (cell_to_gate_weights != <span class="keyword">nullptr</span>);</div>
<div class="line"><a id="l00074" name="l00074"></a><span class="lineno">   74</span>  <span class="keyword">const</span> <span class="keywordtype">bool</span> use_layer_norm = (layer_norm_coefficients != <span class="keyword">nullptr</span>);</div>
<div class="line"><a id="l00075" name="l00075"></a><span class="lineno">   75</span> </div>
<div class="line"><a id="l00076" name="l00076"></a><span class="lineno">   76</span>  <span class="comment">// Initialize scratch buffers with bias for regular lstm or initialize with</span></div>
<div class="line"><a id="l00077" name="l00077"></a><span class="lineno">   77</span>  <span class="comment">// zero for layer norm lstm.</span></div>
<div class="line"><a id="l00078" name="l00078"></a><span class="lineno">   78</span>  <span class="keywordflow">if</span> (use_layer_norm)</div>
<div class="line"><a id="l00079" name="l00079"></a><span class="lineno">   79</span>  {</div>
<div class="line"><a id="l00080" name="l00080"></a><span class="lineno">   80</span>    std::fill_n(gate, n_cell * n_batch, 0.0f);</div>
<div class="line"><a id="l00081" name="l00081"></a><span class="lineno">   81</span>  }</div>
<div class="line"><a id="l00082" name="l00082"></a><span class="lineno">   82</span>  <span class="keywordflow">else</span></div>
<div class="line"><a id="l00083" name="l00083"></a><span class="lineno">   83</span>  {</div>
<div class="line"><a id="l00084" name="l00084"></a><span class="lineno">   84</span>    <a class="code hl_function" href="namespacennfw_1_1cker.html#a99b2798fb7403b592e8b860ddf97b937">VectorBatchVectorAssign</a>(gate_bias, n_cell, n_batch, gate);</div>
<div class="line"><a id="l00085" name="l00085"></a><span class="lineno">   85</span>  }</div>
<div class="line"><a id="l00086" name="l00086"></a><span class="lineno">   86</span>  <span class="comment">// For each batch and cell: compute input_weight * input.</span></div>
<div class="line"><a id="l00087" name="l00087"></a><span class="lineno">   87</span>  <span class="comment">// Skip if input is all zeros.</span></div>
<div class="line"><a id="l00088" name="l00088"></a><span class="lineno">   88</span>  <span class="keywordflow">if</span> (!is_input_all_zeros)</div>
<div class="line"><a id="l00089" name="l00089"></a><span class="lineno">   89</span>  {</div>
<div class="line"><a id="l00090" name="l00090"></a><span class="lineno">   90</span>    <a class="code hl_function" href="namespacennfw_1_1cker.html#a537a30edc4be725f28ebdd5c2b123c0b">MatrixBatchVectorMultiplyAccumulate</a>(input_to_gate_weights, n_cell, n_input, input, n_batch,</div>
<div class="line"><a id="l00091" name="l00091"></a><span class="lineno">   91</span>                                        gate, <span class="comment">/*result_stride=*/</span>1);</div>
<div class="line"><a id="l00092" name="l00092"></a><span class="lineno">   92</span>  }</div>
<div class="line"><a id="l00093" name="l00093"></a><span class="lineno">   93</span>  <span class="comment">// For each batch and cell: compute aux_input_weight * aux_input.</span></div>
<div class="line"><a id="l00094" name="l00094"></a><span class="lineno">   94</span>  <span class="comment">// Skip if auxiliary input is not available or all zeros.</span></div>
<div class="line"><a id="l00095" name="l00095"></a><span class="lineno">   95</span>  <span class="keywordflow">if</span> (!is_aux_input_all_zeros)</div>
<div class="line"><a id="l00096" name="l00096"></a><span class="lineno">   96</span>  {</div>
<div class="line"><a id="l00097" name="l00097"></a><span class="lineno">   97</span>    <a class="code hl_function" href="namespacennfw_1_1cker.html#a537a30edc4be725f28ebdd5c2b123c0b">MatrixBatchVectorMultiplyAccumulate</a>(aux_input_to_gate_weights, n_cell, n_aux_input, aux_input,</div>
<div class="line"><a id="l00098" name="l00098"></a><span class="lineno">   98</span>                                        n_batch, gate, <span class="comment">/*result_stride=*/</span>1);</div>
<div class="line"><a id="l00099" name="l00099"></a><span class="lineno">   99</span>  }</div>
<div class="line"><a id="l00100" name="l00100"></a><span class="lineno">  100</span>  <span class="comment">// For each batch and cell: compute recurrent_weight * output_state.</span></div>
<div class="line"><a id="l00101" name="l00101"></a><span class="lineno">  101</span>  <a class="code hl_function" href="namespacennfw_1_1cker.html#a537a30edc4be725f28ebdd5c2b123c0b">MatrixBatchVectorMultiplyAccumulate</a>(recurrent_to_gate_weights, n_cell, n_output, output_state,</div>
<div class="line"><a id="l00102" name="l00102"></a><span class="lineno">  102</span>                                      n_batch, gate, <span class="comment">/*result_stride=*/</span>1);</div>
<div class="line"><a id="l00103" name="l00103"></a><span class="lineno">  103</span>  <span class="comment">// For each batch and cell: compute cell_weight .* cell_state (peephole LSTM)</span></div>
<div class="line"><a id="l00104" name="l00104"></a><span class="lineno">  104</span>  <span class="keywordflow">if</span> (use_peephole)</div>
<div class="line"><a id="l00105" name="l00105"></a><span class="lineno">  105</span>  {</div>
<div class="line"><a id="l00106" name="l00106"></a><span class="lineno">  106</span>    <a class="code hl_function" href="namespacennfw_1_1cker.html#afb7760d00eca9875e4974b51e4874de5">VectorBatchVectorCwiseProductAccumulate</a>(cell_to_gate_weights, n_cell, cell_state, n_batch,</div>
<div class="line"><a id="l00107" name="l00107"></a><span class="lineno">  107</span>                                            gate);</div>
<div class="line"><a id="l00108" name="l00108"></a><span class="lineno">  108</span>  }</div>
<div class="line"><a id="l00109" name="l00109"></a><span class="lineno">  109</span>  <span class="comment">// Do layer normalization (if layer norm LSTM)</span></div>
<div class="line"><a id="l00110" name="l00110"></a><span class="lineno">  110</span>  <span class="keywordflow">if</span> (use_layer_norm)</div>
<div class="line"><a id="l00111" name="l00111"></a><span class="lineno">  111</span>  {</div>
<div class="line"><a id="l00112" name="l00112"></a><span class="lineno">  112</span>    <a class="code hl_function" href="namespacennfw_1_1cker.html#a31790e9290fa11905b4c78808b82da55">MeanStddevNormalization</a>(gate, gate, n_cell, n_batch);</div>
<div class="line"><a id="l00113" name="l00113"></a><span class="lineno">  113</span>    <a class="code hl_function" href="namespacennfw_1_1cker.html#a3bbf78c00591ce015db4b9e01772b17f">VectorBatchVectorCwiseProduct</a>(layer_norm_coefficients, n_cell, gate, n_batch, gate);</div>
<div class="line"><a id="l00114" name="l00114"></a><span class="lineno">  114</span>    <a class="code hl_function" href="namespacennfw_1_1cker.html#ad2604d27d5e99cf215fd7f2553e658a8">VectorBatchVectorAdd</a>(gate_bias, n_cell, n_batch, gate);</div>
<div class="line"><a id="l00115" name="l00115"></a><span class="lineno">  115</span>  }</div>
<div class="line"><a id="l00116" name="l00116"></a><span class="lineno">  116</span>  <span class="comment">// Apply activation</span></div>
<div class="line"><a id="l00117" name="l00117"></a><span class="lineno">  117</span>  <a class="code hl_function" href="namespacennfw_1_1cker.html#a85cc8caa28492b796ed31184533005d7">ApplyActivationToVector</a>(gate, n_batch * n_cell, activation, gate);</div>
<div class="line"><a id="l00118" name="l00118"></a><span class="lineno">  118</span>}</div>
<div class="line"><a id="l00119" name="l00119"></a><span class="lineno">  119</span> </div>
<div class="line"><a id="l00120" name="l00120"></a><span class="lineno">  120</span><span class="comment">// Updates the LSTM cell state, used by both float and hybrid LSTM versions.</span></div>
<div class="line"><a id="l00121" name="l00121"></a><span class="lineno">  121</span><span class="comment">//</span></div>
<div class="line"><a id="l00122" name="l00122"></a><span class="lineno">  122</span><span class="comment">// Implements the following formula:</span></div>
<div class="line"><a id="l00123" name="l00123"></a><span class="lineno">  123</span><span class="comment">//   cell_state_new = clip(forget_gate * cell_state + input_gate * cell_gate)</span></div>
<div class="line"><a id="l00124" name="l00124"></a><span class="lineno">  124</span><span class="comment">//</span></div>
<div class="line"><a id="l00125" name="l00125"></a><span class="lineno">  125</span><span class="comment">// With CIFG LSTM, input gate is replaced by (1-forget_gate).</span></div>
<div class="line"><a id="l00126" name="l00126"></a><span class="lineno">  126</span><span class="comment">//</span></div>
<div class="line"><a id="l00127" name="l00127"></a><span class="lineno">  127</span><span class="comment">// Parameters:</span></div>
<div class="line"><a id="l00128" name="l00128"></a><span class="lineno">  128</span><span class="comment">//  - n_batch, n_cell: sizes of vectors</span></div>
<div class="line"><a id="l00129" name="l00129"></a><span class="lineno">  129</span><span class="comment">//  - cell_state: input/output vector, size n_batch*n_cell</span></div>
<div class="line"><a id="l00130" name="l00130"></a><span class="lineno">  130</span><span class="comment">//  - input_gate: input vector, size n_batch*n_cell.</span></div>
<div class="line"><a id="l00131" name="l00131"></a><span class="lineno">  131</span><span class="comment">//  - forget_gate: input/scratch vector, size n_batch*n_cell, modified with CIFG</span></div>
<div class="line"><a id="l00132" name="l00132"></a><span class="lineno">  132</span><span class="comment">//  - cell_gate: input vector, size n_batch*n_cell.</span></div>
<div class="line"><a id="l00133" name="l00133"></a><span class="lineno">  133</span><span class="comment">//  - use_cifg: use 1-forget_gate instead of input_gate.</span></div>
<div class="line"><a id="l00134" name="l00134"></a><span class="lineno">  134</span><span class="comment">//  - clip: if &gt; 0, clip the resulting cell state to [-clip, +clip].</span></div>
<div class="line"><a id="l00135" name="l00135"></a><span class="lineno"><a class="line" href="namespacennfw_1_1cker.html#a96a47995ad33b9e3aef254cd666f0d1b">  135</a></span><span class="keywordtype">void</span> <a class="code hl_function" href="namespacennfw_1_1cker.html#a96a47995ad33b9e3aef254cd666f0d1b">UpdateLstmCellFloat</a>(<span class="keywordtype">int</span> n_batch, <span class="keywordtype">int</span> n_cell, <span class="keywordtype">float</span> *cell_state, <span class="keyword">const</span> <span class="keywordtype">float</span> *input_gate,</div>
<div class="line"><a id="l00136" name="l00136"></a><span class="lineno">  136</span>                         <span class="keywordtype">float</span> *forget_gate, <span class="keyword">const</span> <span class="keywordtype">float</span> *cell_gate, <span class="keywordtype">bool</span> use_cifg, <span class="keywordtype">float</span> clip)</div>
<div class="line"><a id="l00137" name="l00137"></a><span class="lineno">  137</span>{</div>
<div class="line"><a id="l00138" name="l00138"></a><span class="lineno">  138</span>  <span class="comment">// Define variable for 4th argument to avoid warning</span></div>
<div class="line"><a id="l00139" name="l00139"></a><span class="lineno">  139</span>  <span class="comment">// Compiler warning: passing argument 4 to restrict-qualified parameter aliases with argument 2</span></div>
<div class="line"><a id="l00140" name="l00140"></a><span class="lineno">  140</span>  <span class="keyword">const</span> <span class="keywordtype">float</span> *cwise_product_rhs = cell_state;</div>
<div class="line"><a id="l00141" name="l00141"></a><span class="lineno">  141</span>  <a class="code hl_function" href="namespacennfw_1_1cker.html#a14a817bb0312d18594b6b18cf0a73138">VectorVectorCwiseProduct</a>(forget_gate, cwise_product_rhs, n_batch * n_cell, cell_state);</div>
<div class="line"><a id="l00142" name="l00142"></a><span class="lineno">  142</span> </div>
<div class="line"><a id="l00143" name="l00143"></a><span class="lineno">  143</span>  <span class="keywordflow">if</span> (use_cifg)</div>
<div class="line"><a id="l00144" name="l00144"></a><span class="lineno">  144</span>  {</div>
<div class="line"><a id="l00145" name="l00145"></a><span class="lineno">  145</span>    <span class="comment">// With CIFG, input_gate = 1-forget_gate. Use the forget_gate array as</span></div>
<div class="line"><a id="l00146" name="l00146"></a><span class="lineno">  146</span>    <span class="comment">// scratch, as input_gate array is not allocated in this case. (Be careful</span></div>
<div class="line"><a id="l00147" name="l00147"></a><span class="lineno">  147</span>    <span class="comment">// not to write to the scratch before reading the forget gate data.)</span></div>
<div class="line"><a id="l00148" name="l00148"></a><span class="lineno">  148</span>    <span class="keywordtype">float</span> *scratch = forget_gate;</div>
<div class="line"><a id="l00149" name="l00149"></a><span class="lineno">  149</span>    <a class="code hl_function" href="namespacennfw_1_1cker.html#a51ef48d1448691d903c3204a0d3f91ca">Sub1Vector</a>(forget_gate, n_batch * n_cell, scratch);</div>
<div class="line"><a id="l00150" name="l00150"></a><span class="lineno">  150</span>    <a class="code hl_function" href="namespacennfw_1_1cker.html#ad0b7e90f00bda647bed28b59caed8927">VectorVectorCwiseProductAccumulate</a>(cell_gate, scratch, n_batch * n_cell, cell_state);</div>
<div class="line"><a id="l00151" name="l00151"></a><span class="lineno">  151</span>  }</div>
<div class="line"><a id="l00152" name="l00152"></a><span class="lineno">  152</span>  <span class="keywordflow">else</span></div>
<div class="line"><a id="l00153" name="l00153"></a><span class="lineno">  153</span>  {</div>
<div class="line"><a id="l00154" name="l00154"></a><span class="lineno">  154</span>    <a class="code hl_function" href="namespacennfw_1_1cker.html#ad0b7e90f00bda647bed28b59caed8927">VectorVectorCwiseProductAccumulate</a>(cell_gate, input_gate, n_batch * n_cell, cell_state);</div>
<div class="line"><a id="l00155" name="l00155"></a><span class="lineno">  155</span>  }</div>
<div class="line"><a id="l00156" name="l00156"></a><span class="lineno">  156</span>  <span class="keywordflow">if</span> (clip &gt; 0.0f)</div>
<div class="line"><a id="l00157" name="l00157"></a><span class="lineno">  157</span>  {</div>
<div class="line"><a id="l00158" name="l00158"></a><span class="lineno">  158</span>    <a class="code hl_function" href="namespacennfw_1_1cker.html#acfbd4ad2e41a2fded6cd669424e07fe5">CwiseClipping</a>(cell_state, n_batch * n_cell, clip);</div>
<div class="line"><a id="l00159" name="l00159"></a><span class="lineno">  159</span>  }</div>
<div class="line"><a id="l00160" name="l00160"></a><span class="lineno">  160</span>}</div>
<div class="line"><a id="l00161" name="l00161"></a><span class="lineno">  161</span> </div>
<div class="line"><a id="l00162" name="l00162"></a><span class="lineno">  162</span><span class="comment">// Calculates the output state tensor of an LSTM step.</span></div>
<div class="line"><a id="l00163" name="l00163"></a><span class="lineno">  163</span><span class="comment">//</span></div>
<div class="line"><a id="l00164" name="l00164"></a><span class="lineno">  164</span><span class="comment">// Implements the following formula:</span></div>
<div class="line"><a id="l00165" name="l00165"></a><span class="lineno">  165</span><span class="comment">//   output_no_projection = output_gate .* activate(cell_state)</span></div>
<div class="line"><a id="l00166" name="l00166"></a><span class="lineno">  166</span><span class="comment">//     (elementwise vector product)</span></div>
<div class="line"><a id="l00167" name="l00167"></a><span class="lineno">  167</span><span class="comment">// If no projection is used:</span></div>
<div class="line"><a id="l00168" name="l00168"></a><span class="lineno">  168</span><span class="comment">//   output = output_state = output_no_projection</span></div>
<div class="line"><a id="l00169" name="l00169"></a><span class="lineno">  169</span><span class="comment">// With projection:</span></div>
<div class="line"><a id="l00170" name="l00170"></a><span class="lineno">  170</span><span class="comment">//   output = output_state = clip(W*output_no_projection + bias)</span></div>
<div class="line"><a id="l00171" name="l00171"></a><span class="lineno">  171</span><span class="comment">//</span></div>
<div class="line"><a id="l00172" name="l00172"></a><span class="lineno">  172</span><span class="comment">// Output might not have a different &#39;stride&#39; than n_batch, so we need to copy.</span></div>
<div class="line"><a id="l00173" name="l00173"></a><span class="lineno">  173</span><span class="comment">//</span></div>
<div class="line"><a id="l00174" name="l00174"></a><span class="lineno">  174</span><span class="comment">// Parameters:</span></div>
<div class="line"><a id="l00175" name="l00175"></a><span class="lineno">  175</span><span class="comment">//  - n_batch: batches: the number of distinct vectors in each array.</span></div>
<div class="line"><a id="l00176" name="l00176"></a><span class="lineno">  176</span><span class="comment">//  - n_cell, n_output: sizes of vectors.</span></div>
<div class="line"><a id="l00177" name="l00177"></a><span class="lineno">  177</span><span class="comment">//  - cell_state, output_gate: input vectors, size n_batch*n_cell.</span></div>
<div class="line"><a id="l00178" name="l00178"></a><span class="lineno">  178</span><span class="comment">//  - projection_weights, projection_weights_scale, projection_bias:</span></div>
<div class="line"><a id="l00179" name="l00179"></a><span class="lineno">  179</span><span class="comment">//      constant inputs, describing projection matrix and bias.</span></div>
<div class="line"><a id="l00180" name="l00180"></a><span class="lineno">  180</span><span class="comment">//  - proj_clip: if &gt; 0, clip the output of the projection.</span></div>
<div class="line"><a id="l00181" name="l00181"></a><span class="lineno">  181</span><span class="comment">//  - output_state: output vector, size n_batch*n_output. Must be contigous.</span></div>
<div class="line"><a id="l00182" name="l00182"></a><span class="lineno">  182</span><span class="comment">//  - scratch: scratch area, size n_batch*n_cell.</span></div>
<div class="line"><a id="l00183" name="l00183"></a><span class="lineno"><a class="line" href="namespacennfw_1_1cker.html#a964f3c61b8ff5306670b7432fdb5334c">  183</a></span><span class="keywordtype">void</span> <a class="code hl_function" href="namespacennfw_1_1cker.html#a964f3c61b8ff5306670b7432fdb5334c">CalculateLstmOutputFloat</a>(<span class="keywordtype">int</span> n_batch, <span class="keywordtype">int</span> n_cell, <span class="keywordtype">int</span> n_output, <span class="keyword">const</span> <span class="keywordtype">float</span> *cell_state,</div>
<div class="line"><a id="l00184" name="l00184"></a><span class="lineno">  184</span>                              <span class="keyword">const</span> <span class="keywordtype">float</span> *output_gate, <a class="code hl_enumeration" href="namespacennfw_1_1cker.html#af09a114337895158c72fde6226b568a2">FusedActivationFunctionType</a> activation,</div>
<div class="line"><a id="l00185" name="l00185"></a><span class="lineno">  185</span>                              <span class="keyword">const</span> <span class="keywordtype">float</span> *projection_weights, <span class="keyword">const</span> <span class="keywordtype">float</span> *projection_bias,</div>
<div class="line"><a id="l00186" name="l00186"></a><span class="lineno">  186</span>                              <span class="keyword">const</span> <span class="keywordtype">float</span> proj_clip, <span class="keywordtype">float</span> *output_state, <span class="keywordtype">float</span> *scratch)</div>
<div class="line"><a id="l00187" name="l00187"></a><span class="lineno">  187</span>{</div>
<div class="line"><a id="l00188" name="l00188"></a><span class="lineno">  188</span>  <a class="code hl_function" href="namespacennfw_1_1cker.html#a85cc8caa28492b796ed31184533005d7">ApplyActivationToVector</a>(cell_state, n_batch * n_cell, activation, scratch);</div>
<div class="line"><a id="l00189" name="l00189"></a><span class="lineno">  189</span> </div>
<div class="line"><a id="l00190" name="l00190"></a><span class="lineno">  190</span>  <span class="comment">// Define variable for 4th argument to avoid warning</span></div>
<div class="line"><a id="l00191" name="l00191"></a><span class="lineno">  191</span>  <span class="comment">// Compiler warning: passing argument 4 to restrict-qualified parameter aliases with argument 2</span></div>
<div class="line"><a id="l00192" name="l00192"></a><span class="lineno">  192</span>  <span class="keyword">const</span> <span class="keywordtype">float</span> *cwise_product_rhs = scratch;</div>
<div class="line"><a id="l00193" name="l00193"></a><span class="lineno">  193</span>  <a class="code hl_function" href="namespacennfw_1_1cker.html#a14a817bb0312d18594b6b18cf0a73138">VectorVectorCwiseProduct</a>(output_gate, cwise_product_rhs, n_batch * n_cell, scratch);</div>
<div class="line"><a id="l00194" name="l00194"></a><span class="lineno">  194</span> </div>
<div class="line"><a id="l00195" name="l00195"></a><span class="lineno">  195</span>  <span class="keyword">const</span> <span class="keywordtype">bool</span> use_projection = (projection_weights != <span class="keyword">nullptr</span>);</div>
<div class="line"><a id="l00196" name="l00196"></a><span class="lineno">  196</span>  <span class="keyword">const</span> <span class="keywordtype">bool</span> use_projection_bias = (projection_bias != <span class="keyword">nullptr</span>);</div>
<div class="line"><a id="l00197" name="l00197"></a><span class="lineno">  197</span> </div>
<div class="line"><a id="l00198" name="l00198"></a><span class="lineno">  198</span>  <span class="keywordflow">if</span> (use_projection)</div>
<div class="line"><a id="l00199" name="l00199"></a><span class="lineno">  199</span>  {</div>
<div class="line"><a id="l00200" name="l00200"></a><span class="lineno">  200</span>    <span class="keywordflow">if</span> (use_projection_bias)</div>
<div class="line"><a id="l00201" name="l00201"></a><span class="lineno">  201</span>    {</div>
<div class="line"><a id="l00202" name="l00202"></a><span class="lineno">  202</span>      <a class="code hl_function" href="namespacennfw_1_1cker.html#a99b2798fb7403b592e8b860ddf97b937">VectorBatchVectorAssign</a>(projection_bias, n_output, n_batch, output_state);</div>
<div class="line"><a id="l00203" name="l00203"></a><span class="lineno">  203</span>    }</div>
<div class="line"><a id="l00204" name="l00204"></a><span class="lineno">  204</span>    <span class="keywordflow">else</span></div>
<div class="line"><a id="l00205" name="l00205"></a><span class="lineno">  205</span>    {</div>
<div class="line"><a id="l00206" name="l00206"></a><span class="lineno">  206</span>      std::fill_n(output_state, n_batch * n_output, 0.0f);</div>
<div class="line"><a id="l00207" name="l00207"></a><span class="lineno">  207</span>    }</div>
<div class="line"><a id="l00208" name="l00208"></a><span class="lineno">  208</span>    <a class="code hl_function" href="namespacennfw_1_1cker.html#a537a30edc4be725f28ebdd5c2b123c0b">MatrixBatchVectorMultiplyAccumulate</a>(projection_weights, n_output, n_cell, scratch, n_batch,</div>
<div class="line"><a id="l00209" name="l00209"></a><span class="lineno">  209</span>                                        output_state, <span class="comment">/*result_stride=*/</span>1);</div>
<div class="line"><a id="l00210" name="l00210"></a><span class="lineno">  210</span>    <span class="keywordflow">if</span> (proj_clip &gt; 0.0f)</div>
<div class="line"><a id="l00211" name="l00211"></a><span class="lineno">  211</span>    {</div>
<div class="line"><a id="l00212" name="l00212"></a><span class="lineno">  212</span>      <a class="code hl_function" href="namespacennfw_1_1cker.html#acfbd4ad2e41a2fded6cd669424e07fe5">CwiseClipping</a>(output_state, n_batch * n_output, proj_clip);</div>
<div class="line"><a id="l00213" name="l00213"></a><span class="lineno">  213</span>    }</div>
<div class="line"><a id="l00214" name="l00214"></a><span class="lineno">  214</span>  }</div>
<div class="line"><a id="l00215" name="l00215"></a><span class="lineno">  215</span>  <span class="keywordflow">else</span></div>
<div class="line"><a id="l00216" name="l00216"></a><span class="lineno">  216</span>  {</div>
<div class="line"><a id="l00217" name="l00217"></a><span class="lineno">  217</span>    std::copy_n(scratch, n_batch * n_output, output_state);</div>
<div class="line"><a id="l00218" name="l00218"></a><span class="lineno">  218</span>  }</div>
<div class="line"><a id="l00219" name="l00219"></a><span class="lineno">  219</span>}</div>
<div class="line"><a id="l00220" name="l00220"></a><span class="lineno">  220</span> </div>
<div class="line"><a id="l00221" name="l00221"></a><span class="lineno">  221</span><span class="comment">// Performs an LSTM batch inference step for input specified by input_ptr.</span></div>
<div class="line"><a id="l00222" name="l00222"></a><span class="lineno">  222</span><span class="comment">// The LSTM cell is specified by the pointers to its weights (*_weights_ptr) and</span></div>
<div class="line"><a id="l00223" name="l00223"></a><span class="lineno">  223</span><span class="comment">// biases (*_bias_ptr), and buffers (*_scratch), along with additional</span></div>
<div class="line"><a id="l00224" name="l00224"></a><span class="lineno">  224</span><span class="comment">// parameters:</span></div>
<div class="line"><a id="l00225" name="l00225"></a><span class="lineno">  225</span><span class="comment">//  - params: various LSTM params including activation, clipping, etc.,</span></div>
<div class="line"><a id="l00226" name="l00226"></a><span class="lineno">  226</span><span class="comment">//  - n_batch: size of batch,</span></div>
<div class="line"><a id="l00227" name="l00227"></a><span class="lineno">  227</span><span class="comment">//  - n_cell: number of cells (or units),</span></div>
<div class="line"><a id="l00228" name="l00228"></a><span class="lineno">  228</span><span class="comment">//  - n_input: the input size,</span></div>
<div class="line"><a id="l00229" name="l00229"></a><span class="lineno">  229</span><span class="comment">//  - n_aux_input: the auxiliary input size.</span></div>
<div class="line"><a id="l00230" name="l00230"></a><span class="lineno">  230</span><span class="comment">//  - n_output: the output size.</span></div>
<div class="line"><a id="l00231" name="l00231"></a><span class="lineno">  231</span><span class="comment">//  - output_batch_leading_dim: the leading dimension of the output buffer.</span></div>
<div class="line"><a id="l00232" name="l00232"></a><span class="lineno">  232</span><span class="comment">//</span></div>
<div class="line"><a id="l00233" name="l00233"></a><span class="lineno">  233</span><span class="comment">// Input of size &#39;n_batch * n_input&#39;:</span></div>
<div class="line"><a id="l00234" name="l00234"></a><span class="lineno">  234</span><span class="comment">//   input_ptr</span></div>
<div class="line"><a id="l00235" name="l00235"></a><span class="lineno">  235</span><span class="comment">// Input of size &#39;n_batch * n_aux_input&#39;:</span></div>
<div class="line"><a id="l00236" name="l00236"></a><span class="lineno">  236</span><span class="comment">//   aux_input_ptr                     - optional (can be nullptr)</span></div>
<div class="line"><a id="l00237" name="l00237"></a><span class="lineno">  237</span><span class="comment">//</span></div>
<div class="line"><a id="l00238" name="l00238"></a><span class="lineno">  238</span><span class="comment">// LSTM weights:</span></div>
<div class="line"><a id="l00239" name="l00239"></a><span class="lineno">  239</span><span class="comment">// Input weights of size &#39;n_cell * n_input&#39;:</span></div>
<div class="line"><a id="l00240" name="l00240"></a><span class="lineno">  240</span><span class="comment">//   input_to_input_weights            - optional</span></div>
<div class="line"><a id="l00241" name="l00241"></a><span class="lineno">  241</span><span class="comment">//   input_to_forget_weights</span></div>
<div class="line"><a id="l00242" name="l00242"></a><span class="lineno">  242</span><span class="comment">//   input_to_cell_weights</span></div>
<div class="line"><a id="l00243" name="l00243"></a><span class="lineno">  243</span><span class="comment">//   input_to_output_weights</span></div>
<div class="line"><a id="l00244" name="l00244"></a><span class="lineno">  244</span><span class="comment">// Auxiliary input weights of size &#39;n_cell * n_aux_input&#39;:</span></div>
<div class="line"><a id="l00245" name="l00245"></a><span class="lineno">  245</span><span class="comment">//   aux_input_to_input_weights        - optional</span></div>
<div class="line"><a id="l00246" name="l00246"></a><span class="lineno">  246</span><span class="comment">//   aux_input_to_forget_weights       - optional</span></div>
<div class="line"><a id="l00247" name="l00247"></a><span class="lineno">  247</span><span class="comment">//   aux_input_to_cell_weights         - optional</span></div>
<div class="line"><a id="l00248" name="l00248"></a><span class="lineno">  248</span><span class="comment">//   aux_input_to_output_weights       - optional</span></div>
<div class="line"><a id="l00249" name="l00249"></a><span class="lineno">  249</span><span class="comment">// Recurrent weights of size &#39;n_cell * n_output&#39;:</span></div>
<div class="line"><a id="l00250" name="l00250"></a><span class="lineno">  250</span><span class="comment">//   recurrent_to_input_weights        - optional</span></div>
<div class="line"><a id="l00251" name="l00251"></a><span class="lineno">  251</span><span class="comment">//   recurrent_to_forget_weights</span></div>
<div class="line"><a id="l00252" name="l00252"></a><span class="lineno">  252</span><span class="comment">//   recurrent_to_cell_weights</span></div>
<div class="line"><a id="l00253" name="l00253"></a><span class="lineno">  253</span><span class="comment">//   recurrent_to_input_weights</span></div>
<div class="line"><a id="l00254" name="l00254"></a><span class="lineno">  254</span><span class="comment">// Peephole weights of size &#39;n_cell&#39;, representing diagonal matrices.</span></div>
<div class="line"><a id="l00255" name="l00255"></a><span class="lineno">  255</span><span class="comment">//   cell_to_input_weights             - optional</span></div>
<div class="line"><a id="l00256" name="l00256"></a><span class="lineno">  256</span><span class="comment">//   cell_to_cell_weights              - optional</span></div>
<div class="line"><a id="l00257" name="l00257"></a><span class="lineno">  257</span><span class="comment">//   cell_to_output_weights            - optional</span></div>
<div class="line"><a id="l00258" name="l00258"></a><span class="lineno">  258</span><span class="comment">// Projection weights of size &#39;n_output * n_cell&#39;</span></div>
<div class="line"><a id="l00259" name="l00259"></a><span class="lineno">  259</span><span class="comment">//   projection_weights_ptr            - optional</span></div>
<div class="line"><a id="l00260" name="l00260"></a><span class="lineno">  260</span><span class="comment">// Gate biases of size &#39;n_cell&#39;:</span></div>
<div class="line"><a id="l00261" name="l00261"></a><span class="lineno">  261</span><span class="comment">//   input_gate_bias_ptr               - optional</span></div>
<div class="line"><a id="l00262" name="l00262"></a><span class="lineno">  262</span><span class="comment">//   forget_gate_bias_ptr</span></div>
<div class="line"><a id="l00263" name="l00263"></a><span class="lineno">  263</span><span class="comment">//   cell_gate_bias_ptr</span></div>
<div class="line"><a id="l00264" name="l00264"></a><span class="lineno">  264</span><span class="comment">//   output_gate_bias_ptr</span></div>
<div class="line"><a id="l00265" name="l00265"></a><span class="lineno">  265</span><span class="comment">//</span></div>
<div class="line"><a id="l00266" name="l00266"></a><span class="lineno">  266</span><span class="comment">// Layer norm coefficients of size &#39;n_cell&#39;, representing diagonal matrices.</span></div>
<div class="line"><a id="l00267" name="l00267"></a><span class="lineno">  267</span><span class="comment">//   input_layer_norm_coefficients_ptr  - optional</span></div>
<div class="line"><a id="l00268" name="l00268"></a><span class="lineno">  268</span><span class="comment">//   forget_layer_norm_coefficients_ptr - optional</span></div>
<div class="line"><a id="l00269" name="l00269"></a><span class="lineno">  269</span><span class="comment">//   cell_layer_norm_coefficients_ptr   - optional</span></div>
<div class="line"><a id="l00270" name="l00270"></a><span class="lineno">  270</span><span class="comment">//   output_layer_norm_coefficients_ptr - optional</span></div>
<div class="line"><a id="l00271" name="l00271"></a><span class="lineno">  271</span><span class="comment">//</span></div>
<div class="line"><a id="l00272" name="l00272"></a><span class="lineno">  272</span><span class="comment">// The pointers to the cell and output state and the output are updated.</span></div>
<div class="line"><a id="l00273" name="l00273"></a><span class="lineno">  273</span><span class="comment">//</span></div>
<div class="line"><a id="l00274" name="l00274"></a><span class="lineno">  274</span><span class="comment">// The pointers input_ptr, aux_input_ptr, and output_ptr point to data aligned</span></div>
<div class="line"><a id="l00275" name="l00275"></a><span class="lineno">  275</span><span class="comment">// in batch_major order, and each step processes batch_size many inputs from</span></div>
<div class="line"><a id="l00276" name="l00276"></a><span class="lineno">  276</span><span class="comment">// input_ptr, and updates batch_size many cell and output states.</span></div>
<div class="line"><a id="l00277" name="l00277"></a><span class="lineno">  277</span><span class="comment">//</span></div>
<div class="line"><a id="l00278" name="l00278"></a><span class="lineno">  278</span><span class="comment">// The output_batch_dim is output.shape[-1], i.e. the outermost dimension of the</span></div>
<div class="line"><a id="l00279" name="l00279"></a><span class="lineno">  279</span><span class="comment">// output tensor, and in most cases will be equal to n_output. It is usually not</span></div>
<div class="line"><a id="l00280" name="l00280"></a><span class="lineno">  280</span><span class="comment">// when we want to store the LSTM output into a slice of the output tensor, e.g.</span></div>
<div class="line"><a id="l00281" name="l00281"></a><span class="lineno">  281</span><span class="comment">// for bidirectional LSTMs with merge_outputs. In this case, the batched</span></div>
<div class="line"><a id="l00282" name="l00282"></a><span class="lineno">  282</span><span class="comment">// operations cannot be used since they assume that the batched outputs are</span></div>
<div class="line"><a id="l00283" name="l00283"></a><span class="lineno">  283</span><span class="comment">// contiguous, and we manually loop over the batched outputs.</span></div>
<div class="line"><a id="l00284" name="l00284"></a><span class="lineno">  284</span><span class="comment">// LINT.IfChange</span></div>
<div class="line"><a id="l00285" name="l00285"></a><span class="lineno"><a class="line" href="namespacennfw_1_1cker.html#ae6c06b361e57b13dd187ca45ed5fc737">  285</a></span><span class="keyword">inline</span> <span class="keywordtype">void</span> <a class="code hl_function" href="namespacennfw_1_1cker.html#ae6c06b361e57b13dd187ca45ed5fc737">LstmStepFloat</a>(</div>
<div class="line"><a id="l00286" name="l00286"></a><span class="lineno">  286</span>  <span class="keyword">const</span> <span class="keywordtype">float</span> *input_ptr, <span class="keyword">const</span> <span class="keywordtype">float</span> *input_to_input_weights_ptr,</div>
<div class="line"><a id="l00287" name="l00287"></a><span class="lineno">  287</span>  <span class="keyword">const</span> <span class="keywordtype">float</span> *input_to_forget_weights_ptr, <span class="keyword">const</span> <span class="keywordtype">float</span> *input_to_cell_weights_ptr,</div>
<div class="line"><a id="l00288" name="l00288"></a><span class="lineno">  288</span>  <span class="keyword">const</span> <span class="keywordtype">float</span> *input_to_output_weights_ptr, <span class="keyword">const</span> <span class="keywordtype">float</span> *aux_input_ptr,</div>
<div class="line"><a id="l00289" name="l00289"></a><span class="lineno">  289</span>  <span class="keyword">const</span> <span class="keywordtype">float</span> *aux_input_to_input_weights_ptr, <span class="keyword">const</span> <span class="keywordtype">float</span> *aux_input_to_forget_weights_ptr,</div>
<div class="line"><a id="l00290" name="l00290"></a><span class="lineno">  290</span>  <span class="keyword">const</span> <span class="keywordtype">float</span> *aux_input_to_cell_weights_ptr, <span class="keyword">const</span> <span class="keywordtype">float</span> *aux_input_to_output_weights_ptr,</div>
<div class="line"><a id="l00291" name="l00291"></a><span class="lineno">  291</span>  <span class="keyword">const</span> <span class="keywordtype">float</span> *recurrent_to_input_weights_ptr, <span class="keyword">const</span> <span class="keywordtype">float</span> *recurrent_to_forget_weights_ptr,</div>
<div class="line"><a id="l00292" name="l00292"></a><span class="lineno">  292</span>  <span class="keyword">const</span> <span class="keywordtype">float</span> *recurrent_to_cell_weights_ptr, <span class="keyword">const</span> <span class="keywordtype">float</span> *recurrent_to_output_weights_ptr,</div>
<div class="line"><a id="l00293" name="l00293"></a><span class="lineno">  293</span>  <span class="keyword">const</span> <span class="keywordtype">float</span> *cell_to_input_weights_ptr, <span class="keyword">const</span> <span class="keywordtype">float</span> *cell_to_forget_weights_ptr,</div>
<div class="line"><a id="l00294" name="l00294"></a><span class="lineno">  294</span>  <span class="keyword">const</span> <span class="keywordtype">float</span> *cell_to_output_weights_ptr, <span class="keyword">const</span> <span class="keywordtype">float</span> *input_layer_norm_coefficients_ptr,</div>
<div class="line"><a id="l00295" name="l00295"></a><span class="lineno">  295</span>  <span class="keyword">const</span> <span class="keywordtype">float</span> *forget_layer_norm_coefficients_ptr, <span class="keyword">const</span> <span class="keywordtype">float</span> *cell_layer_norm_coefficients_ptr,</div>
<div class="line"><a id="l00296" name="l00296"></a><span class="lineno">  296</span>  <span class="keyword">const</span> <span class="keywordtype">float</span> *output_layer_norm_coefficients_ptr, <span class="keyword">const</span> <span class="keywordtype">float</span> *input_gate_bias_ptr,</div>
<div class="line"><a id="l00297" name="l00297"></a><span class="lineno">  297</span>  <span class="keyword">const</span> <span class="keywordtype">float</span> *forget_gate_bias_ptr, <span class="keyword">const</span> <span class="keywordtype">float</span> *cell_gate_bias_ptr,</div>
<div class="line"><a id="l00298" name="l00298"></a><span class="lineno">  298</span>  <span class="keyword">const</span> <span class="keywordtype">float</span> *output_gate_bias_ptr, <span class="keyword">const</span> <span class="keywordtype">float</span> *projection_weights_ptr,</div>
<div class="line"><a id="l00299" name="l00299"></a><span class="lineno">  299</span>  <span class="keyword">const</span> <span class="keywordtype">float</span> *projection_bias_ptr, <span class="keyword">const</span> <a class="code hl_struct" href="structnnfw_1_1cker_1_1_l_s_t_m_params.html">LSTMParams</a> *params, <span class="keywordtype">int</span> n_batch, <span class="keywordtype">int</span> n_cell, <span class="keywordtype">int</span> n_input,</div>
<div class="line"><a id="l00300" name="l00300"></a><span class="lineno">  300</span>  <span class="keywordtype">int</span> n_aux_input, <span class="keywordtype">int</span> n_output, <span class="keywordtype">int</span> output_batch_leading_dim, <span class="keywordtype">float</span> *output_state_ptr,</div>
<div class="line"><a id="l00301" name="l00301"></a><span class="lineno">  301</span>  <span class="keywordtype">float</span> *cell_state_ptr, <span class="keywordtype">float</span> *scratch0, <span class="keywordtype">float</span> *scratch1, <span class="keywordtype">float</span> *scratch2, <span class="keywordtype">float</span> *scratch3,</div>
<div class="line"><a id="l00302" name="l00302"></a><span class="lineno">  302</span>  <span class="keywordtype">float</span> *output_ptr)</div>
<div class="line"><a id="l00303" name="l00303"></a><span class="lineno">  303</span>{</div>
<div class="line"><a id="l00304" name="l00304"></a><span class="lineno">  304</span>  <span class="comment">// Since we have already checked that weights are all there or none, we can</span></div>
<div class="line"><a id="l00305" name="l00305"></a><span class="lineno">  305</span>  <span class="comment">// check the existence of only one to the get the condition.</span></div>
<div class="line"><a id="l00306" name="l00306"></a><span class="lineno">  306</span>  <span class="keyword">const</span> <span class="keywordtype">bool</span> use_cifg = (input_to_input_weights_ptr == <span class="keyword">nullptr</span>);</div>
<div class="line"><a id="l00307" name="l00307"></a><span class="lineno">  307</span> </div>
<div class="line"><a id="l00308" name="l00308"></a><span class="lineno">  308</span>  <span class="comment">// Make named scratch buffers.</span></div>
<div class="line"><a id="l00309" name="l00309"></a><span class="lineno">  309</span>  <span class="keywordtype">float</span> *input_gate_scratch = scratch0;</div>
<div class="line"><a id="l00310" name="l00310"></a><span class="lineno">  310</span>  <span class="keywordtype">float</span> *forget_gate_scratch = scratch1;</div>
<div class="line"><a id="l00311" name="l00311"></a><span class="lineno">  311</span>  <span class="keywordtype">float</span> *cell_gate_scratch = scratch2;</div>
<div class="line"><a id="l00312" name="l00312"></a><span class="lineno">  312</span>  <span class="keywordtype">float</span> *output_gate_scratch = scratch3;</div>
<div class="line"><a id="l00313" name="l00313"></a><span class="lineno">  313</span> </div>
<div class="line"><a id="l00314" name="l00314"></a><span class="lineno">  314</span>  <span class="comment">// Check if inputs are all zeros so we can skip some computations.</span></div>
<div class="line"><a id="l00315" name="l00315"></a><span class="lineno">  315</span>  <span class="keyword">const</span> <span class="keywordtype">bool</span> is_input_all_zeros = <a class="code hl_function" href="namespacennfw_1_1cker.html#af82fa9f0cdbd15fed59567999835cebf">IsZeroVector</a>(input_ptr, n_batch * n_input);</div>
<div class="line"><a id="l00316" name="l00316"></a><span class="lineno">  316</span>  <span class="keyword">const</span> <span class="keywordtype">bool</span> is_aux_input_all_zeros =</div>
<div class="line"><a id="l00317" name="l00317"></a><span class="lineno">  317</span>    (aux_input_ptr == <span class="keyword">nullptr</span> || <a class="code hl_function" href="namespacennfw_1_1cker.html#af82fa9f0cdbd15fed59567999835cebf">IsZeroVector</a>(aux_input_ptr, n_batch * n_aux_input));</div>
<div class="line"><a id="l00318" name="l00318"></a><span class="lineno">  318</span>  <span class="keywordflow">if</span> (!use_cifg)</div>
<div class="line"><a id="l00319" name="l00319"></a><span class="lineno">  319</span>  {</div>
<div class="line"><a id="l00320" name="l00320"></a><span class="lineno">  320</span>    <span class="comment">// Calculate the input gate. (If not CIFG.)</span></div>
<div class="line"><a id="l00321" name="l00321"></a><span class="lineno">  321</span>    <a class="code hl_function" href="namespacennfw_1_1cker.html#ae60dea0f6844a22a3ed03e20d8a00e78">CalculateLstmGateFloat</a>(input_ptr, input_to_input_weights_ptr, aux_input_ptr,</div>
<div class="line"><a id="l00322" name="l00322"></a><span class="lineno">  322</span>                           aux_input_to_input_weights_ptr, output_state_ptr,</div>
<div class="line"><a id="l00323" name="l00323"></a><span class="lineno">  323</span>                           recurrent_to_input_weights_ptr, cell_state_ptr,</div>
<div class="line"><a id="l00324" name="l00324"></a><span class="lineno">  324</span>                           cell_to_input_weights_ptr, input_layer_norm_coefficients_ptr,</div>
<div class="line"><a id="l00325" name="l00325"></a><span class="lineno">  325</span>                           input_gate_bias_ptr, n_batch, n_input, n_aux_input, n_output, n_cell,</div>
<div class="line"><a id="l00326" name="l00326"></a><span class="lineno">  326</span>                           <span class="comment">/*activation=kTfLiteActSigmoid*/</span> <a class="code hl_enumvalue" href="namespacennfw_1_1cker.html#af09a114337895158c72fde6226b568a2ac00732693e14261bf9c2a4612a7f9bf9">FusedActivationFunctionType::kSigmoid</a>,</div>
<div class="line"><a id="l00327" name="l00327"></a><span class="lineno">  327</span>                           input_gate_scratch, is_input_all_zeros, is_aux_input_all_zeros);</div>
<div class="line"><a id="l00328" name="l00328"></a><span class="lineno">  328</span>  }</div>
<div class="line"><a id="l00329" name="l00329"></a><span class="lineno">  329</span>  <span class="comment">// Calculate the forget gate.</span></div>
<div class="line"><a id="l00330" name="l00330"></a><span class="lineno">  330</span>  <a class="code hl_function" href="namespacennfw_1_1cker.html#ae60dea0f6844a22a3ed03e20d8a00e78">CalculateLstmGateFloat</a>(input_ptr, input_to_forget_weights_ptr, aux_input_ptr,</div>
<div class="line"><a id="l00331" name="l00331"></a><span class="lineno">  331</span>                         aux_input_to_forget_weights_ptr, output_state_ptr,</div>
<div class="line"><a id="l00332" name="l00332"></a><span class="lineno">  332</span>                         recurrent_to_forget_weights_ptr, cell_state_ptr,</div>
<div class="line"><a id="l00333" name="l00333"></a><span class="lineno">  333</span>                         cell_to_forget_weights_ptr, forget_layer_norm_coefficients_ptr,</div>
<div class="line"><a id="l00334" name="l00334"></a><span class="lineno">  334</span>                         forget_gate_bias_ptr, n_batch, n_input, n_aux_input, n_output, n_cell,</div>
<div class="line"><a id="l00335" name="l00335"></a><span class="lineno">  335</span>                         <span class="comment">/*activation=kTfLiteActSigmoid*/</span> <a class="code hl_enumvalue" href="namespacennfw_1_1cker.html#af09a114337895158c72fde6226b568a2ac00732693e14261bf9c2a4612a7f9bf9">FusedActivationFunctionType::kSigmoid</a>,</div>
<div class="line"><a id="l00336" name="l00336"></a><span class="lineno">  336</span>                         forget_gate_scratch, is_input_all_zeros, is_aux_input_all_zeros);</div>
<div class="line"><a id="l00337" name="l00337"></a><span class="lineno">  337</span>  <span class="comment">// Calculate the cell update gate.</span></div>
<div class="line"><a id="l00338" name="l00338"></a><span class="lineno">  338</span>  <a class="code hl_function" href="namespacennfw_1_1cker.html#ae60dea0f6844a22a3ed03e20d8a00e78">CalculateLstmGateFloat</a>(</div>
<div class="line"><a id="l00339" name="l00339"></a><span class="lineno">  339</span>    input_ptr, input_to_cell_weights_ptr, aux_input_ptr, aux_input_to_cell_weights_ptr,</div>
<div class="line"><a id="l00340" name="l00340"></a><span class="lineno">  340</span>    output_state_ptr, recurrent_to_cell_weights_ptr, <span class="comment">/*cell_state=*/</span><span class="keyword">nullptr</span>,</div>
<div class="line"><a id="l00341" name="l00341"></a><span class="lineno">  341</span>    <span class="comment">/*cell_to_gate_weights=*/</span><span class="keyword">nullptr</span>, cell_layer_norm_coefficients_ptr, cell_gate_bias_ptr, n_batch,</div>
<div class="line"><a id="l00342" name="l00342"></a><span class="lineno">  342</span>    n_input, n_aux_input, n_output, n_cell, params-&gt;<a class="code hl_variable" href="structnnfw_1_1cker_1_1_l_s_t_m_params.html#ab3a9e171f2b52072433b8da3935ab740">activation</a>, cell_gate_scratch,</div>
<div class="line"><a id="l00343" name="l00343"></a><span class="lineno">  343</span>    is_input_all_zeros, is_aux_input_all_zeros);</div>
<div class="line"><a id="l00344" name="l00344"></a><span class="lineno">  344</span>  <span class="comment">// Update the cell state.</span></div>
<div class="line"><a id="l00345" name="l00345"></a><span class="lineno">  345</span>  <a class="code hl_function" href="namespacennfw_1_1cker.html#a96a47995ad33b9e3aef254cd666f0d1b">UpdateLstmCellFloat</a>(n_batch, n_cell, cell_state_ptr, input_gate_scratch, forget_gate_scratch,</div>
<div class="line"><a id="l00346" name="l00346"></a><span class="lineno">  346</span>                      cell_gate_scratch, use_cifg, params-&gt;<a class="code hl_variable" href="structnnfw_1_1cker_1_1_l_s_t_m_params.html#a0138a9307034d1fbd0f44aae19d29151">cell_clip</a>);</div>
<div class="line"><a id="l00347" name="l00347"></a><span class="lineno">  347</span>  <span class="comment">// Calculate output gate.</span></div>
<div class="line"><a id="l00348" name="l00348"></a><span class="lineno">  348</span>  <a class="code hl_function" href="namespacennfw_1_1cker.html#ae60dea0f6844a22a3ed03e20d8a00e78">CalculateLstmGateFloat</a>(input_ptr, input_to_output_weights_ptr, aux_input_ptr,</div>
<div class="line"><a id="l00349" name="l00349"></a><span class="lineno">  349</span>                         aux_input_to_output_weights_ptr, output_state_ptr,</div>
<div class="line"><a id="l00350" name="l00350"></a><span class="lineno">  350</span>                         recurrent_to_output_weights_ptr, cell_state_ptr,</div>
<div class="line"><a id="l00351" name="l00351"></a><span class="lineno">  351</span>                         cell_to_output_weights_ptr, output_layer_norm_coefficients_ptr,</div>
<div class="line"><a id="l00352" name="l00352"></a><span class="lineno">  352</span>                         output_gate_bias_ptr, n_batch, n_input, n_aux_input, n_output, n_cell,</div>
<div class="line"><a id="l00353" name="l00353"></a><span class="lineno">  353</span>                         <span class="comment">/*activation=kTfLiteActSigmoid*/</span> <a class="code hl_enumvalue" href="namespacennfw_1_1cker.html#af09a114337895158c72fde6226b568a2ac00732693e14261bf9c2a4612a7f9bf9">FusedActivationFunctionType::kSigmoid</a>,</div>
<div class="line"><a id="l00354" name="l00354"></a><span class="lineno">  354</span>                         output_gate_scratch, is_input_all_zeros, is_aux_input_all_zeros);</div>
<div class="line"><a id="l00355" name="l00355"></a><span class="lineno">  355</span>  <span class="comment">// Update the output state.</span></div>
<div class="line"><a id="l00356" name="l00356"></a><span class="lineno">  356</span>  <a class="code hl_function" href="namespacennfw_1_1cker.html#a964f3c61b8ff5306670b7432fdb5334c">CalculateLstmOutputFloat</a>(n_batch, n_cell, n_output, cell_state_ptr, output_gate_scratch,</div>
<div class="line"><a id="l00357" name="l00357"></a><span class="lineno">  357</span>                           params-&gt;<a class="code hl_variable" href="structnnfw_1_1cker_1_1_l_s_t_m_params.html#ab3a9e171f2b52072433b8da3935ab740">activation</a>, projection_weights_ptr, projection_bias_ptr,</div>
<div class="line"><a id="l00358" name="l00358"></a><span class="lineno">  358</span>                           params-&gt;<a class="code hl_variable" href="structnnfw_1_1cker_1_1_l_s_t_m_params.html#a3d44ac1967b70bcb414a130952a47a0f">proj_clip</a>, output_state_ptr, scratch2);</div>
<div class="line"><a id="l00359" name="l00359"></a><span class="lineno">  359</span>  <span class="comment">// Copy output state to the output. Note that the output&#39;s rows may not be</span></div>
<div class="line"><a id="l00360" name="l00360"></a><span class="lineno">  360</span>  <span class="comment">// contiguous (output_batch_leading_dim != n_output).</span></div>
<div class="line"><a id="l00361" name="l00361"></a><span class="lineno">  361</span>  <span class="keywordflow">for</span> (<span class="keywordtype">int</span> b = 0; b &lt; n_batch; b++)</div>
<div class="line"><a id="l00362" name="l00362"></a><span class="lineno">  362</span>  {</div>
<div class="line"><a id="l00363" name="l00363"></a><span class="lineno">  363</span>    std::copy_n(output_state_ptr + b * n_output, n_output,</div>
<div class="line"><a id="l00364" name="l00364"></a><span class="lineno">  364</span>                output_ptr + b * output_batch_leading_dim);</div>
<div class="line"><a id="l00365" name="l00365"></a><span class="lineno">  365</span>  }</div>
<div class="line"><a id="l00366" name="l00366"></a><span class="lineno">  366</span>}</div>
<div class="line"><a id="l00367" name="l00367"></a><span class="lineno">  367</span> </div>
<div class="line"><a id="l00368" name="l00368"></a><span class="lineno">  368</span>} <span class="comment">// namespace cker</span></div>
<div class="line"><a id="l00369" name="l00369"></a><span class="lineno">  369</span>} <span class="comment">// namespace nnfw</span></div>
<div class="line"><a id="l00370" name="l00370"></a><span class="lineno">  370</span> </div>
<div class="line"><a id="l00371" name="l00371"></a><span class="lineno">  371</span><span class="preprocessor">#endif </span><span class="comment">// __NNFW_CKER_UNIDIRECTIONALSEQUENCELSTM_H__</span></div>
<div class="ttc" id="acompute_2cker_2include_2cker_2_tensor_utils_8h_html"><div class="ttname"><a href="compute_2cker_2include_2cker_2_tensor_utils_8h.html">TensorUtils.h</a></div></div>
<div class="ttc" id="acompute_2cker_2include_2cker_2_types_8h_html"><div class="ttname"><a href="compute_2cker_2include_2cker_2_types_8h.html">Types.h</a></div></div>
<div class="ttc" id="anamespacennfw_1_1cker_html_a14a817bb0312d18594b6b18cf0a73138"><div class="ttname"><a href="namespacennfw_1_1cker.html#a14a817bb0312d18594b6b18cf0a73138">nnfw::cker::VectorVectorCwiseProduct</a></div><div class="ttdeci">void VectorVectorCwiseProduct(const T *__restrict__ vector1, const T *__restrict__ vector2, int v_size, T *__restrict__ result)</div><div class="ttdef"><b>Definition:</b> <a href="compute_2cker_2include_2cker_2_tensor_utils_8h_source.html#l00052">TensorUtils.h:52</a></div></div>
<div class="ttc" id="anamespacennfw_1_1cker_html_a31790e9290fa11905b4c78808b82da55"><div class="ttname"><a href="namespacennfw_1_1cker.html#a31790e9290fa11905b4c78808b82da55">nnfw::cker::MeanStddevNormalization</a></div><div class="ttdeci">void MeanStddevNormalization(const float *input_vector, float *output_vector, int v_size, int n_batch)</div><div class="ttdef"><b>Definition:</b> <a href="compute_2cker_2include_2cker_2_tensor_utils_8h_source.html#l00154">TensorUtils.h:154</a></div></div>
<div class="ttc" id="anamespacennfw_1_1cker_html_a3bbf78c00591ce015db4b9e01772b17f"><div class="ttname"><a href="namespacennfw_1_1cker.html#a3bbf78c00591ce015db4b9e01772b17f">nnfw::cker::VectorBatchVectorCwiseProduct</a></div><div class="ttdeci">void VectorBatchVectorCwiseProduct(const T *vector, int v_size, const T *batch_vector, int n_batch, T *result)</div><div class="ttdef"><b>Definition:</b> <a href="compute_2cker_2include_2cker_2_tensor_utils_8h_source.html#l00076">TensorUtils.h:76</a></div></div>
<div class="ttc" id="anamespacennfw_1_1cker_html_a51ef48d1448691d903c3204a0d3f91ca"><div class="ttname"><a href="namespacennfw_1_1cker.html#a51ef48d1448691d903c3204a0d3f91ca">nnfw::cker::Sub1Vector</a></div><div class="ttdeci">void Sub1Vector(const float *vector, int v_size, float *result)</div><div class="ttdef"><b>Definition:</b> <a href="compute_2cker_2include_2cker_2_tensor_utils_8h_source.html#l00115">TensorUtils.h:115</a></div></div>
<div class="ttc" id="anamespacennfw_1_1cker_html_a537a30edc4be725f28ebdd5c2b123c0b"><div class="ttname"><a href="namespacennfw_1_1cker.html#a537a30edc4be725f28ebdd5c2b123c0b">nnfw::cker::MatrixBatchVectorMultiplyAccumulate</a></div><div class="ttdeci">void MatrixBatchVectorMultiplyAccumulate(const int8_t *matrix, const int m_rows, const int m_cols, const int8_t *vector, const float *scaling_factors, int n_batch, float *result, int result_stride)</div><div class="ttdef"><b>Definition:</b> <a href="compute_2cker_2include_2cker_2_tensor_utils_8h_source.html#l00127">TensorUtils.h:127</a></div></div>
<div class="ttc" id="anamespacennfw_1_1cker_html_a85cc8caa28492b796ed31184533005d7"><div class="ttname"><a href="namespacennfw_1_1cker.html#a85cc8caa28492b796ed31184533005d7">nnfw::cker::ApplyActivationToVector</a></div><div class="ttdeci">void ApplyActivationToVector(const float *vector, int v_size, FusedActivationFunctionType activation, float *result)</div><div class="ttdef"><b>Definition:</b> <a href="compute_2cker_2include_2cker_2_tensor_utils_8h_source.html#l00109">TensorUtils.h:109</a></div></div>
<div class="ttc" id="anamespacennfw_1_1cker_html_a964f3c61b8ff5306670b7432fdb5334c"><div class="ttname"><a href="namespacennfw_1_1cker.html#a964f3c61b8ff5306670b7432fdb5334c">nnfw::cker::CalculateLstmOutputFloat</a></div><div class="ttdeci">void CalculateLstmOutputFloat(int n_batch, int n_cell, int n_output, const float *cell_state, const float *output_gate, FusedActivationFunctionType activation, const float *projection_weights, const float *projection_bias, const float proj_clip, float *output_state, float *scratch)</div><div class="ttdef"><b>Definition:</b> <a href="compute_2cker_2include_2cker_2operation_2_l_s_t_m_8h_source.html#l00183">LSTM.h:183</a></div></div>
<div class="ttc" id="anamespacennfw_1_1cker_html_a96a47995ad33b9e3aef254cd666f0d1b"><div class="ttname"><a href="namespacennfw_1_1cker.html#a96a47995ad33b9e3aef254cd666f0d1b">nnfw::cker::UpdateLstmCellFloat</a></div><div class="ttdeci">void UpdateLstmCellFloat(int n_batch, int n_cell, float *cell_state, const float *input_gate, float *forget_gate, const float *cell_gate, bool use_cifg, float clip)</div><div class="ttdef"><b>Definition:</b> <a href="compute_2cker_2include_2cker_2operation_2_l_s_t_m_8h_source.html#l00135">LSTM.h:135</a></div></div>
<div class="ttc" id="anamespacennfw_1_1cker_html_a99b2798fb7403b592e8b860ddf97b937"><div class="ttname"><a href="namespacennfw_1_1cker.html#a99b2798fb7403b592e8b860ddf97b937">nnfw::cker::VectorBatchVectorAssign</a></div><div class="ttdeci">void VectorBatchVectorAssign(const float *vector, int v_size, int n_batch, float *batch_vector)</div><div class="ttdef"><b>Definition:</b> <a href="compute_2cker_2include_2cker_2_tensor_utils_8h_source.html#l00044">TensorUtils.h:44</a></div></div>
<div class="ttc" id="anamespacennfw_1_1cker_html_acfbd4ad2e41a2fded6cd669424e07fe5"><div class="ttname"><a href="namespacennfw_1_1cker.html#acfbd4ad2e41a2fded6cd669424e07fe5">nnfw::cker::CwiseClipping</a></div><div class="ttdeci">void CwiseClipping(float *vector, const int v_size, const float clipping_value)</div><div class="ttdef"><b>Definition:</b> <a href="compute_2cker_2include_2cker_2_tensor_utils_8h_source.html#l00034">TensorUtils.h:34</a></div></div>
<div class="ttc" id="anamespacennfw_1_1cker_html_ad0b7e90f00bda647bed28b59caed8927"><div class="ttname"><a href="namespacennfw_1_1cker.html#ad0b7e90f00bda647bed28b59caed8927">nnfw::cker::VectorVectorCwiseProductAccumulate</a></div><div class="ttdeci">void VectorVectorCwiseProductAccumulate(const T *__restrict__ vector1, const T *__restrict__ vector2, int v_size, T *__restrict__ result)</div><div class="ttdef"><b>Definition:</b> <a href="compute_2cker_2include_2cker_2_tensor_utils_8h_source.html#l00064">TensorUtils.h:64</a></div></div>
<div class="ttc" id="anamespacennfw_1_1cker_html_ad2604d27d5e99cf215fd7f2553e658a8"><div class="ttname"><a href="namespacennfw_1_1cker.html#ad2604d27d5e99cf215fd7f2553e658a8">nnfw::cker::VectorBatchVectorAdd</a></div><div class="ttdeci">void VectorBatchVectorAdd(const float *vector, int v_size, int n_batch, float *batch_vector)</div><div class="ttdef"><b>Definition:</b> <a href="compute_2cker_2include_2cker_2_tensor_utils_8h_source.html#l00039">TensorUtils.h:39</a></div></div>
<div class="ttc" id="anamespacennfw_1_1cker_html_ae60dea0f6844a22a3ed03e20d8a00e78"><div class="ttname"><a href="namespacennfw_1_1cker.html#ae60dea0f6844a22a3ed03e20d8a00e78">nnfw::cker::CalculateLstmGateFloat</a></div><div class="ttdeci">void CalculateLstmGateFloat(const float *input, const float *input_to_gate_weights, const float *aux_input, const float *aux_input_to_gate_weights, const float *output_state, const float *recurrent_to_gate_weights, const float *cell_state, const float *cell_to_gate_weights, const float *layer_norm_coefficients, const float *gate_bias, const int n_batch, const int n_input, const int n_aux_input, const int n_output, const int n_cell, const FusedActivationFunctionType activation, float *gate, const bool is_input_all_zeros, const bool is_aux_input_all_zeros)</div><div class="ttdef"><b>Definition:</b> <a href="compute_2cker_2include_2cker_2operation_2_l_s_t_m_8h_source.html#l00062">LSTM.h:62</a></div></div>
<div class="ttc" id="anamespacennfw_1_1cker_html_ae6c06b361e57b13dd187ca45ed5fc737"><div class="ttname"><a href="namespacennfw_1_1cker.html#ae6c06b361e57b13dd187ca45ed5fc737">nnfw::cker::LstmStepFloat</a></div><div class="ttdeci">void LstmStepFloat(const float *input_ptr, const float *input_to_input_weights_ptr, const float *input_to_forget_weights_ptr, const float *input_to_cell_weights_ptr, const float *input_to_output_weights_ptr, const float *aux_input_ptr, const float *aux_input_to_input_weights_ptr, const float *aux_input_to_forget_weights_ptr, const float *aux_input_to_cell_weights_ptr, const float *aux_input_to_output_weights_ptr, const float *recurrent_to_input_weights_ptr, const float *recurrent_to_forget_weights_ptr, const float *recurrent_to_cell_weights_ptr, const float *recurrent_to_output_weights_ptr, const float *cell_to_input_weights_ptr, const float *cell_to_forget_weights_ptr, const float *cell_to_output_weights_ptr, const float *input_layer_norm_coefficients_ptr, const float *forget_layer_norm_coefficients_ptr, const float *cell_layer_norm_coefficients_ptr, const float *output_layer_norm_coefficients_ptr, const float *input_gate_bias_ptr, const float *forget_gate_bias_ptr, const float *cell_gate_bias_ptr, const float *output_gate_bias_ptr, const float *projection_weights_ptr, const float *projection_bias_ptr, const LSTMParams *params, int n_batch, int n_cell, int n_input, int n_aux_input, int n_output, int output_batch_leading_dim, float *output_state_ptr, float *cell_state_ptr, float *scratch0, float *scratch1, float *scratch2, float *scratch3, float *output_ptr)</div><div class="ttdef"><b>Definition:</b> <a href="compute_2cker_2include_2cker_2operation_2_l_s_t_m_8h_source.html#l00285">LSTM.h:285</a></div></div>
<div class="ttc" id="anamespacennfw_1_1cker_html_af09a114337895158c72fde6226b568a2"><div class="ttname"><a href="namespacennfw_1_1cker.html#af09a114337895158c72fde6226b568a2">nnfw::cker::FusedActivationFunctionType</a></div><div class="ttdeci">FusedActivationFunctionType</div><div class="ttdef"><b>Definition:</b> <a href="compute_2cker_2include_2cker_2_types_8h_source.html#l00031">Types.h:32</a></div></div>
<div class="ttc" id="anamespacennfw_1_1cker_html_af09a114337895158c72fde6226b568a2ac00732693e14261bf9c2a4612a7f9bf9"><div class="ttname"><a href="namespacennfw_1_1cker.html#af09a114337895158c72fde6226b568a2ac00732693e14261bf9c2a4612a7f9bf9">nnfw::cker::FusedActivationFunctionType::kSigmoid</a></div><div class="ttdeci">@ kSigmoid</div></div>
<div class="ttc" id="anamespacennfw_1_1cker_html_af82fa9f0cdbd15fed59567999835cebf"><div class="ttname"><a href="namespacennfw_1_1cker.html#af82fa9f0cdbd15fed59567999835cebf">nnfw::cker::IsZeroVector</a></div><div class="ttdeci">bool IsZeroVector(const float *vector, int v_size)</div><div class="ttdef"><b>Definition:</b> <a href="compute_2cker_2include_2cker_2_tensor_utils_8h_source.html#l00104">TensorUtils.h:104</a></div></div>
<div class="ttc" id="anamespacennfw_1_1cker_html_afb7760d00eca9875e4974b51e4874de5"><div class="ttname"><a href="namespacennfw_1_1cker.html#afb7760d00eca9875e4974b51e4874de5">nnfw::cker::VectorBatchVectorCwiseProductAccumulate</a></div><div class="ttdeci">void VectorBatchVectorCwiseProductAccumulate(const T *vector, int v_size, const T *batch_vector, int n_batch, T *result)</div><div class="ttdef"><b>Definition:</b> <a href="compute_2cker_2include_2cker_2_tensor_utils_8h_source.html#l00092">TensorUtils.h:92</a></div></div>
<div class="ttc" id="anamespacennfw_html"><div class="ttname"><a href="namespacennfw.html">nnfw</a></div><div class="ttdef"><b>Definition:</b> <a href="topk__v2_8h_source.html#l00029">topk_v2.h:30</a></div></div>
<div class="ttc" id="astructnnfw_1_1cker_1_1_l_s_t_m_params_html"><div class="ttname"><a href="structnnfw_1_1cker_1_1_l_s_t_m_params.html">nnfw::cker::LSTMParams</a></div><div class="ttdef"><b>Definition:</b> <a href="compute_2cker_2include_2cker_2_types_8h_source.html#l00279">Types.h:280</a></div></div>
<div class="ttc" id="astructnnfw_1_1cker_1_1_l_s_t_m_params_html_a0138a9307034d1fbd0f44aae19d29151"><div class="ttname"><a href="structnnfw_1_1cker_1_1_l_s_t_m_params.html#a0138a9307034d1fbd0f44aae19d29151">nnfw::cker::LSTMParams::cell_clip</a></div><div class="ttdeci">float cell_clip</div><div class="ttdef"><b>Definition:</b> <a href="compute_2cker_2include_2cker_2_types_8h_source.html#l00283">Types.h:283</a></div></div>
<div class="ttc" id="astructnnfw_1_1cker_1_1_l_s_t_m_params_html_a3d44ac1967b70bcb414a130952a47a0f"><div class="ttname"><a href="structnnfw_1_1cker_1_1_l_s_t_m_params.html#a3d44ac1967b70bcb414a130952a47a0f">nnfw::cker::LSTMParams::proj_clip</a></div><div class="ttdeci">float proj_clip</div><div class="ttdef"><b>Definition:</b> <a href="compute_2cker_2include_2cker_2_types_8h_source.html#l00284">Types.h:284</a></div></div>
<div class="ttc" id="astructnnfw_1_1cker_1_1_l_s_t_m_params_html_ab3a9e171f2b52072433b8da3935ab740"><div class="ttname"><a href="structnnfw_1_1cker_1_1_l_s_t_m_params.html#ab3a9e171f2b52072433b8da3935ab740">nnfw::cker::LSTMParams::activation</a></div><div class="ttdeci">FusedActivationFunctionType activation</div><div class="ttdef"><b>Definition:</b> <a href="compute_2cker_2include_2cker_2_types_8h_source.html#l00282">Types.h:282</a></div></div>
</div><!-- fragment --></div><!-- contents -->
</div><!-- doc-content -->
<!-- start footer part -->
<div id="nav-path" class="navpath"><!-- id is needed for treeview function! -->
  <ul>
    <li class="navelem"><a class="el" href="dir_0aec905722638a09b3267c4de0fa0e08.html">compute</a></li><li class="navelem"><a class="el" href="dir_78746ab682e64df5250232aed9fd7b9e.html">cker</a></li><li class="navelem"><a class="el" href="dir_092da0971c33b154e8b1df41a3fbee69.html">include</a></li><li class="navelem"><a class="el" href="dir_639d7c65694fbfe036e3bb78b042dc9c.html">cker</a></li><li class="navelem"><a class="el" href="dir_4d6511aec63f0104ba12c7c66d891cab.html">operation</a></li><li class="navelem"><a class="el" href="compute_2cker_2include_2cker_2operation_2_l_s_t_m_8h.html">LSTM.h</a></li>
    <li class="footer">Generated by <a href="https://www.doxygen.org/index.html"><img class="footer" src="doxygen.svg" width="104" height="31" alt="doxygen"/></a> 1.9.5 </li>
  </ul>
</div>
</body>
</html>
