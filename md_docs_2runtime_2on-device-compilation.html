<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "https://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en-US">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=11"/>
<meta name="generator" content="Doxygen 1.9.8"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<title>ONE - On-device Neural Engine: On-Device Compilation</title>
<link href="tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="dynsections.js"></script>
<link href="navtree.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="resize.js"></script>
<script type="text/javascript" src="navtreedata.js"></script>
<script type="text/javascript" src="navtree.js"></script>
<link href="search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="search/searchdata.js"></script>
<script type="text/javascript" src="search/search.js"></script>
<link href="doxygen.css" rel="stylesheet" type="text/css" />
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr id="projectrow">
  <td id="projectalign">
   <div id="projectname">ONE - On-device Neural Engine
   </div>
  </td>
 </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.9.8 -->
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:d3d9a9a6595521f9666a5e94cc830dab83b65699&amp;dn=expat.txt MIT */
var searchBox = new SearchBox("searchBox", "search/",'.html');
/* @license-end */
</script>
<script type="text/javascript" src="menudata.js"></script>
<script type="text/javascript" src="menu.js"></script>
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:d3d9a9a6595521f9666a5e94cc830dab83b65699&amp;dn=expat.txt MIT */
$(function() {
  initMenu('',true,false,'search.php','Search');
  $(document).ready(function() { init_search(); });
});
/* @license-end */
</script>
<div id="main-nav"></div>
</div><!-- top -->
<div id="side-nav" class="ui-resizable side-nav-resizable">
  <div id="nav-tree">
    <div id="nav-tree-contents">
      <div id="nav-sync" class="sync"></div>
    </div>
  </div>
  <div id="splitbar" style="-moz-user-select:none;" 
       class="ui-resizable-handle">
  </div>
</div>
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:d3d9a9a6595521f9666a5e94cc830dab83b65699&amp;dn=expat.txt MIT */
$(document).ready(function(){initNavTree('md_docs_2runtime_2on-device-compilation.html',''); initResizable(); });
/* @license-end */
</script>
<div id="doc-content">
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<div id="MSearchResults">
<div class="SRPage">
<div id="SRIndex">
<div id="SRResults"></div>
<div class="SRStatus" id="Loading">Loading...</div>
<div class="SRStatus" id="Searching">Searching...</div>
<div class="SRStatus" id="NoMatches">No Matches</div>
</div>
</div>
</div>
</div>

<div><div class="header">
  <div class="headertitle"><div class="title">On-Device Compilation</div></div>
</div><!--header-->
<div class="contents">
<div class="textblock"><p>ONERT supports on-device compilation - on-device quantization and on-device code generation.</p>
<h1>On-device quantization</h1>
<p>ONERT supports on-device quantization for the <b>float32</b> model. On-device quantization has two mode - full quantization and weight-only quantization.</p>
<h2>Weight-only quantization</h2>
<p>Weight-only quantization quantizes only weights of the model. The activation is still in float32 precision. This mode is useful when the model size reduction is more important than the inference speedup.</p>
<p>For weight-only quantization, follow below steps:</p><ul>
<li>Load float32 model</li>
<li>Set quantization type for weight-only quantization</li>
<li>Set path to save quantized model</li>
<li>Call quantize API to perform quantization</li>
</ul>
<div class="fragment"><div class="line"><span class="comment">// Load float32 model</span></div>
<div class="line"><a class="code hl_function" href="onert-micro_8h.html#a6c5b342754b564e4dbec4023206fd8ea">nnfw_load_model_from_file</a>(session, pkg_path);</div>
<div class="line"> </div>
<div class="line"><span class="comment">// Set quantization type: weight-only, symmetric, int8</span></div>
<div class="line"><a class="code hl_function" href="nnfw__experimental_8h.html#a7c18c5f0f8b1746fb5016bb696d215b6">nnfw_set_quantization_type</a>(session, <a class="code hl_enumvalue" href="nnfw__experimental_8h.html#aa01eff0d1a154931c2f7e071f1b0e2e7ae1c740bda073b335cfa9353b5170df10">NNFW_QUANTIZE_TYPE_WO_I8_SYM</a>);</div>
<div class="line"> </div>
<div class="line"><span class="comment">// Set path to save quantized model</span></div>
<div class="line"><a class="code hl_function" href="nnfw__experimental_8h.html#ae1bb47075976b3334e5df27e4110a9c0">nnfw_set_quantized_model_path</a>(session, quantized_model_path);</div>
<div class="line"> </div>
<div class="line"><span class="comment">// Quantize model</span></div>
<div class="line"><a class="code hl_function" href="nnfw__experimental_8h.html#aa4046ec782673ee90f6ab2dca1a317d9">nnfw_quantize</a>(session);</div>
<div class="line"> </div>
<div class="line"><span class="comment">// Run model for inference with quantized model</span></div>
<div class="line"><a class="code hl_function" href="nnfw_8h.html#a8e193702b11c77db6580253a4ae8cd1f">nnfw_run</a>(session);</div>
<div class="ttc" id="annfw_8h_html_a8e193702b11c77db6580253a4ae8cd1f"><div class="ttname"><a href="nnfw_8h.html#a8e193702b11c77db6580253a4ae8cd1f">nnfw_run</a></div><div class="ttdeci">NNFW_STATUS nnfw_run(nnfw_session *session)</div><div class="ttdoc">Run inference.</div><div class="ttdef"><b>Definition</b> <a href="nnfw__api_8cc_source.html#l00078">nnfw_api.cc:78</a></div></div>
<div class="ttc" id="annfw__experimental_8h_html_a7c18c5f0f8b1746fb5016bb696d215b6"><div class="ttname"><a href="nnfw__experimental_8h.html#a7c18c5f0f8b1746fb5016bb696d215b6">nnfw_set_quantization_type</a></div><div class="ttdeci">NNFW_STATUS nnfw_set_quantization_type(nnfw_session *session, NNFW_QUANTIZE_TYPE qtype)</div><div class="ttdoc">Set quantization type.</div><div class="ttdef"><b>Definition</b> <a href="nnfw__api_8cc_source.html#l00335">nnfw_api.cc:335</a></div></div>
<div class="ttc" id="annfw__experimental_8h_html_aa01eff0d1a154931c2f7e071f1b0e2e7ae1c740bda073b335cfa9353b5170df10"><div class="ttname"><a href="nnfw__experimental_8h.html#aa01eff0d1a154931c2f7e071f1b0e2e7ae1c740bda073b335cfa9353b5170df10">NNFW_QUANTIZE_TYPE_WO_I8_SYM</a></div><div class="ttdeci">@ NNFW_QUANTIZE_TYPE_WO_I8_SYM</div><div class="ttdef"><b>Definition</b> <a href="nnfw__experimental_8h_source.html#l00511">nnfw_experimental.h:511</a></div></div>
<div class="ttc" id="annfw__experimental_8h_html_aa4046ec782673ee90f6ab2dca1a317d9"><div class="ttname"><a href="nnfw__experimental_8h.html#aa4046ec782673ee90f6ab2dca1a317d9">nnfw_quantize</a></div><div class="ttdeci">NNFW_STATUS nnfw_quantize(nnfw_session *session)</div><div class="ttdoc">Quantize circle model.</div><div class="ttdef"><b>Definition</b> <a href="nnfw__api_8cc_source.html#l00347">nnfw_api.cc:347</a></div></div>
<div class="ttc" id="annfw__experimental_8h_html_ae1bb47075976b3334e5df27e4110a9c0"><div class="ttname"><a href="nnfw__experimental_8h.html#ae1bb47075976b3334e5df27e4110a9c0">nnfw_set_quantized_model_path</a></div><div class="ttdeci">NNFW_STATUS nnfw_set_quantized_model_path(nnfw_session *session, const char *path)</div><div class="ttdoc">Set exported quantized model path.</div><div class="ttdef"><b>Definition</b> <a href="nnfw__api_8cc_source.html#l00341">nnfw_api.cc:341</a></div></div>
<div class="ttc" id="aonert-micro_8h_html_a6c5b342754b564e4dbec4023206fd8ea"><div class="ttname"><a href="onert-micro_8h.html#a6c5b342754b564e4dbec4023206fd8ea">nnfw_load_model_from_file</a></div><div class="ttdeci">NNFW_STATUS nnfw_load_model_from_file(nnfw_session *session, const char *package_file_path)</div><div class="ttdoc">Load model from nnpackage file or directory.</div><div class="ttdef"><b>Definition</b> <a href="onert-micro_8cpp_source.html#l00392">onert-micro.cpp:392</a></div></div>
</div><!-- fragment --><p>When the model is quantized, you can use the quantized model because the quantized model is loaded automatically after quantization. You don't need to load the quantized model explicitly.</p>
<h2>Full quantization</h2>
<p>Full quantization quantizes both weights and activations of the model. This mode is useful when specific runtime backend requires quantized model. To quantize activation, runtime should gather information about activation range during the execution of the model. Therefore, it needs to run the model enough times to get accurate activation range.</p>
<p>For full quantization, follow below steps:</p>
<ul>
<li>Load float32 model</li>
<li>Gather activation range by running the model multiple times<ul>
<li>Prepare model to run</li>
<li>Set input and output buffer(s)</li>
<li>Set execution configuration to gather activation range</li>
<li>Run model multiple times for inference with gathering activation range</li>
</ul>
</li>
<li>Quantize model if activation range is gathered enough<ul>
<li>Set quantization type for full quantization</li>
<li>Set path to save quantized model</li>
<li>Call quantize API to perform quantization</li>
</ul>
</li>
</ul>
<div class="fragment"><div class="line"><span class="comment">// Load float32 model</span></div>
<div class="line"><a class="code hl_function" href="onert-micro_8h.html#a6c5b342754b564e4dbec4023206fd8ea">nnfw_load_model_from_file</a>(session, pkg_path);</div>
<div class="line"> </div>
<div class="line"><span class="comment">// Prepare model to run</span></div>
<div class="line"><a class="code hl_function" href="nnfw_8h.html#a93da789a116a47c0cb66f26bdff12f24">nnfw_prepare</a>(session);</div>
<div class="line"> </div>
<div class="line"><span class="comment">// Set input and output buffer(s)</span></div>
<div class="line"><a class="code hl_function" href="nnfw_8h.html#a1043343aa0dc8fc8f1b09197a6b7249c">nnfw_set_input</a>(session, input_index, input_type, input_buffer, input_element_size);</div>
<div class="line"><a class="code hl_function" href="nnfw_8h.html#ac1af7b7a6c70fff801171904d9403dba">nnfw_set_output</a>(session, output_index, output_type, output_buffer, output_element_size);</div>
<div class="line"> </div>
<div class="line"><span class="comment">// Set execution configuration to gather activation range</span></div>
<div class="line"><a class="code hl_function" href="nnfw__experimental_8h.html#ac1121e4e6f07fc7dbda4466afbd3f670">nnfw_set_execute_config</a>(session, <a class="code hl_enumvalue" href="nnfw__experimental_8h.html#aa184da9db1560ca26dd7ee889bd1013dabfabb72f6003e307549c0d21b796d2f3">NNFW_RUN_CONFIG_DUMP_MINMAX</a>, <span class="keyword">nullptr</span>);</div>
<div class="line"> </div>
<div class="line"><span class="comment">// Run model multiple times for inference with gathering activation range</span></div>
<div class="line"><span class="keywordflow">for</span> (<span class="keywordtype">int</span> i = 0; i &lt; num_of_inference; ++i)</div>
<div class="line">{</div>
<div class="line">  <a class="code hl_function" href="nnfw_8h.html#a8e193702b11c77db6580253a4ae8cd1f">nnfw_run</a>(session);</div>
<div class="line">}</div>
<div class="line"> </div>
<div class="line"><span class="comment">// Set quantization type: full, asymmetric, uint8</span></div>
<div class="line"><a class="code hl_function" href="nnfw__experimental_8h.html#a7c18c5f0f8b1746fb5016bb696d215b6">nnfw_set_quantization_type</a>(session, <a class="code hl_enumvalue" href="nnfw__experimental_8h.html#aa01eff0d1a154931c2f7e071f1b0e2e7a481bdd8149fc4808e2df6e6e1b738e72">NNFW_QUANTIZE_TYPE_U8_ASYM</a>);</div>
<div class="line"> </div>
<div class="line"><span class="comment">// Set path to save quantized model</span></div>
<div class="line"><a class="code hl_function" href="nnfw__experimental_8h.html#ae1bb47075976b3334e5df27e4110a9c0">nnfw_set_quantized_model_path</a>(session, quantized_model_path);</div>
<div class="line"> </div>
<div class="line"><span class="comment">// Quantize model</span></div>
<div class="line"><a class="code hl_function" href="nnfw__experimental_8h.html#aa4046ec782673ee90f6ab2dca1a317d9">nnfw_quantize</a>(session);</div>
<div class="line"> </div>
<div class="line"><span class="comment">// Reset execution configuration to normal execution</span></div>
<div class="line"><a class="code hl_function" href="nnfw__experimental_8h.html#ab8ac04143e1db077ea615e463ea1ce38">nnfw_reset_execute_config</a>(session);</div>
<div class="line"> </div>
<div class="line"><span class="comment">// Run model for inference with quantized model</span></div>
<div class="line"><a class="code hl_function" href="nnfw_8h.html#a8e193702b11c77db6580253a4ae8cd1f">nnfw_run</a>(session);</div>
<div class="ttc" id="annfw_8h_html_a1043343aa0dc8fc8f1b09197a6b7249c"><div class="ttname"><a href="nnfw_8h.html#a1043343aa0dc8fc8f1b09197a6b7249c">nnfw_set_input</a></div><div class="ttdeci">NNFW_STATUS nnfw_set_input(nnfw_session *session, uint32_t index, NNFW_TYPE type, const void *buffer, size_t length)</div><div class="ttdoc">Set input buffer.</div><div class="ttdef"><b>Definition</b> <a href="nnfw__api_8cc_source.html#l00096">nnfw_api.cc:96</a></div></div>
<div class="ttc" id="annfw_8h_html_a93da789a116a47c0cb66f26bdff12f24"><div class="ttname"><a href="nnfw_8h.html#a93da789a116a47c0cb66f26bdff12f24">nnfw_prepare</a></div><div class="ttdeci">NNFW_STATUS nnfw_prepare(nnfw_session *session)</div><div class="ttdoc">Prepare session to be ready for inference.</div><div class="ttdef"><b>Definition</b> <a href="nnfw__api_8cc_source.html#l00072">nnfw_api.cc:72</a></div></div>
<div class="ttc" id="annfw_8h_html_ac1af7b7a6c70fff801171904d9403dba"><div class="ttname"><a href="nnfw_8h.html#ac1af7b7a6c70fff801171904d9403dba">nnfw_set_output</a></div><div class="ttdeci">NNFW_STATUS nnfw_set_output(nnfw_session *session, uint32_t index, NNFW_TYPE type, void *buffer, size_t length)</div><div class="ttdoc">Set output buffer.</div><div class="ttdef"><b>Definition</b> <a href="nnfw__api_8cc_source.html#l00103">nnfw_api.cc:103</a></div></div>
<div class="ttc" id="annfw__experimental_8h_html_aa01eff0d1a154931c2f7e071f1b0e2e7a481bdd8149fc4808e2df6e6e1b738e72"><div class="ttname"><a href="nnfw__experimental_8h.html#aa01eff0d1a154931c2f7e071f1b0e2e7a481bdd8149fc4808e2df6e6e1b738e72">NNFW_QUANTIZE_TYPE_U8_ASYM</a></div><div class="ttdeci">@ NNFW_QUANTIZE_TYPE_U8_ASYM</div><div class="ttdef"><b>Definition</b> <a href="nnfw__experimental_8h_source.html#l00507">nnfw_experimental.h:507</a></div></div>
<div class="ttc" id="annfw__experimental_8h_html_aa184da9db1560ca26dd7ee889bd1013dabfabb72f6003e307549c0d21b796d2f3"><div class="ttname"><a href="nnfw__experimental_8h.html#aa184da9db1560ca26dd7ee889bd1013dabfabb72f6003e307549c0d21b796d2f3">NNFW_RUN_CONFIG_DUMP_MINMAX</a></div><div class="ttdeci">@ NNFW_RUN_CONFIG_DUMP_MINMAX</div><div class="ttdef"><b>Definition</b> <a href="nnfw__experimental_8h_source.html#l00711">nnfw_experimental.h:711</a></div></div>
<div class="ttc" id="annfw__experimental_8h_html_ab8ac04143e1db077ea615e463ea1ce38"><div class="ttname"><a href="nnfw__experimental_8h.html#ab8ac04143e1db077ea615e463ea1ce38">nnfw_reset_execute_config</a></div><div class="ttdeci">NNFW_STATUS nnfw_reset_execute_config(nnfw_session *session)</div><div class="ttdoc">Reset execution (run or train) configurations.</div><div class="ttdef"><b>Definition</b> <a href="nnfw__api_8cc_source.html#l00406">nnfw_api.cc:406</a></div></div>
<div class="ttc" id="annfw__experimental_8h_html_ac1121e4e6f07fc7dbda4466afbd3f670"><div class="ttname"><a href="nnfw__experimental_8h.html#ac1121e4e6f07fc7dbda4466afbd3f670">nnfw_set_execute_config</a></div><div class="ttdeci">NNFW_STATUS nnfw_set_execute_config(nnfw_session *session, const NNFW_RUN_CONFIG key, const char *value)</div><div class="ttdoc">Set execution (run or train) configuration.</div><div class="ttdef"><b>Definition</b> <a href="nnfw__api_8cc_source.html#l00399">nnfw_api.cc:399</a></div></div>
</div><!-- fragment --><p>When the model is quantized, you can use the quantized model because the quantized model is loaded automatically after quantization. You don't need to load the quantized model explicitly. Also, you don't need to set input and output buffers for the quantized data type because runtime automatically casts input and output buffers data between float32 and quantized data type. But you can set input and output buffers for the quantized data type after model full quantization if you want to use them directly without data casting.</p>
<h1>On-device code generation</h1>
<p>ONE supports on-device code generation. On-device code generation generates backend-specific code from the model and saves it as a supported file format. This feature is useful when the backend requires a specific precompiled model file format.</p>
<h2>Prerequisites</h2>
<p>To use on-device code generation, you need to install plugin that supports on-device code generation. On-device code generation plugin must fulfill interface defined in <code>ICodegen.h</code>.</p>
<p>Plugin should be installed in <code>{libdir}/nnfw/codegen</code> with <code>lib&lt;filetype&gt;-gen.so</code> name pattern. For example, if your plugin generates file with <code>.abc</code> extension, then plugin library should be named <code>libabc-gen.so</code>.</p>
<h2>Usage</h2>
<p>To generate code, follow below steps:</p>
<ul>
<li>Load model</li>
<li>(Optional) Set path to save generated code<ul>
<li>If path is not set, generated code will be saved in same directory with model with same name but target name extension</li>
</ul>
</li>
<li>Call generate_code API to perform code generation</li>
</ul>
<div class="fragment"><div class="line"><span class="comment">// Load model</span></div>
<div class="line"><a class="code hl_function" href="onert-micro_8h.html#a6c5b342754b564e4dbec4023206fd8ea">nnfw_load_model_from_file</a>(session, pkg_path);</div>
<div class="line"> </div>
<div class="line"><span class="comment">// (Optional) Set path to save generated code</span></div>
<div class="line"><span class="comment">// nnfw_set_codegen_model_path(session, codegen_model_path);</span></div>
<div class="line"> </div>
<div class="line"><span class="comment">// Generate code for target backend: target codegen plugin name is &quot;abc&quot; (installed as `libabc-gen.lib`)</span></div>
<div class="line"><a class="code hl_function" href="nnfw__experimental_8h.html#a9c434de04d0c57bf7e28a1c7721bdd49">nnfw_codegen</a>(session, <span class="stringliteral">&quot;abc-gen&quot;</span>, <a class="code hl_enumvalue" href="nnfw__experimental_8h.html#a6b579b96cb876a738cb8ccfd174c0b19a53cc66fab85eab846a8d44da4d743fac">NNFW_CODEGEN_PREF_DEFAULT</a>);</div>
<div class="line"> </div>
<div class="line"><span class="comment">// Prepare model to run</span></div>
<div class="line"><a class="code hl_function" href="nnfw_8h.html#a93da789a116a47c0cb66f26bdff12f24">nnfw_prepare</a>(session);</div>
<div class="line"> </div>
<div class="line"><span class="comment">// Set backend to use generated code on specific target backend if need</span></div>
<div class="line">nnfw_set_available_backend(session, <span class="stringliteral">&quot;abc&quot;</span>);</div>
<div class="line"> </div>
<div class="line"><span class="comment">// Set input and output buffer(s)</span></div>
<div class="line"><a class="code hl_function" href="nnfw_8h.html#a1043343aa0dc8fc8f1b09197a6b7249c">nnfw_set_input</a>(session, input_index, input_type, input_buffer, input_element_size);</div>
<div class="line"><a class="code hl_function" href="nnfw_8h.html#ac1af7b7a6c70fff801171904d9403dba">nnfw_set_output</a>(session, output_index, output_type, output_buffer, output_element_size);</div>
<div class="line"> </div>
<div class="line"><span class="comment">// Run model</span></div>
<div class="line"><a class="code hl_function" href="nnfw_8h.html#a8e193702b11c77db6580253a4ae8cd1f">nnfw_run</a>(session);</div>
<div class="ttc" id="annfw__experimental_8h_html_a6b579b96cb876a738cb8ccfd174c0b19a53cc66fab85eab846a8d44da4d743fac"><div class="ttname"><a href="nnfw__experimental_8h.html#a6b579b96cb876a738cb8ccfd174c0b19a53cc66fab85eab846a8d44da4d743fac">NNFW_CODEGEN_PREF_DEFAULT</a></div><div class="ttdeci">@ NNFW_CODEGEN_PREF_DEFAULT</div><div class="ttdef"><b>Definition</b> <a href="nnfw__experimental_8h_source.html#l00557">nnfw_experimental.h:557</a></div></div>
<div class="ttc" id="annfw__experimental_8h_html_a9c434de04d0c57bf7e28a1c7721bdd49"><div class="ttname"><a href="nnfw__experimental_8h.html#a9c434de04d0c57bf7e28a1c7721bdd49">nnfw_codegen</a></div><div class="ttdeci">NNFW_STATUS nnfw_codegen(nnfw_session *session, const char *target, NNFW_CODEGEN_PREF pref)</div><div class="ttdoc">Generate target-dependent code.</div><div class="ttdef"><b>Definition</b> <a href="nnfw__api_8cc_source.html#l00359">nnfw_api.cc:359</a></div></div>
</div><!-- fragment --><h1>Collaboration on-device quantization and code generation</h1>
<p>On-device quantization and code generation can be used together when target backend requires quantized model and specific precompiled model file format.</p>
<h1>Test tool support</h1>
<p>On-device compilation is supported in test tools <code>onert_run</code></p>
<h2>Quantization</h2>
<p>Example: weight-only quantization</p><ul>
<li>Input file: <code>test.circle</code></li>
<li>Quantization type: weight-only, symmetric, int8</li>
<li>Output file: <code>test.q.circle</code></li>
</ul>
<div class="fragment"><div class="line">$ onert_run --quantize int8_wo \</div>
<div class="line">    --qpath test.q.circle \</div>
<div class="line">    test.circle</div>
</div><!-- fragment --><p>Example: full quantization</p><ul>
<li>Input file: <code>test.circle</code></li>
<li>Quantization type: full, asymmetric, uint8</li>
<li>Output file: <code>test.q.circle</code></li>
<li>Number of inference to gather activation range: 10</li>
</ul>
<div class="fragment"><div class="line">$ onert_run -- quantize uint8 \</div>
<div class="line">    --qpath test.q.circle \</div>
<div class="line">    --minmax_run 10 \</div>
<div class="line">    test.circle</div>
</div><!-- fragment --><h2>Code generation</h2>
<p>Example</p><ul>
<li>Input file: <code>test.circle</code></li>
<li>Target backend: <code>abc_back</code></li>
<li>Target plugin name: <code>abc</code></li>
<li>Output file: <code>test.abc</code></li>
</ul>
<div class="fragment"><div class="line">$ BACKENDS=&#39;abc_back&#39; onert_run --codegen abc-gen \</div>
<div class="line">    --cpath test.abc \</div>
<div class="line">    test.circle</div>
</div><!-- fragment --><h2>Quantization and code generation</h2>
<p>Example</p><ul>
<li>Input file: <code>test.circle</code></li>
<li>Quantization type: full, asymmetric, uint8</li>
<li>Number of inference to gather activation range: 10</li>
<li>Quantized model file: <code>test.q.circle</code></li>
<li>Target backend: <code>abc_back</code></li>
<li>Target plugin name: <code>abc</code></li>
<li>Codegen output file: <code>test.abc</code></li>
</ul>
<p>``&lsquo;sh $ BACKENDS='abc_back&rsquo; onert_run &ndash;quantize uint8 \ &ndash;qpath test.q.circle \ &ndash;minmax_run 10 \ &ndash;codegen abc-gen \ &ndash;cpath test.abc \ test.circle </p>
</div></div><!-- contents -->
</div><!-- PageDoc -->
</div><!-- doc-content -->
<!-- start footer part -->
<div id="nav-path" class="navpath"><!-- id is needed for treeview function! -->
  <ul>
    <li class="footer">Generated by <a href="https://www.doxygen.org/index.html"><img class="footer" src="doxygen.svg" width="104" height="31" alt="doxygen"/></a> 1.9.8 </li>
  </ul>
</div>
</body>
</html>
