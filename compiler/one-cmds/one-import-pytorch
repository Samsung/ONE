#!/usr/bin/env bash
''''export SCRIPT_PATH="$(cd "$(dirname "$(readlink -f "${BASH_SOURCE[0]}")")" && pwd)" # '''
''''export PY_PATH=${SCRIPT_PATH}/venv/bin/python                                       # '''
''''test -f ${PY_PATH} && exec ${PY_PATH} "$0" "$@"                                     # '''
''''echo "Error: Virtual environment not found. Please run 'one-prepare-venv' command." # '''
''''exit 255                                                                            # '''

# Copyright (c) 2021 Samsung Electronics Co., Ltd. All Rights Reserved
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#    http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

import argparse
import importlib.machinery
import importlib.util
import inspect
import os
import sys
import tempfile
import torch
import onnx
import onnx_tf
import json
import zipfile

import onnx_legalizer
import onelib.make_cmd as _make_cmd
import onelib.utils as oneutils

# TODO Find better way to suppress trackback on error
sys.tracebacklimit = 0


def get_driver_spec():
    return ("one-import-pytorch", oneutils.DriverType.IMPORTER)


def _get_parser():
    parser = argparse.ArgumentParser(
        description='command line tool to convert PyTorch to Circle')

    oneutils.add_default_arg(parser)

    ## converter arguments
    converter_group = parser.add_argument_group('converter arguments')

    # input and output path.
    converter_group.add_argument('-i',
                                 '--input_path',
                                 type=str,
                                 help='full filepath of the input file')
    converter_group.add_argument('-p',
                                 '--python_path',
                                 type=str,
                                 help='full filepath of the python model file')
    converter_group.add_argument('-o',
                                 '--output_path',
                                 type=str,
                                 help='full filepath of the output file')

    # input arrays.
    converter_group.add_argument(
        '-s',
        '--input_shapes',
        type=str,
        help=
        'Shapes corresponding to --input_arrays, colon-separated.(ex:\"1,4,4,3:1,20,20,3\")'
    )
    converter_group.add_argument(
        '-t',
        '--input_types',
        type=str,
        help='data types of input tensors, colon-separated (ex: float32, uint8, int32)')

    # fixed options
    tf2tflite_group = parser.add_argument_group('tf2tfliteV2 arguments')
    tf2tflite_group.add_argument('--model_format', default='saved_model')
    tf2tflite_group.add_argument('--converter_version', default='v2')

    parser.add_argument('--unroll_rnn', action='store_true', help='Unroll RNN operators')
    parser.add_argument('--unroll_lstm',
                        action='store_true',
                        help='Unroll LSTM operators')

    # save intermediate file(s)
    parser.add_argument('--save_intermediate',
                        action='store_true',
                        help='Save intermediate files to output folder')

    return parser


def _verify_arg(parser, args):
    """verify given arguments"""
    # check if required arguments is given
    missing = []
    if not oneutils.is_valid_attr(args, 'input_path'):
        missing.append('-i/--input_path')
    if not oneutils.is_valid_attr(args, 'output_path'):
        missing.append('-o/--output_path')
    if not oneutils.is_valid_attr(args, 'input_shapes'):
        missing.append('-s/--input_shapes')
    if not oneutils.is_valid_attr(args, 'input_types'):
        missing.append('-t/--input_types')

    if len(missing):
        parser.error('the following arguments are required: ' + ' '.join(missing))


def _parse_arg(parser):
    args = parser.parse_args()
    # print version
    if args.version:
        oneutils.print_version_and_exit(__file__)

    return args


def _apply_verbosity(verbosity):
    # NOTE
    # TF_CPP_MIN_LOG_LEVEL
    #   0 : INFO + WARNING + ERROR + FATAL
    #   1 : WARNING + ERROR + FATAL
    #   2 : ERROR + FATAL
    #   3 : FATAL
    if verbosity:
        os.environ['TF_CPP_MIN_LOG_LEVEL'] = '0'
    else:
        os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'


def _parse_shapes(shapes_str):
    shapes = []
    for shape_str in shapes_str.split(":"):
        if shape_str != "":
            shapes += [list(map(int, shape_str.split(",")))]
        else:
            shapes += [[]]
    return shapes


def _parse_types(types_str):
    # There are no convenient way to create torch from string ot numpy dtype, so using this workaround
    dtype_dict = {
        "bool": torch.bool,
        "uint8": torch.uint8,
        "int8": torch.int8,
        "int16": torch.int16,
        "int32": torch.int32,
        "int64": torch.int64,
        "float16": torch.float16,
        "float32": torch.float32,
        "float64": torch.float64,
        "complex64": torch.complex64,
        "complex128": torch.complex128
    }
    array = types_str.split(",")
    types = [dtype_dict[type_str.strip()] for type_str in array]
    return types


# merge contents of module into global namespace
def _merge_module(module):
    # is there an __all__?  if so respect it
    if "__all__" in module.__dict__:
        names = module.__dict__["__all__"]
    else:
        # otherwise we import all names that don't begin with _
        names = [x for x in module.__dict__ if not x.startswith("_")]
    globals().update({k: getattr(module, k) for k in names})


def _list_classes_from_module(module):
    # Parsing the module to get all defined classes
    is_member = lambda member: inspect.isclass(member
                                               ) and member.__module__ == module.__name__
    classes = [cls[1] for cls in inspect.getmembers(module, is_member)]
    return classes


def _extract_pytorch_model(log_file, parameters_path, python_path):
    log_file.write(('Trying to load saved model\n').encode())
    python_model_path = os.path.abspath(python_path)
    module_name = os.path.basename(python_model_path)
    module_dir = os.path.dirname(python_model_path)
    sys.path.append(module_dir)
    log_file.write(('Trying to load given python module\n').encode())
    module_loader = importlib.machinery.SourceFileLoader(module_name, python_model_path)
    module_spec = importlib.util.spec_from_loader(module_name, module_loader)
    python_model_module = importlib.util.module_from_spec(module_spec)

    try:
        module_loader.exec_module(python_model_module)
    except:
        raise ValueError('Failed to execute given python model file')

    log_file.write(('Model python module is loaded\n').encode())
    try:
        # this branch assumes this parameters_path contains state_dict
        state_dict = torch.load(parameters_path)
        log_file.write(('Trying to find model class and fill it`s state dict\n').encode())
        model_class_definitions = _list_classes_from_module(python_model_module)
        if len(model_class_definitions) != 1:
            raise ValueError("Expected only one class as model definition. {}".format(
                model_class_definitions))
        pytorch_model_class = model_class_definitions[0]
        model = pytorch_model_class()
        model.load_state_dict(state_dict)
        return model
    except:
        # this branch assumes this parameters_path contains "entire" model
        _merge_module(python_model_module)
        log_file.write(('Model python module is merged into main environment\n').encode())
        model = torch.load(parameters_path)
        log_file.write(('Pytorch model loaded\n').encode())
        return model


def _extract_torchscript_model(log_file, input_path):
    # assuming this is a pytorch script
    log_file.write(('Trying to load TorchScript model\n').encode())
    try:
        pytorch_model = torch.jit.load(input_path)
        return pytorch_model
    except RuntimeError as e:
        log_file.write((str(e) + '\n').encode())
        log_file.write(
            'Failed to import input file. Maybe this it contains only weights? Try pass "python_path" argument\n'
            .encode())
        raise
    log_file.write(('TorchScript model is loaded\n').encode())


def _extract_mar_model(log_file, tmpdir, input_path):
    mar_dir_path = os.path.join(tmpdir, 'mar')
    with zipfile.ZipFile(input_path) as zip_input:
        zip_input.extractall(path=mar_dir_path)
    manifest_path = os.path.join(mar_dir_path, 'MAR-INF/MANIFEST.json')
    with open(manifest_path) as manifest_file:
        manifest = json.load(manifest_file)
    serialized_file = os.path.join(mar_dir_path, manifest['model']['serializedFile'])
    if 'modelFile' in manifest['model']:
        model_file = os.path.join(mar_dir_path, manifest['model']['modelFile'])
        return _extract_pytorch_model(log_file, serialized_file, model_file)
    else:
        return _extract_torchscript_model(log_file, serialized_file)


def _convert(args):
    _apply_verbosity(args.verbose)

    # get file path to log
    dir_path = os.path.dirname(os.path.realpath(__file__))
    logfile_path = os.path.realpath(args.output_path) + '.log'
    with open(logfile_path, 'wb') as f, tempfile.TemporaryDirectory() as tmpdir:
        # save intermediate
        if oneutils.is_valid_attr(args, 'save_intermediate'):
            tmpdir = os.path.dirname(logfile_path)
        # convert pytorch to onnx model
        input_path = getattr(args, 'input_path')
        model_file = getattr(args, 'python_path')

        if input_path[-4:] == '.mar':
            pytorch_model = _extract_mar_model(f, tmpdir, input_path)
        elif model_file is None:
            pytorch_model = _extract_torchscript_model(f, input_path)
        else:
            pytorch_model = _extract_pytorch_model(f, input_path, model_file)

        input_shapes = _parse_shapes(getattr(args, 'input_shapes'))
        input_types = _parse_types(getattr(args, 'input_types'))

        if len(input_shapes) != len(input_types):
            raise ValueError('number of input shapes and input types must be equal')

        sample_inputs = []
        for input_spec in zip(input_shapes, input_types):
            sample_inputs += [torch.ones(input_spec[0], dtype=input_spec[1])]

        f.write(('Trying to inference loaded model').encode())
        sample_outputs = pytorch_model(*sample_inputs)
        f.write(('Acquired sample outputs\n').encode())

        onnx_output_name = os.path.splitext(os.path.basename(
            args.output_path))[0] + '.onnx'
        onnx_output_path = os.path.join(tmpdir, onnx_output_name)

        onnx_saved = False
        # some operations are not supported in early opset versions, try several
        for onnx_opset_version in range(9, 15):
            f.write(('Trying to save onnx model using opset version ' +
                     str(onnx_opset_version) + '\n').encode())
            try:
                torch.onnx.export(pytorch_model,
                                  tuple(sample_inputs),
                                  onnx_output_path,
                                  example_outputs=sample_outputs,
                                  opset_version=onnx_opset_version)
                onnx_saved = True
                break
            except:
                f.write(('attempt failed\n').encode())

        if not onnx_saved:
            raise ValueError('Failed to save temporary onnx model')

        # convert onnx to tf saved mode
        onnx_model = onnx.load(onnx_output_path)

        options = onnx_legalizer.LegalizeOptions()
        options.unroll_rnn = oneutils.is_valid_attr(args, 'unroll_rnn')
        options.unroll_lstm = oneutils.is_valid_attr(args, 'unroll_lstm')
        onnx_legalizer.legalize(onnx_model, options)

        tf_savedmodel = onnx_tf.backend.prepare(onnx_model)

        savedmodel_name = os.path.splitext(os.path.basename(
            args.output_path))[0] + '.savedmodel'
        savedmodel_output_path = os.path.join(tmpdir, savedmodel_name)
        tf_savedmodel.export_graph(savedmodel_output_path)

        # make a command to convert from tf to tflite
        tf2tfliteV2_path = os.path.join(dir_path, 'tf2tfliteV2.py')
        tf2tfliteV2_output_name = os.path.splitext(os.path.basename(
            args.output_path))[0] + '.tflite'
        tf2tfliteV2_output_path = os.path.join(tmpdir, tf2tfliteV2_output_name)

        del args.input_shapes
        tf2tfliteV2_cmd = _make_cmd.make_tf2tfliteV2_cmd(args, tf2tfliteV2_path,
                                                         savedmodel_output_path,
                                                         tf2tfliteV2_output_path)

        f.write((' '.join(tf2tfliteV2_cmd) + '\n').encode())

        # convert tf to tflite
        oneutils.run(tf2tfliteV2_cmd, logfile=f)

        # make a command to convert from tflite to circle
        tflite2circle_path = os.path.join(dir_path, 'tflite2circle')
        tflite2circle_cmd = _make_cmd.make_tflite2circle_cmd(tflite2circle_path,
                                                             tf2tfliteV2_output_path,
                                                             getattr(args, 'output_path'))

        f.write((' '.join(tflite2circle_cmd) + '\n').encode())

        # convert tflite to circle
        oneutils.run(tflite2circle_cmd, err_prefix="tflite2circle", logfile=f)


def main():
    # parse arguments
    parser = _get_parser()
    args = _parse_arg(parser)

    # parse configuration file
    oneutils.parse_cfg(args.config, 'one-import-pytorch', args)

    # verify arguments
    _verify_arg(parser, args)

    # convert
    _convert(args)


if __name__ == '__main__':
    oneutils.safemain(main, __file__)
