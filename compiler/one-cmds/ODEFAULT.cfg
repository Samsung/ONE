[one-optimize]
; These options copied from 'onelib/constant.py`
convert_nchw_to_nhwc=True ; Experimental: This will convert NCHW operators to NHWC under the assumption that input model is NCHW.
expand_broadcast_const=True ; expand broadcastable constant node inputs
nchw_to_nhwc_input_shape=True ; onvert the input shape of the model (argument for convert_nchw_to_nhwc)
nchw_to_nhwc_output_shape=True ; convert the output shape of the model (argument for convert_nchw_to_nhwc)
fold_add_v2=True ; fold AddV2 op with constant inputs
fold_cast=True ; fold Cast op with constant input
fold_dequantize=True ; fold Dequantize op
fold_dwconv=True ; fold Depthwise Convolution op with constant inputs
fold_gather=True ; fold Gather op
fold_sparse_to_dense=True ; fold SparseToDense op
forward_reshape_to_unaryop=True ; Forward Reshape op
fuse_add_with_tconv=True ; fuse Add op to Transposed
fuse_add_with_fully_connected=True ; fuse Add op to FullyConnected op
fuse_batchnorm_with_conv=True ; fuse BatchNorm op to Convolution op
fuse_batchnorm_with_dwconv=True ; fuse BatchNorm op to Depthwise Convolution op
fuse_batchnorm_with_tconv=True ; fuse BatchNorm op to Transposed Convolution op
fuse_bcq=True ; apply Binary Coded Quantization
fuse_preactivation_batchnorm=True ; fuse BatchNorm operators of pre-activations to Convolution op
fuse_mean_with_mean=True ; fuse two consecutive Mean ops
fuse_transpose_with_mean=True ; fuse Mean with a preceding Transpose under certain conditions
make_batchnorm_gamma_positive=True ; make negative gamma of BatchNorm to a small positive value (1e-10). Note that this pass can change the execution result of the model. So, use it only when the impact is known to be acceptable.
fuse_activation_function=True ; fuse Activation function to a preceding operator
fuse_instnorm=True ; fuse ops to InstanceNorm operator
replace_cw_mul_add_with_depthwise_conv=True ; replace channel-wise Mul/Add with DepthwiseConv2D
remove_fakequant=True ; remove FakeQuant ops
remove_quantdequant=True ; remove Quantize-Dequantize sequence
remove_redundant_quantize=True ; remove redundant Quantize ops
remove_redundant_reshape=True ; fuse or remove subsequent Reshape ops
remove_redundant_transpose=True ; fuse or remove subsequent Transpose ops
remove_unnecessary_reshape=True ; remove unnecessary reshape ops
remove_unnecessary_slice=True ; remove unnecessary slice ops
remove_unnecessary_strided_slice=True ; remove unnecessary strided slice ops
remove_unnecessary_split=True ; remove unnecessary split ops
replace_non_const_fc_with_batch_matmul=True ; replace FullyConnected op with non-const weights to BatchMatMul op
resolve_customop_add=True ; convert Custom(Add) op to Add op
resolve_customop_batchmatmul=True ; convert Custom(BatchMatmul) op to BatchMatmul op
resolve_customop_matmul=True ; convert Custom(Matmul) op to Matmul op
resolve_customop_max_pool_with_argmax=True ; convert Custom(MaxPoolWithArgmax) to net of builtin operators
shuffle_weight_to_16x1float32=True ; convert weight format of FullyConnected op to SHUFFLED16x1FLOAT32. Note that it only converts weights whose row is a multiple of 16
substitute_pack_to_reshape=True ; convert single input Pack op to Reshape op
substitute_padv2_to_pad=True ; convert certain condition PadV2 to Pad
substitute_splitv_to_split=True ; convert certain condition SplitV to Split
substitute_squeeze_to_reshape=True ; convert certain condition Squeeze to Reshape
substitute_strided_slice_to_reshape=True ; convert certain condition StridedSlice to Reshape
substitute_transpose_to_reshape=True ; convert certain condition Transpose to Reshape
transform_min_max_to_relu6=True ; transform Minimum-Maximum pattern to Relu6 op
transform_min_relu_to_relu6=True ; transform Minimum(6)-Relu pattern to Relu6 op
