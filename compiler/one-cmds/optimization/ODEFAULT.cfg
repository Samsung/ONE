# These options copied from 'onelib/constant.py`
[one-optimize]
; Experimental: This will convert NCHW operators to NHWC under the assumption that input model is NCHW.
convert_nchw_to_nhwc=True
; expand broadcastable constant node inputs
expand_broadcast_const=True
; onvert the input shape of the model (argument for convert_nchw_to_nhwc)
nchw_to_nhwc_input_shape=True
; convert the output shape of the model (argument for convert_nchw_to_nhwc)
nchw_to_nhwc_output_shape=True
; fold AddV2 op with constant inputs
fold_add_v2=True
; fold Cast op with constant input
fold_cast=True
; fold Dequantize op
fold_dequantize=True
; fold Depthwise Convolution op with constant inputs
fold_dwconv=True
; fold Gather op
fold_gather=True
; fold SparseToDense op
fold_sparse_to_dense=True
; Forward Reshape op
forward_reshape_to_unaryop=True
; fuse Add op to Transposed
fuse_add_with_tconv=True
; fuse Add op to FullyConnected op
fuse_add_with_fully_connected=True
; fuse BatchNorm op to Convolution op
fuse_batchnorm_with_conv=True
; fuse BatchNorm op to Depthwise Convolution op
fuse_batchnorm_with_dwconv=True
; fuse BatchNorm op to Transposed Convolution op
fuse_batchnorm_with_tconv=True
; apply Binary Coded Quantization
fuse_bcq=True
; fuse BatchNorm operators of pre-activations to Convolution op
fuse_preactivation_batchnorm=True
; fuse two consecutive Mean ops
fuse_mean_with_mean=True
; fuse Mean with a preceding Transpose under certain conditions
fuse_transpose_with_mean=True
; make negative gamma of BatchNorm to a small positive value (1e-10).
; Note that this pass can change the execution result of the model.
; So, use it only when the impact is known to be acceptable.
make_batchnorm_gamma_positive=True
; fuse Activation function to a preceding operator
fuse_activation_function=True
; fuse ops to InstanceNorm operator
fuse_instnorm=True
; replace channel-wise Mul/Add with DepthwiseConv2D
replace_cw_mul_add_with_depthwise_conv=True
; remove FakeQuant ops
remove_fakequant=True
; remove Quantize-Dequantize sequence
remove_quantdequant=True
; remove redundant Quantize ops
remove_redundant_quantize=True
; fuse or remove subsequent Reshape ops
remove_redundant_reshape=True
; fuse or remove subsequent Transpose ops
remove_redundant_transpose=True
; remove unnecessary reshape ops
remove_unnecessary_reshape=True
; remove unnecessary slice ops
remove_unnecessary_slice=True
; remove unnecessary strided slice ops
remove_unnecessary_strided_slice=True
; remove unnecessary split ops
remove_unnecessary_split=True
; replace FullyConnected op with non-const weights to BatchMatMul op
replace_non_const_fc_with_batch_matmul=True
; convert Custom(Add) op to Add op
resolve_customop_add=True
; convert Custom(BatchMatmul) op to BatchMatmul op
resolve_customop_batchmatmul=True
; convert Custom(Matmul) op to Matmul op
resolve_customop_matmul=True
; convert Custom(MaxPoolWithArgmax) to net of builtin operators
resolve_customop_max_pool_with_argmax=True
; convert weight format of FullyConnected op to SHUFFLED16x1FLOAT32.
; Note that it only converts weights whose row is a multiple of 16
shuffle_weight_to_16x1float32=True
; convert single input Pack op to Reshape op
substitute_pack_to_reshape=True
; convert certain condition PadV2 to Pad
substitute_padv2_to_pad=True
; convert certain condition SplitV to Split
substitute_splitv_to_split=True
; convert certain condition Squeeze to Reshape
substitute_squeeze_to_reshape=True
; convert certain condition StridedSlice to Reshape
substitute_strided_slice_to_reshape=True
; convert certain condition Transpose to Reshape
substitute_transpose_to_reshape=True
; transform Minimum-Maximum pattern to Relu6 op
transform_min_max_to_relu6=True
; transform Minimum(6)-Relu pattern to Relu6 op
transform_min_relu_to_relu6=True
