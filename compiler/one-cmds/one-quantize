#!/usr/bin/env bash
''''export SCRIPT_PATH="$(cd "$(dirname "$(readlink -f "${BASH_SOURCE[0]}")")" && pwd)" # '''
''''export PY_PATH=${SCRIPT_PATH}/venv/bin/python                                       # '''
''''test -f ${PY_PATH} && exec ${PY_PATH} "$0" "$@"                                     # '''
''''echo "Error: Virtual environment not found. Please run 'one-prepare-venv' command." # '''
''''exit 255                                                                            # '''

# Copyright (c) 2020 Samsung Electronics Co., Ltd. All Rights Reserved
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#    http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

import argparse
import os
import sys
import tempfile
import json

import utils as _utils
from utils import Command

# TODO Find better way to suppress trackback on error
sys.tracebacklimit = 0


def _get_parser():
    parser = argparse.ArgumentParser(
        description='command line tool to quantize circle model')

    _utils._add_default_arg(parser)

    # input and output path.
    parser.add_argument(
        '-i', '--input_path', type=str, help='full filepath of the input circle model')
    parser.add_argument(
        '-d',
        '--input_data',
        type=str,
        help=
        'full filepath of the input data used for post-training quantization. if not specified, run with random input data.'
    )
    parser.add_argument(
        '-f',
        '--input_data_format',
        type=str,
        help=
        'file format of input data. h5/hdf5 (default), list/filelist (a text file where a file path of input data is written in each line), or dir/directory (a directory where input data are saved)'
    )
    parser.add_argument(
        '-o',
        '--output_path',
        type=str,
        help='full filepath of the output quantized model')

    # argument for profiling
    parser.add_argument(
        '-p',
        '--generate_profile_data',
        action='store_true',
        help='generate profiling data')

    # save intermediate file(s)
    parser.add_argument(
        '--save_intermediate',
        action='store_true',
        help='Save intermediate files to output folder')

    ## arguments for quantization
    quantization_group = parser.add_argument_group('arguments for quantization')

    quantization_group.add_argument(
        '--input_dtype',
        type=str,
        help=
        'input model data type (supported: float32, default=float32). Deprecated (Use input_model_dtype)'
    )
    quantization_group.add_argument(
        '--input_model_dtype',
        type=str,
        help='input model data type (supported: float32, default=float32)')
    quantization_group.add_argument(
        '--quantized_dtype',
        type=str,
        help='data type of output quantized model (supported: uint8, int16, default=uint8)'
    )
    quantization_group.add_argument(
        '--granularity',
        type=str,
        help='quantization granularity (supported: layer, channel, default=layer)')
    quantization_group.add_argument(
        '--input_type',
        type=str,
        help=
        'data type of inputs of quantized model (supported: uint8, int16, float32, default=quantized_dtype). QUANTIZE Op will be inserted at the beginning of the quantized model if input_type is different from quantized_dtype.'
    )
    quantization_group.add_argument(
        '--output_type',
        type=str,
        help=
        'data type of outputs of quantized model (supported: uint8, int16, float32, default=quantized_dtype). QUANTIZE Op will be inserted at the end of the quantized model if output_type is different from quantized_dtype.'
    )
    quantization_group.add_argument(
        '--min_percentile',
        type=str,
        help=
        'minimum percentile (0.0~100.0, default=1.0). Algorithm parameter for calibration. This is valid when calibration algorithm is percentile.'
    )
    quantization_group.add_argument(
        '--max_percentile',
        type=str,
        help=
        'maximum percentile (0.0~100.0, default=99.0). Algorithm parameter for calibration. This is valid when calibration algorithm is percentile.'
    )
    quantization_group.add_argument(
        '--mode',
        type=str,
        help=
        "calibration algorithm for post-training quantization (supported: percentile/moving_average, default=percentile). 'percentile' mode uses the n-th percentiles as min/max values. 'moving_average' mode records the moving average of min/max."
    )
    quantization_group.add_argument(
        '--TF-style_maxpool',
        action='store_true',
        help=
        "Force MaxPool Op to have the same input/output quantparams. NOTE: This option can degrade accuracy of some models.)"
    )
    quantization_group.add_argument(
        '--quant_config', type=str, help="Path to the quantization configuration file.")
    quantization_group.add_argument(
        '--evaluate_result',
        action='store_true',
        help=
        "Evaluate accuracy of quantized model. Run inference for both fp32 model and the quantized model, and compare the inference results."
    )
    quantization_group.add_argument(
        '--test_data', type=str, help="Path to the test data used for evaluation.")
    quantization_group.add_argument(
        '--print_mae',
        action='store_true',
        help=
        "Print MAE (Mean Absolute Error) of inference results between quantized model and fp32 model."
    )
    quantization_group.add_argument(
        '--print_mape',
        action='store_true',
        help=
        "Print MAPE (Mean Absolute Percentage Error) of inference results between quantized model and fp32 model."
    )
    quantization_group.add_argument(
        '--print_mpeir',
        action='store_true',
        help=
        "Print MPEIR (Mean Peak Error to Interval Ratio) of inference results between quantized model and fp32 model."
    )
    quantization_group.add_argument(
        '--print_top1_match',
        action='store_true',
        help=
        "Print Top-1 match ratio of inference results between quantized model and fp32 model."
    )
    quantization_group.add_argument(
        '--print_top5_match',
        action='store_true',
        help=
        "Print Top-5 match ratio of inference results between quantized model and fp32 model."
    )

    # arguments for force_quantparam option
    force_quantparam_group = parser.add_argument_group(
        'arguments for force_quantparam option')

    force_quantparam_group.add_argument(
        '--force_quantparam',
        action='store_true',
        help=
        'overwrite quantparam (scale, zero_point) to the specified tensor in the quantized model.'
    )
    force_quantparam_group.add_argument(
        '--tensor_name', type=str, action='append', help='tensor name (string)')
    force_quantparam_group.add_argument(
        '--scale', type=float, action='append', help='scale (float)')
    force_quantparam_group.add_argument(
        '--zero_point', type=int, action='append', help='zero point (int)')

    # arguments for copy_quantparam option
    copy_quantparam_group = parser.add_argument_group(
        'arguments for copy_quantparam option')

    copy_quantparam_group.add_argument(
        '--copy_quantparam',
        action='store_true',
        help='copy quantparam (scale, zero_point) of a tensor to another tensor.')
    copy_quantparam_group.add_argument(
        '--src_tensor_name', type=str, action='append', help='tensor name (string)')
    copy_quantparam_group.add_argument(
        '--dst_tensor_name', type=str, action='append', help='tensor name (string)')

    # arguments for fake_quant option
    fake_quant_group = parser.add_argument_group('arguments for fake_quantize option')

    fake_quant_group.add_argument(
        '--fake_quantize',
        action='store_true',
        help='convert quantized model to fake-quantized fp32 model.')

    return parser


def _set_default_values(args):
    if not _utils._is_valid_attr(args, 'input_model_dtype') and not _utils._is_valid_attr(
            args, 'input_dtype'):
        setattr(args, 'input_model_dtype', 'float32')
    if not _utils._is_valid_attr(args, 'quantized_dtype'):
        setattr(args, 'quantized_dtype', 'uint8')
        if _utils._is_valid_attr(args, 'quant_config'):
            # Get quantized_dtype from qconfig file
            try:
                with open(getattr(args, 'quant_config')) as f:
                    qconf = json.load(f)
                    if 'default_quantization_dtype' in qconf:
                        setattr(args, 'quantized_dtype',
                                qconf['default_quantization_dtype'])
            except json.decoder.JSONDecodeError:
                print('Failed to decode ' + getattr(args, 'quant_config') +
                      '. Please check it is a json file.')
    if not _utils._is_valid_attr(args, 'granularity'):
        setattr(args, 'granularity', 'layer')
        if _utils._is_valid_attr(args, 'quant_config'):
            # Get granularity from qconfig file
            try:
                with open(getattr(args, 'quant_config')) as f:
                    qconf = json.load(f)
                    if 'default_granularity' in qconf:
                        setattr(args, 'granularity', qconf['default_granularity'])
            except json.decoder.JSONDecodeError:
                print('Failed to decode ' + getattr(args, 'quant_config') +
                      '. Please check it is a json file.')
    if not _utils._is_valid_attr(args, 'mode'):
        setattr(args, 'mode', 'percentile')
    if not _utils._is_valid_attr(args, 'min_percentile'):
        setattr(args, 'min_percentile', '1.0')
    if not _utils._is_valid_attr(args, 'max_percentile'):
        setattr(args, 'max_percentile', '99.0')


def _verify_arg(parser, args):
    """verify given arguments"""
    # check if required arguments is given
    missing = []
    if not _utils._is_valid_attr(args, 'input_path'):
        missing.append('-i/--input_path')
    if not _utils._is_valid_attr(args, 'output_path'):
        missing.append('-o/--output_path')
    if _utils._is_valid_attr(args, 'force_quantparam'):
        if not _utils._is_valid_attr(args, 'tensor_name'):
            missing.append('--tensor_name')
        if not _utils._is_valid_attr(args, 'scale'):
            missing.append('--scale')
        if not _utils._is_valid_attr(args, 'zero_point'):
            missing.append('--zero_point')
    if _utils._is_valid_attr(args, 'copy_quantparam'):
        if not _utils._is_valid_attr(args, 'src_tensor_name'):
            missing.append('--src_tensor_name')
        if not _utils._is_valid_attr(args, 'dst_tensor_name'):
            missing.append('--dst_tensor_name')
    if len(missing):
        parser.error('the following arguments are required: ' + ' '.join(missing))
    if _utils._is_valid_attr(args, 'force_quantparam'):
        tensors = getattr(args, 'tensor_name')
        scales = getattr(args, 'scale')
        zerops = getattr(args, 'zero_point')
        if len(tensors) != len(scales) or len(tensors) != len(zerops):
            parser.error(
                'The same number of tensor_name, scale, and zero_point should be given.')
    if _utils._is_valid_attr(args, 'copy_quantparam'):
        src_tensors = getattr(args, 'src_tensor_name')
        dst_tensors = getattr(args, 'dst_tensor_name')
        if len(src_tensors) != len(dst_tensors):
            parser.error(
                'The same number of src_tensor_name and dst_tensor_name should be given.')


def _parse_arg(parser):
    args = parser.parse_args()
    # print version
    if args.version:
        _utils._print_version_and_exit(__file__)

    return args


def _quantize(args):
    if _utils._is_valid_attr(args, 'force_quantparam'):
        # write quantization parameters
        _write_qparam(args)
        return

    if _utils._is_valid_attr(args, 'copy_quantparam'):
        # copy quantization parameters
        _copy_qparam(args)
        return

    if _utils._is_valid_attr(args, 'fake_quantize'):
        # fake-quantize model
        _fake_quantize(args)
        return

    # get file path to log
    dir_path = os.path.dirname(os.path.realpath(__file__))
    logfile_path = os.path.realpath(args.output_path) + '.log'

    with open(logfile_path, 'wb') as f, tempfile.TemporaryDirectory() as tmpdir:
        if _utils._is_valid_attr(args, 'save_intermediate'):
            tmpdir = os.path.dirname(logfile_path)
        # get driver path
        circle_quantizer_path = os.path.join(dir_path, 'circle-quantizer')
        record_minmax_path = os.path.join(dir_path, 'record-minmax')

        ## make a command to quantize and dequantize the weights of the model
        circle_quantizer_cmd = [circle_quantizer_path]
        # verbose
        if _utils._is_valid_attr(args, 'verbose'):
            circle_quantizer_cmd.append('--verbose')
        # quantize_dequantize_weights
        circle_quantizer_cmd.append('--quantize_dequantize_weights')
        # Use input_model_dtype if it exists. Use input_dtype otherwise.
        if _utils._is_valid_attr(args, 'input_model_dtype'):
            circle_quantizer_cmd.append(getattr(args, 'input_model_dtype'))
        elif _utils._is_valid_attr(args, 'input_dtype'):
            circle_quantizer_cmd.append(getattr(args, 'input_dtype'))
        if _utils._is_valid_attr(args, 'quantized_dtype'):
            circle_quantizer_cmd.append(getattr(args, 'quantized_dtype'))
        if _utils._is_valid_attr(args, 'granularity'):
            circle_quantizer_cmd.append(getattr(args, 'granularity'))
        if _utils._is_valid_attr(args, 'quant_config'):
            # NOTE --config conflicts with --config option in onecc, so
            # we use quant_config for one-quantize
            circle_quantizer_cmd.append('--config')
            circle_quantizer_cmd.append(getattr(args, 'quant_config'))
        # input and output path
        if _utils._is_valid_attr(args, 'input_path'):
            circle_quantizer_cmd.append(getattr(args, 'input_path'))
        tmp_weights_fake_quant_path = os.path.join(
            tmpdir,
            os.path.splitext(os.path.basename(
                args.input_path))[0]) + '.weights_fake_quant.circle'
        circle_quantizer_cmd.append(tmp_weights_fake_quant_path)
        # profiling
        if _utils._is_valid_attr(args, 'generate_profile_data'):
            circle_quantizer_cmd.append('--generate_profile_data')

        f.write((' '.join(circle_quantizer_cmd) + '\n').encode())

        # run circle-quantizer
        _utils._run(circle_quantizer_cmd, err_prefix="circle_quantizer", logfile=f)

        tmp_minmax_recorded_path = os.path.join(
            tmpdir,
            os.path.splitext(os.path.basename(
                args.input_path))[0]) + '.minmax_recorded.circle'

        ## make a command to record min-max value of each tensor while running the representative dataset
        record_minmax_cmd = Command(record_minmax_path, args, f)
        record_minmax_cmd.add_noarg_option_if_valid_arg('--verbose', 'verbose') \
            .add_option_with_values('--input_model', [tmp_weights_fake_quant_path]) \
            .add_option_with_values('--output_model', [tmp_minmax_recorded_path]) \
            .add_option_with_valid_args('--input_data', ['input_data']) \
            .add_option_with_valid_args('--input_data_format', ['input_data_format']) \
            .add_option_with_valid_args('--min_percentile', ['min_percentile']) \
            .add_option_with_valid_args('--max_percentile', ['max_percentile']) \
            .add_option_with_valid_args('--mode', ['mode']) \
            .add_noarg_option_if_valid_arg('--generate_profile_data', 'generate_profile_data') \
            .run()

        ## make a second command to quantize the model using the embedded information
        circle_quantizer_cmd = [circle_quantizer_path]
        # verbose
        if _utils._is_valid_attr(args, 'verbose'):
            circle_quantizer_cmd.append('--verbose')
        # quantize_dequantize_weights
        circle_quantizer_cmd.append('--quantize_with_minmax')
        # Use input_model_dtype if it exists. Use input_dtype otherwise.
        if _utils._is_valid_attr(args, 'input_model_dtype'):
            circle_quantizer_cmd.append(getattr(args, 'input_model_dtype'))
        elif _utils._is_valid_attr(args, 'input_dtype'):
            circle_quantizer_cmd.append(getattr(args, 'input_dtype'))
        if _utils._is_valid_attr(args, 'quantized_dtype'):
            circle_quantizer_cmd.append(getattr(args, 'quantized_dtype'))
        if _utils._is_valid_attr(args, 'granularity'):
            circle_quantizer_cmd.append(getattr(args, 'granularity'))
        if _utils._is_valid_attr(args, 'TF-style_maxpool'):
            circle_quantizer_cmd.append('--TF-style_maxpool')
        if _utils._is_valid_attr(args, 'input_type'):
            circle_quantizer_cmd.append('--input_type')
            circle_quantizer_cmd.append(getattr(args, 'input_type'))
        if _utils._is_valid_attr(args, 'output_type'):
            circle_quantizer_cmd.append('--output_type')
            circle_quantizer_cmd.append(getattr(args, 'output_type'))
        if _utils._is_valid_attr(args, 'quant_config'):
            # NOTE --config conflicts with --config option in onecc, so
            # we use quant_config for one-quantize
            circle_quantizer_cmd.append('--config')
            circle_quantizer_cmd.append(getattr(args, 'quant_config'))
        # input and output path
        circle_quantizer_cmd.append(tmp_minmax_recorded_path)
        if _utils._is_valid_attr(args, 'output_path'):
            circle_quantizer_cmd.append(getattr(args, 'output_path'))
        # profiling
        if _utils._is_valid_attr(args, 'generate_profile_data'):
            circle_quantizer_cmd.append('--generate_profile_data')

        f.write((' '.join(circle_quantizer_cmd) + '\n').encode())

        # run circle-quantizer
        _utils._run(circle_quantizer_cmd, err_prefix="circle_quantizer", logfile=f)

        # evaluate
        if _utils._is_valid_attr(args, 'evaluate_result'):
            circle_eval_diff_path = os.path.join(dir_path, 'circle-eval-diff')
            quant_model = ""
            if _utils._is_valid_attr(args, 'output_path'):
                quant_model = getattr(args, 'output_path')
            tmp_fake_quant_model = os.path.join(
                tmpdir,
                os.path.splitext(os.path.basename(
                    args.input_path))[0]) + '.fake_quant.circle'

            # do fake quantization
            fake_quantize_cmd = Command(circle_quantizer_path, args, f)
            fake_quantize_cmd.add_noarg_option_if_valid_arg('--verbose', 'verbose') \
                .add_option_with_values('--fake_quantize', [quant_model, tmp_fake_quant_model]) \
                .run()

            # compare fake-quant model and fp32 model
            circle_eval_diff_cmd = Command(circle_eval_diff_path, args, f)
            circle_eval_diff_cmd.add_option_with_valid_args('--first_model', ['input_path']) \
                .add_option_with_values('--second_model', [tmp_fake_quant_model]) \
                .add_option_with_valid_args('--first_input_data', ['test_data']) \
                .add_option_with_valid_args('--second_input_data', ['test_data']) \
                .add_option_with_valid_args('--input_data_format', ['input_data_format']) \
                .add_noarg_option_if_valid_arg('--print_mae', 'print_mae') \
                .add_noarg_option_if_valid_arg('--print_mape', 'print_mape') \
                .add_noarg_option_if_valid_arg('--print_mpeir', 'print_mpeir') \
                .add_noarg_option_if_valid_arg('--print_top1_match', 'print_top1_match') \
                .add_noarg_option_if_valid_arg('--print_top5_match', 'print_top5_match') \
                .run()


def _write_qparam(args):
    # get file path to log
    dir_path = os.path.dirname(os.path.realpath(__file__))
    logfile_path = os.path.realpath(args.output_path) + '.log'

    with open(logfile_path, 'wb') as f:
        # get driver path
        circle_quantizer_path = os.path.join(dir_path, 'circle-quantizer')

        # make a command to write qparams to the tensors
        circle_quantizer_cmd = [circle_quantizer_path]
        # verbose
        if _utils._is_valid_attr(args, 'verbose'):
            circle_quantizer_cmd.append('--verbose')
        if _utils._is_valid_attr(args, 'tensor_name'):
            tensor_name = getattr(args, 'tensor_name')
        if _utils._is_valid_attr(args, 'scale'):
            scale = getattr(args, 'scale')
        if _utils._is_valid_attr(args, 'zero_point'):
            zero_point = getattr(args, 'zero_point')
        for (t, s, zp) in zip(tensor_name, scale, zero_point):
            circle_quantizer_cmd.append('--force_quantparam')
            circle_quantizer_cmd.append(t)
            circle_quantizer_cmd.append(str(s))
            circle_quantizer_cmd.append(str(zp))
        # input and output path
        if _utils._is_valid_attr(args, 'input_path'):
            circle_quantizer_cmd.append(getattr(args, 'input_path'))
        if _utils._is_valid_attr(args, 'output_path'):
            circle_quantizer_cmd.append(getattr(args, 'output_path'))

        f.write((' '.join(circle_quantizer_cmd) + '\n').encode())

        # run circle-quantizer
        _utils._run(circle_quantizer_cmd, err_prefix="circle_quantizer", logfile=f)


def _copy_qparam(args):
    # get file path to log
    dir_path = os.path.dirname(os.path.realpath(__file__))
    logfile_path = os.path.realpath(args.output_path) + '.log'

    with open(logfile_path, 'wb') as f:
        # get driver path
        circle_quantizer_path = os.path.join(dir_path, 'circle-quantizer')

        # make a command to write qparams to the tensors
        circle_quantizer_cmd = [circle_quantizer_path]
        # verbose
        if _utils._is_valid_attr(args, 'verbose'):
            circle_quantizer_cmd.append('--verbose')
        if _utils._is_valid_attr(args, 'src_tensor_name'):
            src_tensor_name = getattr(args, 'src_tensor_name')
        if _utils._is_valid_attr(args, 'dst_tensor_name'):
            dst_tensor_name = getattr(args, 'dst_tensor_name')
        for (src, dst) in zip(src_tensor_name, dst_tensor_name):
            circle_quantizer_cmd.append('--copy_quantparam')
            circle_quantizer_cmd.append(src)
            circle_quantizer_cmd.append(dst)
        # input and output path
        if _utils._is_valid_attr(args, 'input_path'):
            circle_quantizer_cmd.append(getattr(args, 'input_path'))
        if _utils._is_valid_attr(args, 'output_path'):
            circle_quantizer_cmd.append(getattr(args, 'output_path'))

        f.write((' '.join(circle_quantizer_cmd) + '\n').encode())

        # run circle-quantizer
        _utils._run(circle_quantizer_cmd, err_prefix="circle_quantizer", logfile=f)


def _fake_quantize(args):
    # get file path to log
    dir_path = os.path.dirname(os.path.realpath(__file__))
    logfile_path = os.path.realpath(args.output_path) + '.log'

    with open(logfile_path, 'wb') as f:
        # get driver path
        circle_quantizer_path = os.path.join(dir_path, 'circle-quantizer')
        q_model = getattr(args, 'input_path')
        fq_model = getattr(args, 'output_path')

        # do fake quantization
        fake_quantize_cmd = Command(circle_quantizer_path, args, f)
        fake_quantize_cmd.add_noarg_option_if_valid_arg('--verbose', 'verbose') \
            .add_option_with_values('--fake_quantize', [q_model, fq_model]) \
            .run()


def main():
    # parse arguments
    parser = _get_parser()
    args = _parse_arg(parser)

    # parse configuration file
    _utils._parse_cfg(args, 'one-quantize')

    # set default values
    _set_default_values(args)

    # verify arguments
    _verify_arg(parser, args)

    # quantize
    _quantize(args)


if __name__ == '__main__':
    _utils._safemain(main, __file__)
