#!/usr/bin/env bash
''''export SCRIPT_PATH="$(cd "$(dirname "$(readlink -f "${BASH_SOURCE[0]}")")" && pwd)" # '''
''''export PY_PATH=${SCRIPT_PATH}/venv/bin/python                                       # '''
''''test -f ${PY_PATH} && exec ${PY_PATH} "$0" "$@"                                     # '''
''''echo "Error: Virtual environment not found. Please run 'one-prepare-venv' command." # '''
''''exit 255                                                                            # '''

# Copyright (c) 2020 Samsung Electronics Co., Ltd. All Rights Reserved
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#    http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

import argparse
import os
import sys
import tempfile
import json

import onelib.utils as oneutils
from onelib.Command import Command

# TODO Find better way to suppress trackback on error
sys.tracebacklimit = 0


def _get_parser():
    parser = argparse.ArgumentParser(
        description='command line tool to quantize circle model')

    oneutils.add_default_arg(parser)

    # input and output path.
    parser.add_argument(
        '-i', '--input_path', type=str, help='full filepath of the input circle model')
    parser.add_argument(
        '-d',
        '--input_data',
        type=str,
        help=
        'full filepath of the input data used for post-training quantization. if not specified, run with random input data.'
    )
    parser.add_argument(
        '-f',
        '--input_data_format',
        type=str,
        help=
        'file format of input data. h5/hdf5 (default), list/filelist (a text file where a file path of input data is written in each line), or dir/directory (a directory where input data are saved)'
    )
    parser.add_argument(
        '-o',
        '--output_path',
        type=str,
        help='full filepath of the output quantized model')

    # argument for profiling
    parser.add_argument(
        '-p',
        '--generate_profile_data',
        action='store_true',
        help='generate profiling data')

    # save intermediate file(s)
    parser.add_argument(
        '--save_intermediate',
        action='store_true',
        help='Save intermediate files to output folder')

    ## arguments for quantization
    quantization_group = parser.add_argument_group('arguments for quantization')

    quantization_group.add_argument(
        '--input_dtype',
        type=str,
        help=
        'input model data type (supported: float32, default=float32). Deprecated (Use input_model_dtype)'
    )
    quantization_group.add_argument(
        '--input_model_dtype',
        type=str,
        help='input model data type (supported: float32, default=float32)')
    quantization_group.add_argument(
        '--quantized_dtype',
        type=str,
        help='data type of output quantized model (supported: uint8, int16, default=uint8)'
    )
    quantization_group.add_argument(
        '--granularity',
        type=str,
        help='quantization granularity (supported: layer, channel, default=layer)')
    quantization_group.add_argument(
        '--input_type',
        type=str,
        help=
        'data type of inputs of quantized model (supported: uint8, int16, float32, default=quantized_dtype). QUANTIZE Op will be inserted at the beginning of the quantized model if input_type is different from quantized_dtype.'
    )
    quantization_group.add_argument(
        '--output_type',
        type=str,
        help=
        'data type of outputs of quantized model (supported: uint8, int16, float32, default=quantized_dtype). QUANTIZE Op will be inserted at the end of the quantized model if output_type is different from quantized_dtype.'
    )
    quantization_group.add_argument(
        '--min_percentile',
        type=str,
        help=
        'minimum percentile (0.0~100.0, default=1.0). Algorithm parameter for calibration. This is valid when calibration algorithm is percentile.'
    )
    quantization_group.add_argument(
        '--max_percentile',
        type=str,
        help=
        'maximum percentile (0.0~100.0, default=99.0). Algorithm parameter for calibration. This is valid when calibration algorithm is percentile.'
    )
    quantization_group.add_argument(
        '--moving_avg_batch',
        type=str,
        help=
        'batch size of moving average (default=16). This is valid when calibration algorithm is moving_average.'
    )
    quantization_group.add_argument(
        '--moving_avg_const',
        type=str,
        help=
        'hyperparameter (C) to compute moving average (default=0.1). Update equation: avg <- avg + C * (curr_batch_avg - avg). This is valid when calibration algorithm is moving_average.'
    )
    quantization_group.add_argument(
        '--mode',
        type=str,
        help=
        "calibration algorithm for post-training quantization (supported: percentile/moving_average, default=percentile). 'percentile' mode uses the n-th percentiles as min/max values. 'moving_average' mode records the moving average of min/max."
    )
    quantization_group.add_argument(
        '--TF-style_maxpool',
        action='store_true',
        help=
        "Force MaxPool Op to have the same input/output quantparams. NOTE: This option can degrade accuracy of some models.)"
    )
    quantization_group.add_argument(
        '--quant_config', type=str, help="Path to the quantization configuration file.")
    quantization_group.add_argument(
        '--evaluate_result',
        action='store_true',
        help=
        "Evaluate accuracy of quantized model. Run inference for both fp32 model and the quantized model, and compare the inference results."
    )
    quantization_group.add_argument(
        '--test_data', type=str, help="Path to the test data used for evaluation.")
    quantization_group.add_argument(
        '--print_mae',
        action='store_true',
        help=
        "Print MAE (Mean Absolute Error) of inference results between quantized model and fp32 model."
    )
    quantization_group.add_argument(
        '--print_mape',
        action='store_true',
        help=
        "Print MAPE (Mean Absolute Percentage Error) of inference results between quantized model and fp32 model."
    )
    quantization_group.add_argument(
        '--print_mpeir',
        action='store_true',
        help=
        "Print MPEIR (Mean Peak Error to Interval Ratio) of inference results between quantized model and fp32 model."
    )
    quantization_group.add_argument(
        '--print_top1_match',
        action='store_true',
        help=
        "Print Top-1 match ratio of inference results between quantized model and fp32 model."
    )
    quantization_group.add_argument(
        '--print_top5_match',
        action='store_true',
        help=
        "Print Top-5 match ratio of inference results between quantized model and fp32 model."
    )
    quantization_group.add_argument(
        '--print_mse',
        action='store_true',
        help=
        "Print MSE (Mean Squared Error) of inference results between quantized model and fp32 model."
    )

    # arguments for force_quantparam option
    force_quantparam_group = parser.add_argument_group(
        'arguments for force_quantparam option')

    force_quantparam_group.add_argument(
        '--force_quantparam',
        action='store_true',
        help=
        'overwrite quantparam (scale, zero_point) to the specified tensor in the quantized model.'
    )
    force_quantparam_group.add_argument(
        '--tensor_name', type=str, action='append', help='tensor name (string)')
    force_quantparam_group.add_argument(
        '--scale', type=float, action='append', help='scale (float)')
    force_quantparam_group.add_argument(
        '--zero_point', type=int, action='append', help='zero point (int)')

    # arguments for copy_quantparam option
    copy_quantparam_group = parser.add_argument_group(
        'arguments for copy_quantparam option')

    copy_quantparam_group.add_argument(
        '--copy_quantparam',
        action='store_true',
        help='copy quantparam (scale, zero_point) of a tensor to another tensor.')
    copy_quantparam_group.add_argument(
        '--src_tensor_name', type=str, action='append', help='tensor name (string)')
    copy_quantparam_group.add_argument(
        '--dst_tensor_name', type=str, action='append', help='tensor name (string)')

    # arguments for fake_quant option
    fake_quant_group = parser.add_argument_group('arguments for fake_quantize option')

    fake_quant_group.add_argument(
        '--fake_quantize',
        action='store_true',
        help='convert quantized model to fake-quantized fp32 model.')

    # arguments for requantize option
    requantize_group = parser.add_argument_group('arguments for requantize option')

    requantize_group.add_argument(
        '--requantize',
        action='store_true',
        help='convert quantized model to another-typed quantized model (ex: int8 -> uin8).'
    )

    # arguments for ampq option
    ampq_quant_group = parser.add_argument_group('arguments for ampq option')
    # ampq
    ampq_quant_group.add_argument(
        '--ampq', action='store_true', help='quantize model using ampq solver.')

    # ampq_qerror_ratio
    ampq_quant_group.add_argument(
        '--ampq_qerror_ratio', type=str, help='quantization error ratio ([0, 1])')

    # ampq_algorithm
    ampq_quant_group.add_argument(
        '--ampq_algorithm', type=str, help='type of algorithm (bisection, pattern)')

    ampq_quant_group.add_argument(
        '--bisection_type', type=str, help="one of 'auto', 'i16_front', 'i16_back'")

    ampq_quant_group.add_argument(
        '--u8_layernorm_with_s16_variance',
        action='store_true',
        help='Use int16 for computing variance in uint8 layer normalization')

    ampq_quant_group.add_argument(
        '--u8_softmax_with_s16_sub_exp',
        action='store_true',
        help='Use int16 for computing Sub and Exp nodes in uint8 Softmax')

    # ampq_bisection_visq
    ampq_quant_group.add_argument(
        '--ampq_bisection_visq',
        type=str,
        help='.visq.json file path with quantization errors')

    return parser


def _set_default_values(args):
    if not oneutils.is_valid_attr(args,
                                  'input_model_dtype') and not oneutils.is_valid_attr(
                                      args, 'input_dtype'):
        setattr(args, 'input_model_dtype', 'float32')
    if not oneutils.is_valid_attr(args, 'quantized_dtype'):
        setattr(args, 'quantized_dtype', 'uint8')
        if oneutils.is_valid_attr(args, 'quant_config'):
            # Get quantized_dtype from qconfig file
            try:
                with open(getattr(args, 'quant_config')) as f:
                    qconf = json.load(f)
                    if 'default_quantization_dtype' in qconf:
                        setattr(args, 'quantized_dtype',
                                qconf['default_quantization_dtype'])
            except json.decoder.JSONDecodeError:
                print('Failed to decode ' + getattr(args, 'quant_config') +
                      '. Please check it is a json file.')
    if not oneutils.is_valid_attr(args, 'granularity'):
        setattr(args, 'granularity', 'layer')
        if oneutils.is_valid_attr(args, 'quant_config'):
            # Get granularity from qconfig file
            try:
                with open(getattr(args, 'quant_config')) as f:
                    qconf = json.load(f)
                    if 'default_granularity' in qconf:
                        setattr(args, 'granularity', qconf['default_granularity'])
            except json.decoder.JSONDecodeError:
                print('Failed to decode ' + getattr(args, 'quant_config') +
                      '. Please check it is a json file.')
    if not oneutils.is_valid_attr(args, 'mode'):
        setattr(args, 'mode', 'percentile')
    if not oneutils.is_valid_attr(args, 'min_percentile'):
        setattr(args, 'min_percentile', '1.0')
    if not oneutils.is_valid_attr(args, 'max_percentile'):
        setattr(args, 'max_percentile', '99.0')
    if not oneutils.is_valid_attr(args, 'moving_avg_batch'):
        setattr(args, 'moving_avg_batch', '16')
    if not oneutils.is_valid_attr(args, 'moving_avg_const'):
        setattr(args, 'moving_avg_const', '0.1')
    if not oneutils.is_valid_attr(args, 'ampq_algorithm'):
        setattr(args, 'ampq_algorithm', 'bisection')
    if not oneutils.is_valid_attr(args, 'bisection_type'):
        setattr(args, 'bisection_type', 'auto')


def _verify_arg_pre(parser, args):
    """verify given arguments before default values are set"""
    # check if required arguments is given
    missing = []
    if oneutils.is_valid_attr(args, 'requantize'):
        if not oneutils.is_valid_attr(args,
                                      'input_model_dtype') and not oneutils.is_valid_attr(
                                          args, 'input_dtype'):
            missing.append('--input_model_dtype')
        if not oneutils.is_valid_attr(args, 'quantized_dtype'):
            missing.append('--quantized_dtype')
    if len(missing):
        parser.error('the following arguments are required: ' + ' '.join(missing))


def _verify_arg(parser, args):
    """verify given arguments"""
    # check if required arguments is given
    missing = []
    if not oneutils.is_valid_attr(args, 'input_path'):
        missing.append('-i/--input_path')
    if not oneutils.is_valid_attr(args, 'output_path'):
        missing.append('-o/--output_path')
    if oneutils.is_valid_attr(args, 'force_quantparam'):
        if not oneutils.is_valid_attr(args, 'tensor_name'):
            missing.append('--tensor_name')
        if not oneutils.is_valid_attr(args, 'scale'):
            missing.append('--scale')
        if not oneutils.is_valid_attr(args, 'zero_point'):
            missing.append('--zero_point')
    if oneutils.is_valid_attr(args, 'copy_quantparam'):
        if not oneutils.is_valid_attr(args, 'src_tensor_name'):
            missing.append('--src_tensor_name')
        if not oneutils.is_valid_attr(args, 'dst_tensor_name'):
            missing.append('--dst_tensor_name')
    if len(missing):
        parser.error('the following arguments are required: ' + ' '.join(missing))
    if oneutils.is_valid_attr(args, 'force_quantparam'):
        tensors = getattr(args, 'tensor_name')
        scales = getattr(args, 'scale')
        zerops = getattr(args, 'zero_point')
        if len(tensors) != len(scales) or len(tensors) != len(zerops):
            parser.error(
                'The same number of tensor_name, scale, and zero_point should be given.')
    if oneutils.is_valid_attr(args, 'copy_quantparam'):
        src_tensors = getattr(args, 'src_tensor_name')
        dst_tensors = getattr(args, 'dst_tensor_name')
        if len(src_tensors) != len(dst_tensors):
            parser.error(
                'The same number of src_tensor_name and dst_tensor_name should be given.')

    # Check calibration parameters
    if oneutils.is_valid_attr(args, 'mode'):
        if getattr(args, 'mode') == 'percentile':
            # Check dtype
            try:
                min_percentile = float(getattr(args, 'min_percentile'))
            except ValueError:
                parser.error('min_percentile must be float')
            try:
                max_percentile = float(getattr(args, 'max_percentile'))
            except ValueError:
                parser.error('max_percentile must be float')
        elif getattr(args, 'mode') == 'moving_average':
            # Check dtype
            try:
                moving_avg_batch = int(getattr(args, 'moving_avg_batch'))
            except ValueError:
                parser.error('moving_avg_batch must be integer')
            try:
                moving_avg_const = float(getattr(args, 'moving_avg_const'))
            except ValueError:
                parser.error('moving_avg_const must be float')
        else:
            parser.error('Unsupported mode')


def _parse_arg(parser):
    args = parser.parse_args()
    # print version
    if args.version:
        oneutils.print_version_and_exit(__file__)

    return args


def _quantize(args):
    if oneutils.is_valid_attr(args, 'ampq'):
        _ampq_solve(args)
        return

    if oneutils.is_valid_attr(args, 'force_quantparam'):
        # write quantization parameters
        _write_qparam(args)
        return

    if oneutils.is_valid_attr(args, 'copy_quantparam'):
        # copy quantization parameters
        _copy_qparam(args)
        return

    if oneutils.is_valid_attr(args, 'fake_quantize'):
        # fake-quantize model
        _fake_quantize(args)
        return

    if oneutils.is_valid_attr(args, 'requantize'):
        # requantize model
        _requantize(args)
        return

    # get file path to log
    dir_path = os.path.dirname(os.path.realpath(__file__))
    logfile_path = os.path.realpath(args.output_path) + '.log'

    with open(logfile_path, 'wb') as f, tempfile.TemporaryDirectory() as tmpdir:
        if oneutils.is_valid_attr(args, 'save_intermediate'):
            tmpdir = os.path.dirname(logfile_path)
        # get driver path
        circle_quantizer_path = os.path.join(dir_path, 'circle-quantizer')
        record_minmax_path = os.path.join(dir_path, 'record-minmax')

        ## make a command to quantize and dequantize the weights of the model
        circle_quantizer_cmd = [circle_quantizer_path]
        # verbose
        if oneutils.is_valid_attr(args, 'verbose'):
            circle_quantizer_cmd.append('--verbose')
        # quantize_dequantize_weights
        circle_quantizer_cmd.append('--quantize_dequantize_weights')
        # Use input_model_dtype if it exists. Use input_dtype otherwise.
        if oneutils.is_valid_attr(args, 'input_model_dtype'):
            circle_quantizer_cmd.append(getattr(args, 'input_model_dtype'))
        elif oneutils.is_valid_attr(args, 'input_dtype'):
            circle_quantizer_cmd.append(getattr(args, 'input_dtype'))
        if oneutils.is_valid_attr(args, 'quantized_dtype'):
            circle_quantizer_cmd.append(getattr(args, 'quantized_dtype'))
        if oneutils.is_valid_attr(args, 'granularity'):
            circle_quantizer_cmd.append(getattr(args, 'granularity'))
        if oneutils.is_valid_attr(args, 'quant_config'):
            # NOTE --config conflicts with --config option in onecc, so
            # we use quant_config for one-quantize
            circle_quantizer_cmd.append('--config')
            circle_quantizer_cmd.append(getattr(args, 'quant_config'))
        # input and output path
        if oneutils.is_valid_attr(args, 'input_path'):
            circle_quantizer_cmd.append(getattr(args, 'input_path'))
        tmp_weights_fake_quant_path = os.path.join(
            tmpdir,
            os.path.splitext(os.path.basename(
                args.input_path))[0]) + '.weights_fake_quant.circle'
        circle_quantizer_cmd.append(tmp_weights_fake_quant_path)
        # profiling
        if oneutils.is_valid_attr(args, 'generate_profile_data'):
            circle_quantizer_cmd.append('--generate_profile_data')

        f.write((' '.join(circle_quantizer_cmd) + '\n').encode())

        # run circle-quantizer
        oneutils.run(circle_quantizer_cmd, err_prefix="circle_quantizer", logfile=f)

        tmp_minmax_recorded_path = os.path.join(
            tmpdir,
            os.path.splitext(os.path.basename(
                args.input_path))[0]) + '.minmax_recorded.circle'

        ## make a command to record min-max value of each tensor while running the representative dataset
        record_minmax_cmd = Command(record_minmax_path, args, f)
        record_minmax_cmd.add_noarg_option_if_valid_arg('--verbose', 'verbose') \
            .add_option_with_values('--input_model', [tmp_weights_fake_quant_path]) \
            .add_option_with_values('--output_model', [tmp_minmax_recorded_path]) \
            .add_option_with_valid_args('--input_data', ['input_data']) \
            .add_option_with_valid_args('--input_data_format', ['input_data_format']) \
            .add_option_with_valid_args('--min_percentile', ['min_percentile']) \
            .add_option_with_valid_args('--max_percentile', ['max_percentile']) \
            .add_option_with_valid_args('--moving_avg_batch', ['moving_avg_batch']) \
            .add_option_with_valid_args('--moving_avg_const', ['moving_avg_const']) \
            .add_option_with_valid_args('--mode', ['mode']) \
            .add_noarg_option_if_valid_arg('--generate_profile_data', 'generate_profile_data') \
            .run()

        ## make a second command to quantize the model using the embedded information
        circle_quantizer_cmd = [circle_quantizer_path]
        # verbose
        if oneutils.is_valid_attr(args, 'verbose'):
            circle_quantizer_cmd.append('--verbose')
        # quantize_dequantize_weights
        circle_quantizer_cmd.append('--quantize_with_minmax')
        # Use input_model_dtype if it exists. Use input_dtype otherwise.
        if oneutils.is_valid_attr(args, 'input_model_dtype'):
            circle_quantizer_cmd.append(getattr(args, 'input_model_dtype'))
        elif oneutils.is_valid_attr(args, 'input_dtype'):
            circle_quantizer_cmd.append(getattr(args, 'input_dtype'))
        if oneutils.is_valid_attr(args, 'quantized_dtype'):
            circle_quantizer_cmd.append(getattr(args, 'quantized_dtype'))
        if oneutils.is_valid_attr(args, 'granularity'):
            circle_quantizer_cmd.append(getattr(args, 'granularity'))
        if oneutils.is_valid_attr(args, 'TF-style_maxpool'):
            circle_quantizer_cmd.append('--TF-style_maxpool')
        if oneutils.is_valid_attr(args, 'input_type'):
            circle_quantizer_cmd.append('--input_type')
            circle_quantizer_cmd.append(getattr(args, 'input_type'))
        if oneutils.is_valid_attr(args, 'output_type'):
            circle_quantizer_cmd.append('--output_type')
            circle_quantizer_cmd.append(getattr(args, 'output_type'))
        if oneutils.is_valid_attr(args, 'quant_config'):
            # NOTE --config conflicts with --config option in onecc, so
            # we use quant_config for one-quantize
            circle_quantizer_cmd.append('--config')
            circle_quantizer_cmd.append(getattr(args, 'quant_config'))
        # input and output path
        circle_quantizer_cmd.append(tmp_minmax_recorded_path)
        if oneutils.is_valid_attr(args, 'output_path'):
            circle_quantizer_cmd.append(getattr(args, 'output_path'))
        # profiling
        if oneutils.is_valid_attr(args, 'generate_profile_data'):
            circle_quantizer_cmd.append('--generate_profile_data')

        f.write((' '.join(circle_quantizer_cmd) + '\n').encode())

        # run circle-quantizer
        oneutils.run(circle_quantizer_cmd, err_prefix="circle_quantizer", logfile=f)

        # evaluate
        if oneutils.is_valid_attr(args, 'evaluate_result'):
            circle_eval_diff_path = os.path.join(dir_path, 'circle-eval-diff')
            quant_model = ""
            if oneutils.is_valid_attr(args, 'output_path'):
                quant_model = getattr(args, 'output_path')
            tmp_fake_quant_model = os.path.join(
                tmpdir,
                os.path.splitext(os.path.basename(
                    args.input_path))[0]) + '.fake_quant.circle'

            # do fake quantization
            fake_quantize_cmd = Command(circle_quantizer_path, args, f)
            fake_quantize_cmd.add_noarg_option_if_valid_arg('--verbose', 'verbose') \
                .add_option_with_values('--fake_quantize', [quant_model, tmp_fake_quant_model]) \
                .run()

            # compare fake-quant model and fp32 model
            circle_eval_diff_cmd = Command(circle_eval_diff_path, args, f)
            circle_eval_diff_cmd.add_option_with_valid_args('--first_model', ['input_path']) \
                .add_option_with_values('--second_model', [tmp_fake_quant_model]) \
                .add_option_with_valid_args('--first_input_data', ['test_data']) \
                .add_option_with_valid_args('--second_input_data', ['test_data']) \
                .add_option_with_valid_args('--input_data_format', ['input_data_format']) \
                .add_noarg_option_if_valid_arg('--print_mae', 'print_mae') \
                .add_noarg_option_if_valid_arg('--print_mape', 'print_mape') \
                .add_noarg_option_if_valid_arg('--print_mpeir', 'print_mpeir') \
                .add_noarg_option_if_valid_arg('--print_top1_match', 'print_top1_match') \
                .add_noarg_option_if_valid_arg('--print_top5_match', 'print_top5_match') \
                .add_noarg_option_if_valid_arg('--print_mse', 'print_mse') \
                .run()


def _write_qparam(args):
    # get file path to log
    dir_path = os.path.dirname(os.path.realpath(__file__))
    logfile_path = os.path.realpath(args.output_path) + '.log'

    with open(logfile_path, 'wb') as f:
        # get driver path
        circle_quantizer_path = os.path.join(dir_path, 'circle-quantizer')

        # make a command to write qparams to the tensors
        circle_quantizer_cmd = [circle_quantizer_path]
        # verbose
        if oneutils.is_valid_attr(args, 'verbose'):
            circle_quantizer_cmd.append('--verbose')
        if oneutils.is_valid_attr(args, 'tensor_name'):
            tensor_name = getattr(args, 'tensor_name')
        if oneutils.is_valid_attr(args, 'scale'):
            scale = getattr(args, 'scale')
        if oneutils.is_valid_attr(args, 'zero_point'):
            zero_point = getattr(args, 'zero_point')
        for (t, s, zp) in zip(tensor_name, scale, zero_point):
            circle_quantizer_cmd.append('--force_quantparam')
            circle_quantizer_cmd.append(t)
            circle_quantizer_cmd.append(str(s))
            circle_quantizer_cmd.append(str(zp))
        # input and output path
        if oneutils.is_valid_attr(args, 'input_path'):
            circle_quantizer_cmd.append(getattr(args, 'input_path'))
        if oneutils.is_valid_attr(args, 'output_path'):
            circle_quantizer_cmd.append(getattr(args, 'output_path'))

        f.write((' '.join(circle_quantizer_cmd) + '\n').encode())

        # run circle-quantizer
        oneutils.run(circle_quantizer_cmd, err_prefix="circle_quantizer", logfile=f)


def _copy_qparam(args):
    # get file path to log
    dir_path = os.path.dirname(os.path.realpath(__file__))
    logfile_path = os.path.realpath(args.output_path) + '.log'

    with open(logfile_path, 'wb') as f:
        # get driver path
        circle_quantizer_path = os.path.join(dir_path, 'circle-quantizer')

        # make a command to write qparams to the tensors
        circle_quantizer_cmd = [circle_quantizer_path]
        # verbose
        if oneutils.is_valid_attr(args, 'verbose'):
            circle_quantizer_cmd.append('--verbose')
        if oneutils.is_valid_attr(args, 'src_tensor_name'):
            src_tensor_name = getattr(args, 'src_tensor_name')
        if oneutils.is_valid_attr(args, 'dst_tensor_name'):
            dst_tensor_name = getattr(args, 'dst_tensor_name')
        for (src, dst) in zip(src_tensor_name, dst_tensor_name):
            circle_quantizer_cmd.append('--copy_quantparam')
            circle_quantizer_cmd.append(src)
            circle_quantizer_cmd.append(dst)
        # input and output path
        if oneutils.is_valid_attr(args, 'input_path'):
            circle_quantizer_cmd.append(getattr(args, 'input_path'))
        if oneutils.is_valid_attr(args, 'output_path'):
            circle_quantizer_cmd.append(getattr(args, 'output_path'))

        f.write((' '.join(circle_quantizer_cmd) + '\n').encode())

        # run circle-quantizer
        oneutils.run(circle_quantizer_cmd, err_prefix="circle_quantizer", logfile=f)


def _fake_quantize(args):
    # get file path to log
    dir_path = os.path.dirname(os.path.realpath(__file__))
    logfile_path = os.path.realpath(args.output_path) + '.log'

    with open(logfile_path, 'wb') as f:
        # get driver path
        circle_quantizer_path = os.path.join(dir_path, 'circle-quantizer')
        q_model = getattr(args, 'input_path')
        fq_model = getattr(args, 'output_path')

        # do fake quantization
        fake_quantize_cmd = Command(circle_quantizer_path, args, f)
        fake_quantize_cmd.add_noarg_option_if_valid_arg('--verbose', 'verbose') \
            .add_option_with_values('--fake_quantize', [q_model, fq_model]) \
            .run()


def _ampq_solve(args):
    # get file path to log
    dir_path = os.path.dirname(os.path.realpath(__file__))
    logfile_path = os.path.realpath(args.output_path) + '.log'

    with open(logfile_path, 'wb') as f, tempfile.TemporaryDirectory() as tmpdir:
        if oneutils.is_valid_attr(args, 'save_intermediate'):
            tmpdir = os.path.dirname(logfile_path)

        # get driver path
        record_minmax_path = os.path.join(dir_path, 'record-minmax')

        tmp_minmax_recorded_path = os.path.join(
            tmpdir,
            os.path.splitext(os.path.basename(
                args.input_path))[0]) + '.minmax_recorded.circle'

        ## make a command to record min-max value of each tensor while running the representative dataset
        record_minmax_cmd = Command(record_minmax_path, args, f)
        record_minmax_cmd.add_noarg_option_if_valid_arg('--verbose', 'verbose') \
            .add_option_with_valid_args('--input_model', ['input_path']) \
            .add_option_with_values('--output_model', [tmp_minmax_recorded_path]) \
            .add_option_with_valid_args('--input_data', ['input_data']) \
            .add_option_with_valid_args('--input_data_format', ['input_data_format']) \
            .add_option_with_valid_args('--min_percentile', ['min_percentile']) \
            .add_option_with_valid_args('--max_percentile', ['max_percentile']) \
            .add_option_with_valid_args('--moving_avg_batch', ['moving_avg_batch']) \
            .add_option_with_valid_args('--moving_avg_const', ['moving_avg_const']) \
            .add_option_with_valid_args('--mode', ['mode']) \
            .add_noarg_option_if_valid_arg('--generate_profile_data', 'generate_profile_data') \
            .run()

        # process visq if needed
        visq_file = None
        if oneutils.is_valid_attr(args, 'ampq_bisection_visq'):
            visq_file = getattr(args, 'ampq_bisection_visq')

        if (oneutils.is_valid_attr(args, 'ampq_algorithm')
                and oneutils.is_valid_attr(args, 'bisection_type')):
            algorithm = getattr(args, 'ampq_algorithm')
            bisection_type = getattr(args, 'bisection_type')
            if algorithm == 'bisection' and bisection_type == 'auto' and visq_file is None:
                # algorithm needs bisection but no file in input configuration

                # to compute visq file we need q8 quantized model
                q8_file = os.path.join(
                    tmpdir,
                    os.path.splitext(os.path.basename(
                        args.input_path))[0]) + '.visq.q8.circle'

                # get drievr path
                circle_quantizer_path = os.path.join(dir_path, 'circle-quantizer')
                circle_quantizer_cmd = [circle_quantizer_path]
                # verbose
                if oneutils.is_valid_attr(args, 'verbose'):
                    circle_quantizer_cmd.append('--verbose')
                circle_quantizer_cmd.append('--quantize_with_minmax')
                circle_quantizer_cmd.append('float32')
                circle_quantizer_cmd.append('uint8')
                circle_quantizer_cmd.append('channel')

                if oneutils.is_valid_attr(args, 'TF-style_maxpool'):
                    circle_quantizer_cmd.append('--TF-style_maxpool')

                circle_quantizer_cmd.extend(['--input_type', 'uint8'])
                circle_quantizer_cmd.extend(['--output_type', 'uint8'])

                # input and output paths
                circle_quantizer_cmd.append(tmp_minmax_recorded_path)
                circle_quantizer_cmd.append(q8_file)

                f.write((' '.join(circle_quantizer_cmd) + '\n').encode())

                # run circle-quantizer
                oneutils.run(
                    circle_quantizer_cmd, err_prefix="circle_quantizer", logfile=f)

                # compute visq file
                visq_path = os.path.join(dir_path, 'visq')

                visq_file = os.path.join(
                    tmpdir,
                    os.path.splitext(os.path.basename(
                        args.input_path))[0]) + '.tae.visq.json'

                visq_cmd = [visq_path]
                visq_cmd.extend(['--fp32_circle', getattr(args, 'input_path')])
                visq_cmd.extend(['--data', getattr(args, 'input_data')])
                visq_cmd.extend(['--q_circle', q8_file])
                visq_cmd.extend(['--tae_output', visq_file])
                visq_cmd.extend(['--batch_size', "1"])
                visq_cmd.append('--dump_dot_graph')
                f.write((' '.join(visq_cmd) + '\n').encode())

                # run visq
                oneutils.run(visq_cmd, err_prefix="visq", logfile=f)

        # get driver path
        circle_mpqsolver_path = os.path.join(dir_path, 'circle-mpqsolver')

        # solve for Mixed Precision Quantization configuration
        ampq_quantize_cmd = [circle_mpqsolver_path]

        # data
        if oneutils.is_valid_attr(args, 'input_data'):
            ampq_quantize_cmd.extend(['--data', getattr(args, 'input_data')])

        # data format
        if oneutils.is_valid_attr(args, 'input_data_format'):
            ampq_quantize_cmd.extend(
                ['--data_format', getattr(args, 'input_data_format')])

        # qerror_ratio
        if oneutils.is_valid_attr(args, 'ampq_qerror_ratio'):
            ampq_quantize_cmd.extend(
                ['--qerror_ratio', getattr(args, 'ampq_qerror_ratio')])

        # algorithm
        if oneutils.is_valid_attr(args, 'ampq_algorithm'):
            algorithm = getattr(args, 'ampq_algorithm')
            if algorithm == 'bisection':
                if oneutils.is_valid_attr(args, 'bisection_type'):
                    bisection_type = getattr(args, 'bisection_type')
                    if bisection_type == 'auto':
                        ampq_quantize_cmd.extend(['--bisection', 'auto'])
                    elif bisection_type == 'i16_front':
                        ampq_quantize_cmd.extend(['--bisection', 'true'])
                    elif bisection_type == 'i16_back':
                        ampq_quantize_cmd.extend(['--bisection', 'false'])
            elif algorithm == 'pattern':
                ampq_quantize_cmd.append('--patterns')
                if oneutils.is_valid_attr(args, 'u8_layernorm_with_s16_variance'):
                    ampq_quantize_cmd.append('--u8_layernorm_with_s16_variance')
                if oneutils.is_valid_attr(args, 'u8_softmax_with_s16_sub_exp'):
                    ampq_quantize_cmd.append('--u8_softmax_with_s16_sub_exp')

        # recorded model as input
        ampq_quantize_cmd.extend(['--input_model', tmp_minmax_recorded_path])

        # input_dtype
        if oneutils.is_valid_attr(args, 'input_type'):
            ampq_quantize_cmd.extend(['--input_dtype', getattr(args, 'input_type')])

        # output dtype
        if oneutils.is_valid_attr(args, 'output_type'):
            ampq_quantize_cmd.extend(['--output_dtype', getattr(args, 'output_type')])

        # output model
        if oneutils.is_valid_attr(args, 'output_path'):
            ampq_quantize_cmd.extend(['--output_model', getattr(args, 'output_path')])

        # visq_file
        if not (visq_file is None):
            ampq_quantize_cmd.extend(['--visq_file', visq_file])

        # save_intermediate
        if oneutils.is_valid_attr(args, 'save_intermediate'):
            intermediate_dir = os.path.dirname(logfile_path)
            ampq_quantize_cmd.extend(['--save_intermediate', intermediate_dir])

        if oneutils.is_valid_attr(args, 'verbose'):
            ampq_quantize_cmd.append('--verbose')

        f.write((' '.join(ampq_quantize_cmd) + '\n').encode())

        # run ampq
        oneutils.run(ampq_quantize_cmd, err_prefix="circle_mpqsolver", logfile=f)


def _requantize(args):
    # get file path to log
    dir_path = os.path.dirname(os.path.realpath(__file__))
    logfile_path = os.path.realpath(args.output_path) + '.log'

    with open(logfile_path, 'wb') as f:
        # get driver path
        circle_quantizer_path = os.path.join(dir_path, 'circle-quantizer')

        ## make a command to quantize and dequantize the weights of the model
        circle_quantizer_cmd = [circle_quantizer_path]
        # verbose
        if oneutils.is_valid_attr(args, 'verbose'):
            circle_quantizer_cmd.append('--verbose')
        # requantize
        circle_quantizer_cmd.append('--requantize')
        # Use input_model_dtype if it exists. Use input_dtype otherwise.
        if oneutils.is_valid_attr(args, 'input_model_dtype'):
            circle_quantizer_cmd.append(getattr(args, 'input_model_dtype'))
        elif oneutils.is_valid_attr(args, 'input_dtype'):
            circle_quantizer_cmd.append(getattr(args, 'input_dtype'))
        if oneutils.is_valid_attr(args, 'quantized_dtype'):
            circle_quantizer_cmd.append(getattr(args, 'quantized_dtype'))
        # input and output path
        if oneutils.is_valid_attr(args, 'input_path'):
            circle_quantizer_cmd.append(getattr(args, 'input_path'))
        if oneutils.is_valid_attr(args, 'output_path'):
            circle_quantizer_cmd.append(getattr(args, 'output_path'))

        f.write((' '.join(circle_quantizer_cmd) + '\n').encode())

        # run circle-quantizer
        oneutils.run(circle_quantizer_cmd, err_prefix="circle_quantizer", logfile=f)


def main():
    # parse arguments
    parser = _get_parser()
    args = _parse_arg(parser)

    # parse configuration file
    oneutils.parse_cfg(args.config, 'one-quantize', args)

    # verify arguments before default value setting
    _verify_arg_pre(parser, args)

    # set default values
    _set_default_values(args)

    # verify arguments
    _verify_arg(parser, args)

    # quantize
    _quantize(args)


if __name__ == '__main__':
    oneutils.safemain(main, __file__)
