; To activate a step (or task),
; set True for the step in [onecc] section and fill options in the corresponding section
[onecc]
; neural network model to circle
one-import-tf=False
one-import-tflite=False
one-import-bcq=False
one-import-onnx=False
; circle to circle with optimization
one-optimize=False
; circle to circle with quantization
one-quantize=False
; partition circle
one-partition=False
; package circle and metadata into nnpackage
one-pack=False
; generate code for backend
one-codegen=False
; profile
one-profile=False

[one-import-tf]
# mandatory
; pb file
input_path=
; circle file
output_path=
# optional
; v1 or v2
converter_version=v2
# optional but mandatory for converter_version=v1
; input tensor names
input_arrays=
; eg) 1,299,299,3
input_shapes=
; output tensor names
output_arrays=

[one-import-tflite]
# mandatory
; tflite file
input_path=
; circle file
output_path=

[one-import-bcq]
# mandatory
; bcq file
input_path=
; circle file
output_path=
# optional
; v1 or v2
converter_version=v2
# optional but mandatory for converter_version=v1
; input tensor names
input_arrays=
; eg) 1,299,299,3
input_shapes=
; output tensor names
output_arrays=

[one-import-onnx]
# mandatory
; onnx file
input_path=
; circle file
output_path=
# optional
; input tensor names
input_arrays=
; output tensor names
output_arrays=
; True or False
unroll_rnn=
; True or False
unroll_lstm=

[one-optimize]
# mandatory
; circle file
input_path=
; circle file
output_path=
# optional
; optimization: DEFAULT
O=DEFAULT

[one-quantize]
# mandatory
; circle file
input_path=
; circle file
output_path=
# optional arguments for quantization
; input data file (if not given, random data will be used for calibration)
input_data=
; h5/hdf5 (default), list/filelist, or dir/directory
input_data_format=
; dtype of quantized model (uint8 (default), int16)
quantized_dtype=
; granularity of quantization (layer (default), channel)
granularity=
; dtype of model's input (uint8, int16, float32). Same with quantized_dtype by default.
input_type=
; dtype of model's output (uint8, int16, float32). Same with quantized_dtype by default.
output_type=

[one-partition]
# mandatory
; partition file which provides backend to assign
part_file=
; circle file
input_file=
# //TODO: Add available options

[one-pack]
# mandatory
; input path
input_path=
; output path
output_path=
# //TODO: Add available options

[one-codegen]
# mandatory
; backend name
backend=
; commands for each backend
command=

[one-profile]
# mandatory
; backend name
backend=
# //TODO: Add available options
