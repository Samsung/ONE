; To activate each step(or task),
; change to True for each [onecc] items and fill sub-items in each corresponding section
[onecc]
; neural network model to circle
one-import-tf=False
one-import-tflite=False
one-import-bcq=False
one-import-onnx=False
; circle to circle with optimization
one-optimize=False
; circle to circle with quantization
one-quantize=False
; partition circle
one-partition=False
; package circle and metadata into nnpackage
one-pack=False
; codegen from circle to backend's type
one-codegen=False
; profile
one-profile=False

[one-import-tf]
; mandatory
input_path=             ; pb file
output_path=            ; circle file
; optional but mandatory for v1
input_arrays=           ; input tensor names
input_shapes=           ; eg) 1,299,299,3
output_arrays=          ; output tensor names
; optional
converter_version=      ; converter version: v1 or v2
model_format=graph_def  ; graph_def, saved_model or keras_model

[one-import-tflite]
; mandatory
input_path=        ; tflite file
output_path=       ; circle file

[one-import-bcq]
; mandatory
input_path=        ; bcq file
output_path=       ; circle file
input_arrays=      ; input tensor names
input_shapes=      ; eg) 1,299,299,3
output_arrays=     ; output tensor names
converter_version= ; tf2tflite converter version: v1 or v2

[one-import-onnx]
; mandatory
input_path=              ; onnx file
output_path=             ; circle file
input_arrays=            ; input tensor names
output_arrays=           ; output tensor names
; optional
model_format=saved_model ; as default
unroll_rnn=              ; True or False
unroll_lstm=             ; True or False

[one-optimize]
; mandatory
input_path=                 ; circle file
output_path=                ; circle file
; optional
generate_profile_data=False ; True or False
; //TODO: Add available options

[one-quantize]
; mandatory
input_path=                 ; circle file
output_path=                ; circle file
; optional argument for profiling
generate_profile_data=False ; True or False
; optional arguments for quantization
input_data=                 ; input data file (if not given, random data will be used for calibration)
input_data_format=          ; h5/hdf5 (default), list/filelist, or dir/directory
quantized_dtype=            ; dtype of quantized model (uint8 (default), int16)
granularity=                ; granularity of quantization (layer (default), channel)
input_type=                 ; dtype of model's input (uint8, int16, float32). Same with quantized_dtype by default.
output_type=                ; dtype of model's output (uint8, int16, float32). Same with quantized_dtype by default.

[one-partition]
; //TODO: Add available options

[one-pack]
; //TODO: Add available options

[one-codegen]
; mandatory
backend= ; backend name
command= ; commands for each backend

[one-profile]
; //TODO: Add available options
