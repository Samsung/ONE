<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "https://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en-US">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=11"/>
<meta name="generator" content="Doxygen 1.9.5"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<title>ONE - On-device Neural Engine: arm_compute::CLFullyConnectedHybridLayer Class Reference</title>
<link href="tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="dynsections.js"></script>
<link href="navtree.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="resize.js"></script>
<script type="text/javascript" src="navtreedata.js"></script>
<script type="text/javascript" src="navtree.js"></script>
<link href="search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="search/searchdata.js"></script>
<script type="text/javascript" src="search/search.js"></script>
<link href="doxygen.css" rel="stylesheet" type="text/css" />
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr id="projectrow">
  <td id="projectalign">
   <div id="projectname">ONE - On-device Neural Engine
   </div>
  </td>
 </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.9.5 -->
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:d3d9a9a6595521f9666a5e94cc830dab83b65699&amp;dn=expat.txt MIT */
var searchBox = new SearchBox("searchBox", "search/",'.html');
/* @license-end */
</script>
<script type="text/javascript" src="menudata.js"></script>
<script type="text/javascript" src="menu.js"></script>
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:d3d9a9a6595521f9666a5e94cc830dab83b65699&amp;dn=expat.txt MIT */
$(function() {
  initMenu('',true,false,'search.php','Search');
  $(document).ready(function() { init_search(); });
});
/* @license-end */
</script>
<div id="main-nav"></div>
</div><!-- top -->
<div id="side-nav" class="ui-resizable side-nav-resizable">
  <div id="nav-tree">
    <div id="nav-tree-contents">
      <div id="nav-sync" class="sync"></div>
    </div>
  </div>
  <div id="splitbar" style="-moz-user-select:none;" 
       class="ui-resizable-handle">
  </div>
</div>
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:d3d9a9a6595521f9666a5e94cc830dab83b65699&amp;dn=expat.txt MIT */
$(document).ready(function(){initNavTree('classarm__compute_1_1_c_l_fully_connected_hybrid_layer.html',''); initResizable(); });
/* @license-end */
</script>
<div id="doc-content">
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<div id="MSearchResults">
<div class="SRPage">
<div id="SRIndex">
<div id="SRResults"></div>
<div class="SRStatus" id="Loading">Loading...</div>
<div class="SRStatus" id="Searching">Searching...</div>
<div class="SRStatus" id="NoMatches">No Matches</div>
</div>
</div>
</div>
</div>

<div class="header">
  <div class="summary">
<a href="#pub-methods">Public Member Functions</a> &#124;
<a href="#pub-static-methods">Static Public Member Functions</a>  </div>
  <div class="headertitle"><div class="title">arm_compute::CLFullyConnectedHybridLayer Class Reference</div></div>
</div><!--header-->
<div class="contents">

<p><code>#include &lt;<a class="el" href="_c_l_fully_connected_hybrid_layer_8h_source.html">CLFullyConnectedHybridLayer.h</a>&gt;</code></p>
<div class="dynheader">
Collaboration diagram for arm_compute::CLFullyConnectedHybridLayer:</div>
<div class="dyncontent">
<div class="center"><img src="classarm__compute_1_1_c_l_fully_connected_hybrid_layer__coll__graph.png" border="0" usemap="#aarm__compute_1_1_c_l_fully_connected_hybrid_layer_coll__map" alt="Collaboration graph"/></div>
<map name="aarm__compute_1_1_c_l_fully_connected_hybrid_layer_coll__map" id="aarm__compute_1_1_c_l_fully_connected_hybrid_layer_coll__map">
<area shape="rect" title=" " alt="" coords="5,79,219,119"/>
<area shape="rect" title=" " alt="" coords="74,5,150,31"/>
</map>
<center><span class="legend">[<a target="top" href="graph_legend.html">legend</a>]</span></center></div>
<table class="memberdecls">
<tr class="heading"><td colspan="2"><h2 class="groupheader"><a id="pub-methods" name="pub-methods"></a>
Public Member Functions</h2></td></tr>
<tr class="memitem:a396056feede413f56d5edb106d8c9023"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classarm__compute_1_1_c_l_fully_connected_hybrid_layer.html#a396056feede413f56d5edb106d8c9023">CLFullyConnectedHybridLayer</a> (std::shared_ptr&lt; IMemoryManager &gt; memory_manager=nullptr)</td></tr>
<tr class="separator:a396056feede413f56d5edb106d8c9023"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ae76d08f99e602281d78f932d0e2618ec"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classarm__compute_1_1_c_l_fully_connected_hybrid_layer.html#ae76d08f99e602281d78f932d0e2618ec">CLFullyConnectedHybridLayer</a> (const <a class="el" href="classarm__compute_1_1_c_l_fully_connected_hybrid_layer.html">CLFullyConnectedHybridLayer</a> &amp;)=delete</td></tr>
<tr class="separator:ae76d08f99e602281d78f932d0e2618ec"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a13e5e472e19f8b47a7fc3cc7777631ad"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classarm__compute_1_1_c_l_fully_connected_hybrid_layer.html#a13e5e472e19f8b47a7fc3cc7777631ad">CLFullyConnectedHybridLayer</a> (<a class="el" href="classarm__compute_1_1_c_l_fully_connected_hybrid_layer.html">CLFullyConnectedHybridLayer</a> &amp;&amp;)=default</td></tr>
<tr class="separator:a13e5e472e19f8b47a7fc3cc7777631ad"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:af9e2f8f242e2c1f2e2c2897efafa98f6"><td class="memItemLeft" align="right" valign="top"><a class="el" href="classarm__compute_1_1_c_l_fully_connected_hybrid_layer.html">CLFullyConnectedHybridLayer</a> &amp;&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classarm__compute_1_1_c_l_fully_connected_hybrid_layer.html#af9e2f8f242e2c1f2e2c2897efafa98f6">operator=</a> (const <a class="el" href="classarm__compute_1_1_c_l_fully_connected_hybrid_layer.html">CLFullyConnectedHybridLayer</a> &amp;)=delete</td></tr>
<tr class="separator:af9e2f8f242e2c1f2e2c2897efafa98f6"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:afe842e9374ac19e2fa9b7eae8f662dfe"><td class="memItemLeft" align="right" valign="top"><a class="el" href="classarm__compute_1_1_c_l_fully_connected_hybrid_layer.html">CLFullyConnectedHybridLayer</a> &amp;&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classarm__compute_1_1_c_l_fully_connected_hybrid_layer.html#afe842e9374ac19e2fa9b7eae8f662dfe">operator=</a> (<a class="el" href="classarm__compute_1_1_c_l_fully_connected_hybrid_layer.html">CLFullyConnectedHybridLayer</a> &amp;&amp;)=default</td></tr>
<tr class="separator:afe842e9374ac19e2fa9b7eae8f662dfe"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a957872db06dc21228ebcd9eac958a7e2"><td class="memItemLeft" align="right" valign="top">void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classarm__compute_1_1_c_l_fully_connected_hybrid_layer.html#a957872db06dc21228ebcd9eac958a7e2">configure</a> (const ICLTensor *input, const ICLTensor *weights, const ICLTensor *biases, ICLTensor *output, FullyConnectedLayerInfo fc_info=FullyConnectedLayerInfo())</td></tr>
<tr class="separator:a957872db06dc21228ebcd9eac958a7e2"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a2188556b885f502ddd78db7e1b56fa17"><td class="memItemLeft" align="right" valign="top">void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classarm__compute_1_1_c_l_fully_connected_hybrid_layer.html#a2188556b885f502ddd78db7e1b56fa17">run</a> () override</td></tr>
<tr class="separator:a2188556b885f502ddd78db7e1b56fa17"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a9a6ac7ee73b176969787dc392b86c3e2"><td class="memItemLeft" align="right" valign="top">void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classarm__compute_1_1_c_l_fully_connected_hybrid_layer.html#a9a6ac7ee73b176969787dc392b86c3e2">prepare</a> () override</td></tr>
<tr class="separator:a9a6ac7ee73b176969787dc392b86c3e2"><td class="memSeparator" colspan="2">&#160;</td></tr>
</table><table class="memberdecls">
<tr class="heading"><td colspan="2"><h2 class="groupheader"><a id="pub-static-methods" name="pub-static-methods"></a>
Static Public Member Functions</h2></td></tr>
<tr class="memitem:a853272d8cb83134d303afb7e4ac9f94f"><td class="memItemLeft" align="right" valign="top">static Status&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classarm__compute_1_1_c_l_fully_connected_hybrid_layer.html#a853272d8cb83134d303afb7e4ac9f94f">validate</a> (const ITensorInfo *input, const ITensorInfo *weights, const ITensorInfo *biases, const ITensorInfo *output, FullyConnectedLayerInfo fc_info=FullyConnectedLayerInfo())</td></tr>
<tr class="separator:a853272d8cb83134d303afb7e4ac9f94f"><td class="memSeparator" colspan="2">&#160;</td></tr>
</table>
<a name="details" id="details"></a><h2 class="groupheader">Detailed Description</h2>
<div class="textblock"><p >Basic function to compute a Fully Connected layer on OpenCL. This function calls the following OpenCL kernels:</p>
<ol type="1">
<li>CLIm2ColKernel (called when the input comes from a convolutional layer)</li>
<li><a class="el" href="classarm__compute_1_1_c_l_fully_connected_hybrid_layer_reshape_weights.html">CLFullyConnectedHybridLayerReshapeWeights</a> (if <code>are_weights_reshaped</code> is set to false and transpose_weights is set to true ) (called once)</li>
<li>CLGEMMLowpMatrixMultiplyCore (if quantized symmetric)</li>
<li><a class="el" href="classarm__compute_1_1_c_l_g_e_m_m_matrix_accumulate_biases_kernel.html">CLGEMMMatrixAccumulateBiasesKernel</a> (if <code>biases</code> is not equal to nullptr)</li>
</ol>
<dl class="section note"><dt>Note</dt><dd>The fully connected layer accepts "weights" tensors only with 2 dimensions. </dd></dl>

<p class="definition">Definition at line <a class="el" href="_c_l_fully_connected_hybrid_layer_8h_source.html#l00099">99</a> of file <a class="el" href="_c_l_fully_connected_hybrid_layer_8h_source.html">CLFullyConnectedHybridLayer.h</a>.</p>
</div><h2 class="groupheader">Constructor &amp; Destructor Documentation</h2>
<a id="a396056feede413f56d5edb106d8c9023" name="a396056feede413f56d5edb106d8c9023"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a396056feede413f56d5edb106d8c9023">&#9670;&#160;</a></span>CLFullyConnectedHybridLayer() <span class="overload">[1/3]</span></h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">CLFullyConnectedHybridLayer::CLFullyConnectedHybridLayer </td>
          <td>(</td>
          <td class="paramtype">std::shared_ptr&lt; IMemoryManager &gt;&#160;</td>
          <td class="paramname"><em>memory_manager</em> = <code>nullptr</code></td><td>)</td>
          <td></td>
        </tr>
      </table>
</div><div class="memdoc">
<p >Constructor </p>

<p class="definition">Definition at line <a class="el" href="_c_l_fully_connected_hybrid_layer_8cpp_source.html#l00081">81</a> of file <a class="el" href="_c_l_fully_connected_hybrid_layer_8cpp_source.html">CLFullyConnectedHybridLayer.cpp</a>.</p>
<div class="fragment"><div class="line"><span class="lineno">   83</span>  : _memory_group(memory_manager), _reshape_weights_kernel(), _quant_input_kernel(),</div>
<div class="line"><span class="lineno">   84</span>    _mm_gemmlowp(memory_manager), _multiply_scale_kernel(), _accumulate_biases_kernel(),</div>
<div class="line"><span class="lineno">   85</span>    _reshape_weights_output(), _quantized_input(), _scale_factor(), _gemmlowp_output(),</div>
<div class="line"><span class="lineno">   86</span>    _are_weights_reshaped(<span class="keyword">true</span>), _accumulate_biases(<span class="keyword">false</span>), _is_prepared(<span class="keyword">false</span>),</div>
<div class="line"><span class="lineno">   87</span>    _original_weights(<span class="keyword">nullptr</span>)</div>
<div class="line"><span class="lineno">   88</span>{</div>
<div class="line"><span class="lineno">   89</span>}</div>
</div><!-- fragment -->
</div>
</div>
<a id="ae76d08f99e602281d78f932d0e2618ec" name="ae76d08f99e602281d78f932d0e2618ec"></a>
<h2 class="memtitle"><span class="permalink"><a href="#ae76d08f99e602281d78f932d0e2618ec">&#9670;&#160;</a></span>CLFullyConnectedHybridLayer() <span class="overload">[2/3]</span></h2>

<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">arm_compute::CLFullyConnectedHybridLayer::CLFullyConnectedHybridLayer </td>
          <td>(</td>
          <td class="paramtype">const <a class="el" href="classarm__compute_1_1_c_l_fully_connected_hybrid_layer.html">CLFullyConnectedHybridLayer</a> &amp;&#160;</td>
          <td class="paramname"></td><td>)</td>
          <td></td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">delete</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">
<p >Prevent instances of this class from being copied (As this class contains pointers) </p>

</div>
</div>
<a id="a13e5e472e19f8b47a7fc3cc7777631ad" name="a13e5e472e19f8b47a7fc3cc7777631ad"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a13e5e472e19f8b47a7fc3cc7777631ad">&#9670;&#160;</a></span>CLFullyConnectedHybridLayer() <span class="overload">[3/3]</span></h2>

<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">arm_compute::CLFullyConnectedHybridLayer::CLFullyConnectedHybridLayer </td>
          <td>(</td>
          <td class="paramtype"><a class="el" href="classarm__compute_1_1_c_l_fully_connected_hybrid_layer.html">CLFullyConnectedHybridLayer</a> &amp;&amp;&#160;</td>
          <td class="paramname"></td><td>)</td>
          <td></td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">default</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">
<p >Default move constructor </p>

</div>
</div>
<h2 class="groupheader">Member Function Documentation</h2>
<a id="a957872db06dc21228ebcd9eac958a7e2" name="a957872db06dc21228ebcd9eac958a7e2"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a957872db06dc21228ebcd9eac958a7e2">&#9670;&#160;</a></span>configure()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">void CLFullyConnectedHybridLayer::configure </td>
          <td>(</td>
          <td class="paramtype">const ICLTensor *&#160;</td>
          <td class="paramname"><em>input</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const ICLTensor *&#160;</td>
          <td class="paramname"><em>weights</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const ICLTensor *&#160;</td>
          <td class="paramname"><em>biases</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">ICLTensor *&#160;</td>
          <td class="paramname"><em>output</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">FullyConnectedLayerInfo&#160;</td>
          <td class="paramname"><em>fc_info</em> = <code>FullyConnectedLayerInfo()</code>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">
<p >Set the input and output tensors.</p>
<dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramdir">[in]</td><td class="paramname">input</td><td>Source tensor. Data type supported: F16/F32. </td></tr>
    <tr><td class="paramdir">[in]</td><td class="paramname">weights</td><td>Weights tensor. The weights must be 2 dimensional. If this function is called after a Convolution Layer, the (transposed) weights will have as many rows as the product of the first 3 input's dimensions. If it is called after another FullyConnected Layer, the (transposed) weights will have as many rows as the input's first dimension. Data type supported: S8. </td></tr>
    <tr><td class="paramdir">[in]</td><td class="paramname">biases</td><td>Bias tensor. Can be nullptr. Data type supported:Same as <code>input</code>. </td></tr>
    <tr><td class="paramdir">[out]</td><td class="paramname">output</td><td>Destination tensor. Its shape should be equal to the output of a matrix multiplication between:<ul>
<li>The output of im2col on the input and the (transposed) 2D weights, if the function is called after a Convolution Layer</li>
<li>The input tensor and the (transposed) 2D weights, if the function is called after another FullyConnected Layer. Data type supported: Same as <code>input</code>. </li>
</ul>
</td></tr>
    <tr><td class="paramdir">[in]</td><td class="paramname">fc_info</td><td>(Optional) Fully connected layer additional info </td></tr>
  </table>
  </dd>
</dl>

<p class="definition">Definition at line <a class="el" href="_c_l_fully_connected_hybrid_layer_8cpp_source.html#l00101">101</a> of file <a class="el" href="_c_l_fully_connected_hybrid_layer_8cpp_source.html">CLFullyConnectedHybridLayer.cpp</a>.</p>
<div class="fragment"><div class="line"><span class="lineno">  104</span>{</div>
<div class="line"><span class="lineno">  105</span>  ARM_COMPUTE_ERROR_ON_NULLPTR(input, weights, output);</div>
<div class="line"><span class="lineno">  106</span> </div>
<div class="line"><span class="lineno">  107</span>  <span class="comment">// Perform validate step</span></div>
<div class="line"><span class="lineno">  108</span>  ARM_COMPUTE_ERROR_THROW_ON(<a class="code hl_function" href="classarm__compute_1_1_c_l_fully_connected_hybrid_layer.html#a853272d8cb83134d303afb7e4ac9f94f">CLFullyConnectedHybridLayer::validate</a>(</div>
<div class="line"><span class="lineno">  109</span>    <a class="code hl_variable" href="namespacegen__h5__explicit__inputs.html#a48dd077479f23bb4552c2d7d6a7a4d37">input</a>-&gt;info(), weights-&gt;info(), biases != <span class="keyword">nullptr</span> ? biases-&gt;info() : <span class="keyword">nullptr</span>, <a class="code hl_variable" href="namespacegen__h5__explicit__inputs.html#acd1aa9ba45d45c6b619b723e6e34c576">output</a>-&gt;info(),</div>
<div class="line"><span class="lineno">  110</span>    fc_info));</div>
<div class="line"><span class="lineno">  111</span> </div>
<div class="line"><span class="lineno">  112</span>  _are_weights_reshaped = fc_info.transpose_weights ? fc_info.are_weights_reshaped : <span class="keyword">true</span>;</div>
<div class="line"><span class="lineno">  113</span>  _accumulate_biases = <span class="keyword">false</span>;</div>
<div class="line"><span class="lineno">  114</span>  _is_prepared = fc_info.retain_internal_weights;</div>
<div class="line"><span class="lineno">  115</span>  _original_weights = weights;</div>
<div class="line"><span class="lineno">  116</span> </div>
<div class="line"><span class="lineno">  117</span>  <span class="comment">// Configure accumulate biases kernel for non quantized asymmetric types</span></div>
<div class="line"><span class="lineno">  118</span>  <span class="keywordflow">if</span> (biases != <span class="keyword">nullptr</span>)</div>
<div class="line"><span class="lineno">  119</span>  {</div>
<div class="line"><span class="lineno">  120</span>    ARM_COMPUTE_ERROR_ON_MISMATCHING_DATA_TYPES(input, biases);</div>
<div class="line"><span class="lineno">  121</span> </div>
<div class="line"><span class="lineno">  122</span>    _accumulate_biases = <span class="keyword">true</span>;</div>
<div class="line"><span class="lineno">  123</span> </div>
<div class="line"><span class="lineno">  124</span>    <span class="comment">// Configure accumulate biases kernel</span></div>
<div class="line"><span class="lineno">  125</span>    _accumulate_biases_kernel.set_target(CLScheduler::get().<a class="code hl_variable" href="namespacennc_1_1cli.html#afac06e8b64babc482ad2c033ba9b1e0c">target</a>());</div>
<div class="line"><span class="lineno">  126</span>    _accumulate_biases_kernel.<a class="code hl_function" href="classarm__compute_1_1_c_l_g_e_m_m_matrix_accumulate_biases_kernel.html#adde11c2465462328ba2a5636037ca10c">configure</a>(output, biases);</div>
<div class="line"><span class="lineno">  127</span>  }</div>
<div class="line"><span class="lineno">  128</span> </div>
<div class="line"><span class="lineno">  129</span>  <span class="keyword">const</span> ICLTensor *weights_to_use = weights;</div>
<div class="line"><span class="lineno">  130</span> </div>
<div class="line"><span class="lineno">  131</span>  <span class="comment">// With the Fully Connected layer we can have 4 different cases:</span></div>
<div class="line"><span class="lineno">  132</span>  <span class="comment">//  1) Convolution layer -&gt; Fully Connected layer without batches</span></div>
<div class="line"><span class="lineno">  133</span>  <span class="comment">//  2) Fully Connected layer -&gt; Fully Connected layer without batches</span></div>
<div class="line"><span class="lineno">  134</span>  <span class="comment">//  3) Convolution layer -&gt; Fully Connected layer with batches</span></div>
<div class="line"><span class="lineno">  135</span>  <span class="comment">//  4) Fully Connected layer -&gt; Fully Connected layer with batches</span></div>
<div class="line"><span class="lineno">  136</span> </div>
<div class="line"><span class="lineno">  137</span>  <span class="comment">// Check if we have a fully connected layer with batches</span></div>
<div class="line"><span class="lineno">  138</span>  <span class="keyword">const</span> <span class="keywordtype">bool</span> is_batched_fc_layer = <a class="code hl_variable" href="namespacegen__h5__explicit__inputs.html#acd1aa9ba45d45c6b619b723e6e34c576">output</a>-&gt;info()-&gt;dimension(1) &gt; 1;</div>
<div class="line"><span class="lineno">  139</span>  <span class="keywordtype">bool</span> is_fc_after_conv = <span class="keyword">false</span>;</div>
<div class="line"><span class="lineno">  140</span>  <span class="keywordflow">if</span> (is_batched_fc_layer)</div>
<div class="line"><span class="lineno">  141</span>  {</div>
<div class="line"><span class="lineno">  142</span>    is_fc_after_conv =</div>
<div class="line"><span class="lineno">  143</span>      (TensorShape::num_max_dimensions &gt;= 4) &amp;&amp;</div>
<div class="line"><span class="lineno">  144</span>      (std::equal(<a class="code hl_variable" href="namespacegen__h5__explicit__inputs.html#a48dd077479f23bb4552c2d7d6a7a4d37">input</a>-&gt;info()-&gt;tensor_shape().cbegin() + 3, <a class="code hl_variable" href="namespacegen__h5__explicit__inputs.html#a48dd077479f23bb4552c2d7d6a7a4d37">input</a>-&gt;info()-&gt;tensor_shape().cend(),</div>
<div class="line"><span class="lineno">  145</span>                  <a class="code hl_variable" href="namespacegen__h5__explicit__inputs.html#acd1aa9ba45d45c6b619b723e6e34c576">output</a>-&gt;info()-&gt;tensor_shape().cbegin() + 1));</div>
<div class="line"><span class="lineno">  146</span>  }</div>
<div class="line"><span class="lineno">  147</span>  <span class="keywordflow">else</span></div>
<div class="line"><span class="lineno">  148</span>  {</div>
<div class="line"><span class="lineno">  149</span>    is_fc_after_conv = <a class="code hl_variable" href="namespacegen__h5__explicit__inputs.html#a48dd077479f23bb4552c2d7d6a7a4d37">input</a>-&gt;info()-&gt;num_dimensions() &gt; 1 &amp;&amp; <a class="code hl_variable" href="namespacegen__h5__explicit__inputs.html#a48dd077479f23bb4552c2d7d6a7a4d37">input</a>-&gt;info()-&gt;dimension(1) &gt; 1;</div>
<div class="line"><span class="lineno">  150</span>  }</div>
<div class="line"><span class="lineno">  151</span>  ARM_COMPUTE_ERROR_ON_MSG(is_fc_after_conv,</div>
<div class="line"><span class="lineno">  152</span>                           <span class="stringliteral">&quot;CLFullyConnectedHybridLayer does not support after conv&quot;</span>);</div>
<div class="line"><span class="lineno">  153</span>  ARM_COMPUTE_UNUSED(is_fc_after_conv);</div>
<div class="line"><span class="lineno">  154</span> </div>
<div class="line"><span class="lineno">  155</span>  <span class="comment">// Reshape weights if needed</span></div>
<div class="line"><span class="lineno">  156</span>  <span class="keywordflow">if</span> (!_are_weights_reshaped)</div>
<div class="line"><span class="lineno">  157</span>  {</div>
<div class="line"><span class="lineno">  158</span>    <span class="comment">// Reshape the weights</span></div>
<div class="line"><span class="lineno">  159</span>    _reshape_weights_output.allocator()-&gt;init(</div>
<div class="line"><span class="lineno">  160</span>      weights-&gt;info()-&gt;clone()-&gt;set_is_resizable(<span class="keyword">true</span>).reset_padding().set_tensor_shape(</div>
<div class="line"><span class="lineno">  161</span>        compute_transposed_shape(*weights-&gt;info())));</div>
<div class="line"><span class="lineno">  162</span>    _reshape_weights_kernel.<a class="code hl_function" href="classarm__compute_1_1_c_l_fully_connected_hybrid_layer_reshape_weights.html#a59be3af85f43f40987f83fbf6b375d86">configure</a>(weights_to_use, &amp;_reshape_weights_output);</div>
<div class="line"><span class="lineno">  163</span>    weights_to_use = &amp;_reshape_weights_output;</div>
<div class="line"><span class="lineno">  164</span>  }</div>
<div class="line"><span class="lineno">  165</span> </div>
<div class="line"><span class="lineno">  166</span>  <span class="comment">// Extract scale factor</span></div>
<div class="line"><span class="lineno">  167</span>  _scale_factor.allocator()-&gt;init(</div>
<div class="line"><span class="lineno">  168</span>    TensorInfo(TensorShape{<a class="code hl_variable" href="namespacegen__h5__explicit__inputs.html#acd1aa9ba45d45c6b619b723e6e34c576">output</a>-&gt;info()-&gt;dimension(1)}, 1, <a class="code hl_variable" href="namespacegen__h5__explicit__inputs.html#a48dd077479f23bb4552c2d7d6a7a4d37">input</a>-&gt;info()-&gt;data_type()));</div>
<div class="line"><span class="lineno">  169</span>  _memory_group.manage(&amp;_scale_factor);</div>
<div class="line"><span class="lineno">  170</span>  _scale_factor_kernel.<a class="code hl_function" href="classarm__compute_1_1_c_l_scale_factor_symm8_kernel.html#a0327767a7a366073114d8694888ec014">configure</a>(input, &amp;_scale_factor);</div>
<div class="line"><span class="lineno">  171</span> </div>
<div class="line"><span class="lineno">  172</span>  <span class="comment">// Quantize input</span></div>
<div class="line"><span class="lineno">  173</span>  _quantized_input.allocator()-&gt;init(</div>
<div class="line"><span class="lineno">  174</span>    <a class="code hl_variable" href="namespacegen__h5__explicit__inputs.html#a48dd077479f23bb4552c2d7d6a7a4d37">input</a>-&gt;info()-&gt;clone()-&gt;set_is_resizable(<span class="keyword">true</span>).reset_padding().set_data_type(</div>
<div class="line"><span class="lineno">  175</span>      DataType::QASYMM8_SIGNED));</div>
<div class="line"><span class="lineno">  176</span>  _memory_group.manage(&amp;_quantized_input);</div>
<div class="line"><span class="lineno">  177</span>  _quant_input_kernel.<a class="code hl_function" href="classarm__compute_1_1_c_l_quantization_symmetric_kernel.html#a8d71eb4ec6d657b5f829bd1074955e2c">configure</a>(input, &amp;_scale_factor, &amp;_quantized_input);</div>
<div class="line"><span class="lineno">  178</span> </div>
<div class="line"><span class="lineno">  179</span>  <span class="comment">// GEMMLowp</span></div>
<div class="line"><span class="lineno">  180</span>  _gemmlowp_output.allocator()-&gt;init(</div>
<div class="line"><span class="lineno">  181</span>    <a class="code hl_variable" href="namespacegen__h5__explicit__inputs.html#acd1aa9ba45d45c6b619b723e6e34c576">output</a>-&gt;info()-&gt;clone()-&gt;set_is_resizable(<span class="keyword">true</span>).reset_padding().set_data_type(DataType::S32));</div>
<div class="line"><span class="lineno">  182</span>  _memory_group.manage(&amp;_gemmlowp_output);</div>
<div class="line"><span class="lineno">  183</span>  configure_mm(&amp;_quantized_input, weights_to_use, &amp;_gemmlowp_output,</div>
<div class="line"><span class="lineno">  184</span>               fc_info.retain_internal_weights);</div>
<div class="line"><span class="lineno">  185</span>  _quantized_input.allocator()-&gt;allocate();</div>
<div class="line"><span class="lineno">  186</span> </div>
<div class="line"><span class="lineno">  187</span>  <span class="comment">// Multiply scale</span></div>
<div class="line"><span class="lineno">  188</span>  _multiply_scale_kernel.<a class="code hl_function" href="classarm__compute_1_1_c_l_multiply_scale_factor_kernel.html#af734cb2ebf7cdd569f1518b4c40c285f">configure</a>(&amp;_gemmlowp_output, &amp;_scale_factor, output,</div>
<div class="line"><span class="lineno">  189</span>                                   weights-&gt;info()-&gt;quantization_info().uniform().scale);</div>
<div class="line"><span class="lineno">  190</span>  _gemmlowp_output.allocator()-&gt;allocate();</div>
<div class="line"><span class="lineno">  191</span>  _scale_factor.allocator()-&gt;allocate();</div>
<div class="line"><span class="lineno">  192</span> </div>
<div class="line"><span class="lineno">  193</span>  _are_weights_reshaped = _are_weights_reshaped || fc_info.retain_internal_weights;</div>
<div class="line"><span class="lineno">  194</span>}</div>
<div class="ttc" id="aclassarm__compute_1_1_c_l_fully_connected_hybrid_layer_html_a853272d8cb83134d303afb7e4ac9f94f"><div class="ttname"><a href="classarm__compute_1_1_c_l_fully_connected_hybrid_layer.html#a853272d8cb83134d303afb7e4ac9f94f">arm_compute::CLFullyConnectedHybridLayer::validate</a></div><div class="ttdeci">static Status validate(const ITensorInfo *input, const ITensorInfo *weights, const ITensorInfo *biases, const ITensorInfo *output, FullyConnectedLayerInfo fc_info=FullyConnectedLayerInfo())</div><div class="ttdef"><b>Definition:</b> <a href="_c_l_fully_connected_hybrid_layer_8cpp_source.html#l00196">CLFullyConnectedHybridLayer.cpp:196</a></div></div>
<div class="ttc" id="aclassarm__compute_1_1_c_l_fully_connected_hybrid_layer_reshape_weights_html_a59be3af85f43f40987f83fbf6b375d86"><div class="ttname"><a href="classarm__compute_1_1_c_l_fully_connected_hybrid_layer_reshape_weights.html#a59be3af85f43f40987f83fbf6b375d86">arm_compute::CLFullyConnectedHybridLayerReshapeWeights::configure</a></div><div class="ttdeci">void configure(const ICLTensor *input, ICLTensor *output)</div><div class="ttdef"><b>Definition:</b> <a href="_c_l_fully_connected_hybrid_layer_8cpp_source.html#l00068">CLFullyConnectedHybridLayer.cpp:68</a></div></div>
<div class="ttc" id="aclassarm__compute_1_1_c_l_g_e_m_m_matrix_accumulate_biases_kernel_html_adde11c2465462328ba2a5636037ca10c"><div class="ttname"><a href="classarm__compute_1_1_c_l_g_e_m_m_matrix_accumulate_biases_kernel.html#adde11c2465462328ba2a5636037ca10c">arm_compute::CLGEMMMatrixAccumulateBiasesKernel::configure</a></div><div class="ttdeci">void configure(ICLTensor *accum, const ICLTensor *biases)</div><div class="ttdef"><b>Definition:</b> <a href="_c_l_g_e_m_m_matrix_accumulate_biases_kernel_8cpp_source.html#l00102">CLGEMMMatrixAccumulateBiasesKernel.cpp:102</a></div></div>
<div class="ttc" id="aclassarm__compute_1_1_c_l_multiply_scale_factor_kernel_html_af734cb2ebf7cdd569f1518b4c40c285f"><div class="ttname"><a href="classarm__compute_1_1_c_l_multiply_scale_factor_kernel.html#af734cb2ebf7cdd569f1518b4c40c285f">arm_compute::CLMultiplyScaleFactorKernel::configure</a></div><div class="ttdeci">void configure(const ICLTensor *input, const ICLTensor *scale_factor, ICLTensor *output, float multiplier=1.f)</div><div class="ttdef"><b>Definition:</b> <a href="_c_l_multiply_scale_factor_kernel_8cpp_source.html#l00110">CLMultiplyScaleFactorKernel.cpp:110</a></div></div>
<div class="ttc" id="aclassarm__compute_1_1_c_l_quantization_symmetric_kernel_html_a8d71eb4ec6d657b5f829bd1074955e2c"><div class="ttname"><a href="classarm__compute_1_1_c_l_quantization_symmetric_kernel.html#a8d71eb4ec6d657b5f829bd1074955e2c">arm_compute::CLQuantizationSymmetricKernel::configure</a></div><div class="ttdeci">void configure(const ICLTensor *input, const ICLTensor *scale_factor, ICLTensor *output)</div><div class="ttdef"><b>Definition:</b> <a href="_c_l_quantization_symmetric_kernel_8cpp_source.html#l00112">CLQuantizationSymmetricKernel.cpp:112</a></div></div>
<div class="ttc" id="aclassarm__compute_1_1_c_l_scale_factor_symm8_kernel_html_a0327767a7a366073114d8694888ec014"><div class="ttname"><a href="classarm__compute_1_1_c_l_scale_factor_symm8_kernel.html#a0327767a7a366073114d8694888ec014">arm_compute::CLScaleFactorSymm8Kernel::configure</a></div><div class="ttdeci">void configure(const ICLTensor *input, ICLTensor *output)</div><div class="ttdef"><b>Definition:</b> <a href="_c_l_scale_factor_symm8_kernel_8cpp_source.html#l00109">CLScaleFactorSymm8Kernel.cpp:109</a></div></div>
<div class="ttc" id="anamespacegen__h5__explicit__inputs_html_a48dd077479f23bb4552c2d7d6a7a4d37"><div class="ttname"><a href="namespacegen__h5__explicit__inputs.html#a48dd077479f23bb4552c2d7d6a7a4d37">gen_h5_explicit_inputs.input</a></div><div class="ttdeci">input</div><div class="ttdef"><b>Definition:</b> <a href="gen__h5__explicit__inputs_8py_source.html#l00034">gen_h5_explicit_inputs.py:34</a></div></div>
<div class="ttc" id="anamespacegen__h5__explicit__inputs_html_acd1aa9ba45d45c6b619b723e6e34c576"><div class="ttname"><a href="namespacegen__h5__explicit__inputs.html#acd1aa9ba45d45c6b619b723e6e34c576">gen_h5_explicit_inputs.output</a></div><div class="ttdeci">output</div><div class="ttdef"><b>Definition:</b> <a href="gen__h5__explicit__inputs_8py_source.html#l00035">gen_h5_explicit_inputs.py:35</a></div></div>
<div class="ttc" id="anamespacennc_1_1cli_html_afac06e8b64babc482ad2c033ba9b1e0c"><div class="ttname"><a href="namespacennc_1_1cli.html#afac06e8b64babc482ad2c033ba9b1e0c">nnc::cli::target</a></div><div class="ttdeci">Option&lt; std::string &gt; target(optname(&quot;--target&quot;), overview(&quot;select target language to emit for given architecture.&quot; &quot;Valid values are '&quot; NNC_TARGET_ARM_CPP &quot;', '&quot; NNC_TARGET_X86_CPP &quot;', '&quot; NNC_TARGET_ARM_GPU_CPP &quot;', '&quot; NNC_TARGET_INTERPRETER &quot;'&quot;), std::string(), optional(false), optvalues(NNC_TARGET_ARM_CPP &quot;,&quot; NNC_TARGET_X86_CPP &quot;,&quot; NNC_TARGET_ARM_GPU_CPP &quot;,&quot; NNC_TARGET_INTERPRETER), nullptr, separators(&quot;=&quot;))</div><div class="ttdef"><b>Definition:</b> <a href="_options_8h_source.html#l00047">Options.h:47</a></div></div>
</div><!-- fragment -->
<p class="reference">References <a class="el" href="_c_l_quantization_symmetric_kernel_8cpp_source.html#l00112">arm_compute::CLQuantizationSymmetricKernel::configure()</a>, <a class="el" href="_c_l_multiply_scale_factor_kernel_8cpp_source.html#l00110">arm_compute::CLMultiplyScaleFactorKernel::configure()</a>, <a class="el" href="_c_l_scale_factor_symm8_kernel_8cpp_source.html#l00109">arm_compute::CLScaleFactorSymm8Kernel::configure()</a>, <a class="el" href="_c_l_fully_connected_hybrid_layer_8cpp_source.html#l00068">arm_compute::CLFullyConnectedHybridLayerReshapeWeights::configure()</a>, <a class="el" href="_c_l_g_e_m_m_matrix_accumulate_biases_kernel_8cpp_source.html#l00102">arm_compute::CLGEMMMatrixAccumulateBiasesKernel::configure()</a>, and <a class="el" href="_c_l_fully_connected_hybrid_layer_8cpp_source.html#l00196">validate()</a>.</p>

</div>
</div>
<a id="afe842e9374ac19e2fa9b7eae8f662dfe" name="afe842e9374ac19e2fa9b7eae8f662dfe"></a>
<h2 class="memtitle"><span class="permalink"><a href="#afe842e9374ac19e2fa9b7eae8f662dfe">&#9670;&#160;</a></span>operator=() <span class="overload">[1/2]</span></h2>

<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname"><a class="el" href="classarm__compute_1_1_c_l_fully_connected_hybrid_layer.html">CLFullyConnectedHybridLayer</a> &amp; arm_compute::CLFullyConnectedHybridLayer::operator= </td>
          <td>(</td>
          <td class="paramtype"><a class="el" href="classarm__compute_1_1_c_l_fully_connected_hybrid_layer.html">CLFullyConnectedHybridLayer</a> &amp;&amp;&#160;</td>
          <td class="paramname"></td><td>)</td>
          <td></td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">default</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">
<p >Default move assignment operator </p>

<p class="reference">References <a class="el" href="_c_l_fully_connected_hybrid_layer_8cpp_source.html#l00196">validate()</a>.</p>

</div>
</div>
<a id="af9e2f8f242e2c1f2e2c2897efafa98f6" name="af9e2f8f242e2c1f2e2c2897efafa98f6"></a>
<h2 class="memtitle"><span class="permalink"><a href="#af9e2f8f242e2c1f2e2c2897efafa98f6">&#9670;&#160;</a></span>operator=() <span class="overload">[2/2]</span></h2>

<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname"><a class="el" href="classarm__compute_1_1_c_l_fully_connected_hybrid_layer.html">CLFullyConnectedHybridLayer</a> &amp; arm_compute::CLFullyConnectedHybridLayer::operator= </td>
          <td>(</td>
          <td class="paramtype">const <a class="el" href="classarm__compute_1_1_c_l_fully_connected_hybrid_layer.html">CLFullyConnectedHybridLayer</a> &amp;&#160;</td>
          <td class="paramname"></td><td>)</td>
          <td></td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">delete</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">
<p >Prevent instances of this class from being copied (As this class contains pointers) </p>

</div>
</div>
<a id="a9a6ac7ee73b176969787dc392b86c3e2" name="a9a6ac7ee73b176969787dc392b86c3e2"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a9a6ac7ee73b176969787dc392b86c3e2">&#9670;&#160;</a></span>prepare()</h2>

<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">void CLFullyConnectedHybridLayer::prepare </td>
          <td>(</td>
          <td class="paramname"></td><td>)</td>
          <td></td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">override</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p class="definition">Definition at line <a class="el" href="_c_l_fully_connected_hybrid_layer_8cpp_source.html#l00304">304</a> of file <a class="el" href="_c_l_fully_connected_hybrid_layer_8cpp_source.html">CLFullyConnectedHybridLayer.cpp</a>.</p>
<div class="fragment"><div class="line"><span class="lineno">  305</span>{</div>
<div class="line"><span class="lineno">  306</span>  <span class="keywordflow">if</span> (!_is_prepared)</div>
<div class="line"><span class="lineno">  307</span>  {</div>
<div class="line"><span class="lineno">  308</span>    ARM_COMPUTE_ERROR_ON(!_original_weights-&gt;is_used());</div>
<div class="line"><span class="lineno">  309</span> </div>
<div class="line"><span class="lineno">  310</span>    <span class="keyword">auto</span> release_unused = [](CLTensor *<a class="code hl_variable" href="namespacejpeg2hdf5.html#ac7eea56215106b0908497982f6eed453">w</a>) {</div>
<div class="line"><span class="lineno">  311</span>      <span class="keywordflow">if</span> (!<a class="code hl_variable" href="namespacejpeg2hdf5.html#ac7eea56215106b0908497982f6eed453">w</a>-&gt;is_used())</div>
<div class="line"><span class="lineno">  312</span>      {</div>
<div class="line"><span class="lineno">  313</span>        CLScheduler::get().queue().finish();</div>
<div class="line"><span class="lineno">  314</span>        <a class="code hl_variable" href="namespacejpeg2hdf5.html#ac7eea56215106b0908497982f6eed453">w</a>-&gt;allocator()-&gt;free();</div>
<div class="line"><span class="lineno">  315</span>      }</div>
<div class="line"><span class="lineno">  316</span>    };</div>
<div class="line"><span class="lineno">  317</span> </div>
<div class="line"><span class="lineno">  318</span>    <span class="comment">// Reshape of the weights if needed (happens only once)</span></div>
<div class="line"><span class="lineno">  319</span>    <span class="keywordflow">if</span> (!_are_weights_reshaped)</div>
<div class="line"><span class="lineno">  320</span>    {</div>
<div class="line"><span class="lineno">  321</span>      <span class="comment">// Run reshape weights kernel and mark weights as unused</span></div>
<div class="line"><span class="lineno">  322</span>      _reshape_weights_output.allocator()-&gt;allocate();</div>
<div class="line"><span class="lineno">  323</span>      _reshape_weights_kernel.run();</div>
<div class="line"><span class="lineno">  324</span> </div>
<div class="line"><span class="lineno">  325</span>      _are_weights_reshaped = <span class="keyword">true</span>;</div>
<div class="line"><span class="lineno">  326</span>      <span class="comment">// We can not release _original_weights because it can be used in other nodes</span></div>
<div class="line"><span class="lineno">  327</span>    }</div>
<div class="line"><span class="lineno">  328</span> </div>
<div class="line"><span class="lineno">  329</span>    <span class="comment">// Prepare GEMM prepare and release unused weights</span></div>
<div class="line"><span class="lineno">  330</span>    _mm_gemmlowp.prepare();</div>
<div class="line"><span class="lineno">  331</span> </div>
<div class="line"><span class="lineno">  332</span>    <span class="comment">// Release reshaped weights if unused</span></div>
<div class="line"><span class="lineno">  333</span>    release_unused(&amp;_reshape_weights_output);</div>
<div class="line"><span class="lineno">  334</span> </div>
<div class="line"><span class="lineno">  335</span>    _is_prepared = <span class="keyword">true</span>;</div>
<div class="line"><span class="lineno">  336</span>  }</div>
<div class="line"><span class="lineno">  337</span>}</div>
<div class="ttc" id="anamespacejpeg2hdf5_html_ac7eea56215106b0908497982f6eed453"><div class="ttname"><a href="namespacejpeg2hdf5.html#ac7eea56215106b0908497982f6eed453">jpeg2hdf5.w</a></div><div class="ttdeci">w</div><div class="ttdef"><b>Definition:</b> <a href="jpeg2hdf5_8py_source.html#l00102">jpeg2hdf5.py:102</a></div></div>
</div><!-- fragment -->
<p class="reference">Referenced by <a class="el" href="_c_l_fully_connected_hybrid_layer_8cpp_source.html#l00279">run()</a>.</p>

</div>
</div>
<a id="a2188556b885f502ddd78db7e1b56fa17" name="a2188556b885f502ddd78db7e1b56fa17"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a2188556b885f502ddd78db7e1b56fa17">&#9670;&#160;</a></span>run()</h2>

<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">void CLFullyConnectedHybridLayer::run </td>
          <td>(</td>
          <td class="paramname"></td><td>)</td>
          <td></td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">override</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p class="definition">Definition at line <a class="el" href="_c_l_fully_connected_hybrid_layer_8cpp_source.html#l00279">279</a> of file <a class="el" href="_c_l_fully_connected_hybrid_layer_8cpp_source.html">CLFullyConnectedHybridLayer.cpp</a>.</p>
<div class="fragment"><div class="line"><span class="lineno">  280</span>{</div>
<div class="line"><span class="lineno">  281</span>  <a class="code hl_function" href="classarm__compute_1_1_c_l_fully_connected_hybrid_layer.html#a9a6ac7ee73b176969787dc392b86c3e2">prepare</a>();</div>
<div class="line"><span class="lineno">  282</span> </div>
<div class="line"><span class="lineno">  283</span>  MemoryGroupResourceScope scope_mg(_memory_group);</div>
<div class="line"><span class="lineno">  284</span> </div>
<div class="line"><span class="lineno">  285</span>  <span class="comment">// Extract scale_factor</span></div>
<div class="line"><span class="lineno">  286</span>  CLScheduler::get().enqueue(_scale_factor_kernel);</div>
<div class="line"><span class="lineno">  287</span> </div>
<div class="line"><span class="lineno">  288</span>  <span class="comment">// Quantize input</span></div>
<div class="line"><span class="lineno">  289</span>  CLScheduler::get().enqueue(_quant_input_kernel);</div>
<div class="line"><span class="lineno">  290</span> </div>
<div class="line"><span class="lineno">  291</span>  <span class="comment">// Run matrix multiply</span></div>
<div class="line"><span class="lineno">  292</span>  _mm_gemmlowp.run();</div>
<div class="line"><span class="lineno">  293</span> </div>
<div class="line"><span class="lineno">  294</span>  <span class="comment">// Multiply scale factor</span></div>
<div class="line"><span class="lineno">  295</span>  CLScheduler::get().enqueue(_multiply_scale_kernel);</div>
<div class="line"><span class="lineno">  296</span> </div>
<div class="line"><span class="lineno">  297</span>  <span class="comment">// Accumulate biases if provided</span></div>
<div class="line"><span class="lineno">  298</span>  <span class="keywordflow">if</span> (_accumulate_biases)</div>
<div class="line"><span class="lineno">  299</span>  {</div>
<div class="line"><span class="lineno">  300</span>    CLScheduler::get().enqueue(_accumulate_biases_kernel);</div>
<div class="line"><span class="lineno">  301</span>  }</div>
<div class="line"><span class="lineno">  302</span>}</div>
<div class="ttc" id="aclassarm__compute_1_1_c_l_fully_connected_hybrid_layer_html_a9a6ac7ee73b176969787dc392b86c3e2"><div class="ttname"><a href="classarm__compute_1_1_c_l_fully_connected_hybrid_layer.html#a9a6ac7ee73b176969787dc392b86c3e2">arm_compute::CLFullyConnectedHybridLayer::prepare</a></div><div class="ttdeci">void prepare() override</div><div class="ttdef"><b>Definition:</b> <a href="_c_l_fully_connected_hybrid_layer_8cpp_source.html#l00304">CLFullyConnectedHybridLayer.cpp:304</a></div></div>
</div><!-- fragment -->
<p class="reference">References <a class="el" href="_c_l_fully_connected_hybrid_layer_8cpp_source.html#l00304">prepare()</a>.</p>

</div>
</div>
<a id="a853272d8cb83134d303afb7e4ac9f94f" name="a853272d8cb83134d303afb7e4ac9f94f"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a853272d8cb83134d303afb7e4ac9f94f">&#9670;&#160;</a></span>validate()</h2>

<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">Status CLFullyConnectedHybridLayer::validate </td>
          <td>(</td>
          <td class="paramtype">const ITensorInfo *&#160;</td>
          <td class="paramname"><em>input</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const ITensorInfo *&#160;</td>
          <td class="paramname"><em>weights</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const ITensorInfo *&#160;</td>
          <td class="paramname"><em>biases</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const ITensorInfo *&#160;</td>
          <td class="paramname"><em>output</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">FullyConnectedLayerInfo&#160;</td>
          <td class="paramname"><em>fc_info</em> = <code>FullyConnectedLayerInfo()</code>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">static</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">
<p >Static function to check if given info will lead to a valid configuration of <a class="el" href="classarm__compute_1_1_c_l_fully_connected_hybrid_layer.html">CLFullyConnectedHybridLayer</a></p>
<dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramdir">[in]</td><td class="paramname">input</td><td>Source tensor info. Data type supported: F16/F32. </td></tr>
    <tr><td class="paramdir">[in]</td><td class="paramname">weights</td><td>Weights tensor info. The weights must be 2 dimensional. If this function is called after a Convolution Layer, the (transposed) weights will have as many rows as the product of the first 3 input's dimensions. If it is called after another FullyConnected Layer, the (transposed) weights will have as many rows as the input's first dimension. Data type supported: S8. </td></tr>
    <tr><td class="paramdir">[in]</td><td class="paramname">biases</td><td>Bias tensor info. Can be nullptr. Data type supported:Same as <code>input</code>. </td></tr>
    <tr><td class="paramdir">[out]</td><td class="paramname">output</td><td>Destination tensor info. Its shape should be equal to the output of a matrix multiplication between:<ul>
<li>The output of im2col on the input and the (transposed) 2D weights, if the function is called after a Convolution Layer</li>
<li>The input tensor and the (transposed) 2D weights, if the function is called after another FullyConnected Layer. Data type supported: Same as <code>input</code>. </li>
</ul>
</td></tr>
    <tr><td class="paramdir">[in]</td><td class="paramname">fc_info</td><td>(Optional) Fully connected layer additional info</td></tr>
  </table>
  </dd>
</dl>
<dl class="section return"><dt>Returns</dt><dd>a status </dd></dl>

<p class="definition">Definition at line <a class="el" href="_c_l_fully_connected_hybrid_layer_8cpp_source.html#l00196">196</a> of file <a class="el" href="_c_l_fully_connected_hybrid_layer_8cpp_source.html">CLFullyConnectedHybridLayer.cpp</a>.</p>
<div class="fragment"><div class="line"><span class="lineno">  199</span>{</div>
<div class="line"><span class="lineno">  200</span>  ARM_COMPUTE_RETURN_ERROR_ON_NULLPTR(input, weights, output);</div>
<div class="line"><span class="lineno">  201</span>  ARM_COMPUTE_RETURN_ERROR_ON_DATA_TYPE_CHANNEL_NOT_IN(input, 1, DataType::F16, DataType::F32);</div>
<div class="line"><span class="lineno">  202</span>  ARM_COMPUTE_RETURN_ERROR_ON_DATA_TYPE_CHANNEL_NOT_IN(weights, 1, DataType::QASYMM8_SIGNED);</div>
<div class="line"><span class="lineno">  203</span>  ARM_COMPUTE_RETURN_ERROR_ON_MISMATCHING_DATA_TYPES(input, output);</div>
<div class="line"><span class="lineno">  204</span>  ARM_COMPUTE_RETURN_ERROR_ON(weights-&gt;num_dimensions() &gt; 2);</div>
<div class="line"><span class="lineno">  205</span> </div>
<div class="line"><span class="lineno">  206</span>  <span class="keywordtype">bool</span> weights_reshaped = fc_info.transpose_weights ? fc_info.are_weights_reshaped : <span class="keyword">true</span>;</div>
<div class="line"><span class="lineno">  207</span>  <span class="keywordtype">bool</span> is_fc_after_conv = <span class="keyword">true</span>;</div>
<div class="line"><span class="lineno">  208</span>  <span class="keyword">const</span> GPUTarget gpu_target = CLScheduler::get().target();</div>
<div class="line"><span class="lineno">  209</span> </div>
<div class="line"><span class="lineno">  210</span>  <span class="keyword">const</span> ITensorInfo &amp;reshaped_weights =</div>
<div class="line"><span class="lineno">  211</span>    TensorInfo(weights-&gt;clone()-&gt;set_is_resizable(<span class="keyword">true</span>).reset_padding().set_tensor_shape(</div>
<div class="line"><span class="lineno">  212</span>      compute_transposed_shape(*weights)));</div>
<div class="line"><span class="lineno">  213</span> </div>
<div class="line"><span class="lineno">  214</span>  <span class="comment">// Configure accumulate biases kernel for non quantized asymmetric types</span></div>
<div class="line"><span class="lineno">  215</span>  <span class="keywordflow">if</span> (biases != <span class="keyword">nullptr</span>)</div>
<div class="line"><span class="lineno">  216</span>  {</div>
<div class="line"><span class="lineno">  217</span>    ARM_COMPUTE_RETURN_ERROR_ON_MISMATCHING_DATA_TYPES(input, biases);</div>
<div class="line"><span class="lineno">  218</span>    ARM_COMPUTE_RETURN_ON_ERROR(</div>
<div class="line"><span class="lineno">  219</span>      <a class="code hl_function" href="classarm__compute_1_1_c_l_g_e_m_m_matrix_accumulate_biases_kernel.html#a78fb5d08e14df2c0b578fd12c7aaedc3">CLGEMMMatrixAccumulateBiasesKernel::validate</a>(output, biases, gpu_target));</div>
<div class="line"><span class="lineno">  220</span>  }</div>
<div class="line"><span class="lineno">  221</span> </div>
<div class="line"><span class="lineno">  222</span>  <span class="comment">// With the Fully Connected layer we can have 4 different cases:</span></div>
<div class="line"><span class="lineno">  223</span>  <span class="comment">//  1) Convolution layer -&gt; Fully Connected layer without batches</span></div>
<div class="line"><span class="lineno">  224</span>  <span class="comment">//  2) Fully Connected layer -&gt; Fully Connected layer without batches</span></div>
<div class="line"><span class="lineno">  225</span>  <span class="comment">//  3) Convolution layer -&gt; Fully Connected layer with batches</span></div>
<div class="line"><span class="lineno">  226</span>  <span class="comment">//  4) Fully Connected layer -&gt; Fully Connected layer with batches</span></div>
<div class="line"><span class="lineno">  227</span> </div>
<div class="line"><span class="lineno">  228</span>  <span class="keyword">const</span> ITensorInfo *weights_to_use = weights;</div>
<div class="line"><span class="lineno">  229</span> </div>
<div class="line"><span class="lineno">  230</span>  <span class="comment">// Check if we have a fully connected layer with batches</span></div>
<div class="line"><span class="lineno">  231</span>  <span class="keyword">const</span> <span class="keywordtype">bool</span> is_batched_fc_layer = <a class="code hl_variable" href="namespacegen__h5__explicit__inputs.html#acd1aa9ba45d45c6b619b723e6e34c576">output</a>-&gt;dimension(1) &gt; 1;</div>
<div class="line"><span class="lineno">  232</span>  <span class="keywordflow">if</span> (is_batched_fc_layer)</div>
<div class="line"><span class="lineno">  233</span>  {</div>
<div class="line"><span class="lineno">  234</span>    is_fc_after_conv = (TensorShape::num_max_dimensions &gt;= 4) &amp;&amp;</div>
<div class="line"><span class="lineno">  235</span>                       (std::equal(<a class="code hl_variable" href="namespacegen__h5__explicit__inputs.html#a48dd077479f23bb4552c2d7d6a7a4d37">input</a>-&gt;tensor_shape().cbegin() + 3, <a class="code hl_variable" href="namespacegen__h5__explicit__inputs.html#a48dd077479f23bb4552c2d7d6a7a4d37">input</a>-&gt;tensor_shape().cend(),</div>
<div class="line"><span class="lineno">  236</span>                                   <a class="code hl_variable" href="namespacegen__h5__explicit__inputs.html#acd1aa9ba45d45c6b619b723e6e34c576">output</a>-&gt;tensor_shape().cbegin() + 1));</div>
<div class="line"><span class="lineno">  237</span>  }</div>
<div class="line"><span class="lineno">  238</span>  <span class="keywordflow">else</span></div>
<div class="line"><span class="lineno">  239</span>  {</div>
<div class="line"><span class="lineno">  240</span>    is_fc_after_conv = <a class="code hl_variable" href="namespacegen__h5__explicit__inputs.html#a48dd077479f23bb4552c2d7d6a7a4d37">input</a>-&gt;num_dimensions() &gt; 1 &amp;&amp; <a class="code hl_variable" href="namespacegen__h5__explicit__inputs.html#a48dd077479f23bb4552c2d7d6a7a4d37">input</a>-&gt;dimension(1) &gt; 1;</div>
<div class="line"><span class="lineno">  241</span>  }</div>
<div class="line"><span class="lineno">  242</span>  ARM_COMPUTE_RETURN_ERROR_ON_MSG(is_fc_after_conv,</div>
<div class="line"><span class="lineno">  243</span>                                  <span class="stringliteral">&quot;CLFullyConnectedHybridLayer does not support after conv&quot;</span>);</div>
<div class="line"><span class="lineno">  244</span> </div>
<div class="line"><span class="lineno">  245</span>  <span class="keywordflow">if</span> (!weights_reshaped)</div>
<div class="line"><span class="lineno">  246</span>  {</div>
<div class="line"><span class="lineno">  247</span>    <span class="comment">// Validate reshape weights kernel</span></div>
<div class="line"><span class="lineno">  248</span>    ARM_COMPUTE_RETURN_ON_ERROR(</div>
<div class="line"><span class="lineno">  249</span>      <a class="code hl_function" href="classarm__compute_1_1_c_l_fully_connected_hybrid_layer_reshape_weights.html#afffe972aed7b92958a094a37af3f93a4">CLFullyConnectedHybridLayerReshapeWeights::validate</a>(weights_to_use, &amp;reshaped_weights));</div>
<div class="line"><span class="lineno">  250</span>    weights_to_use = &amp;reshaped_weights;</div>
<div class="line"><span class="lineno">  251</span>  }</div>
<div class="line"><span class="lineno">  252</span> </div>
<div class="line"><span class="lineno">  253</span>  <span class="comment">// Validate Scale factor kernel</span></div>
<div class="line"><span class="lineno">  254</span>  <span class="keyword">const</span> ITensorInfo &amp;scale_factor =</div>
<div class="line"><span class="lineno">  255</span>    TensorInfo(TensorShape{<a class="code hl_variable" href="namespacegen__h5__explicit__inputs.html#acd1aa9ba45d45c6b619b723e6e34c576">output</a>-&gt;dimension(1)}, 1, <a class="code hl_variable" href="namespacegen__h5__explicit__inputs.html#a48dd077479f23bb4552c2d7d6a7a4d37">input</a>-&gt;data_type());</div>
<div class="line"><span class="lineno">  256</span>  ARM_COMPUTE_RETURN_ON_ERROR(<a class="code hl_function" href="classarm__compute_1_1_c_l_scale_factor_symm8_kernel.html#a8ec0cdd5608fe71ada812dd43bb640ca">CLScaleFactorSymm8Kernel::validate</a>(input, &amp;scale_factor));</div>
<div class="line"><span class="lineno">  257</span> </div>
<div class="line"><span class="lineno">  258</span>  <span class="comment">// Validate quantization symm8 kernel</span></div>
<div class="line"><span class="lineno">  259</span>  <span class="keyword">const</span> ITensorInfo &amp;quantized_input = TensorInfo(</div>
<div class="line"><span class="lineno">  260</span>    <a class="code hl_variable" href="namespacegen__h5__explicit__inputs.html#a48dd077479f23bb4552c2d7d6a7a4d37">input</a>-&gt;clone()-&gt;set_is_resizable(<span class="keyword">true</span>).reset_padding().set_data_type(DataType::QASYMM8_SIGNED));</div>
<div class="line"><span class="lineno">  261</span>  ARM_COMPUTE_RETURN_ON_ERROR(</div>
<div class="line"><span class="lineno">  262</span>    <a class="code hl_function" href="classarm__compute_1_1_c_l_quantization_symmetric_kernel.html#ab9920f5892a8744b5e4759a8ad2ea288">CLQuantizationSymmetricKernel::validate</a>(input, &amp;scale_factor, &amp;quantized_input));</div>
<div class="line"><span class="lineno">  263</span> </div>
<div class="line"><span class="lineno">  264</span>  <span class="comment">// Fully Connected layer after a Fully Connected Layer without batches</span></div>
<div class="line"><span class="lineno">  265</span>  ARM_COMPUTE_RETURN_ERROR_ON(<a class="code hl_variable" href="namespacegen__h5__explicit__inputs.html#a48dd077479f23bb4552c2d7d6a7a4d37">input</a>-&gt;dimension(0) != weights_to_use-&gt;dimension(1));</div>
<div class="line"><span class="lineno">  266</span> </div>
<div class="line"><span class="lineno">  267</span>  <span class="comment">// Validate matrix multiply kernel</span></div>
<div class="line"><span class="lineno">  268</span>  <span class="keyword">const</span> ITensorInfo &amp;gemmlowp_output = TensorInfo(</div>
<div class="line"><span class="lineno">  269</span>    <a class="code hl_variable" href="namespacegen__h5__explicit__inputs.html#acd1aa9ba45d45c6b619b723e6e34c576">output</a>-&gt;clone()-&gt;set_is_resizable(<span class="keyword">true</span>).reset_padding().set_data_type(DataType::S32));</div>
<div class="line"><span class="lineno">  270</span>  ARM_COMPUTE_RETURN_ON_ERROR(validate_mm(quantized_input, *weights_to_use, gemmlowp_output));</div>
<div class="line"><span class="lineno">  271</span> </div>
<div class="line"><span class="lineno">  272</span>  <span class="comment">// Multiply scale</span></div>
<div class="line"><span class="lineno">  273</span>  ARM_COMPUTE_RETURN_ON_ERROR(</div>
<div class="line"><span class="lineno">  274</span>    <a class="code hl_function" href="classarm__compute_1_1_c_l_multiply_scale_factor_kernel.html#ab3e58bc8d847167f212c9527a7a293f0">CLMultiplyScaleFactorKernel::validate</a>(&amp;gemmlowp_output, &amp;scale_factor, output));</div>
<div class="line"><span class="lineno">  275</span> </div>
<div class="line"><span class="lineno">  276</span>  <span class="keywordflow">return</span> Status{};</div>
<div class="line"><span class="lineno">  277</span>}</div>
<div class="ttc" id="aclassarm__compute_1_1_c_l_fully_connected_hybrid_layer_reshape_weights_html_afffe972aed7b92958a094a37af3f93a4"><div class="ttname"><a href="classarm__compute_1_1_c_l_fully_connected_hybrid_layer_reshape_weights.html#afffe972aed7b92958a094a37af3f93a4">arm_compute::CLFullyConnectedHybridLayerReshapeWeights::validate</a></div><div class="ttdeci">static Status validate(const ITensorInfo *input, const ITensorInfo *output)</div><div class="ttdef"><b>Definition:</b> <a href="_c_l_fully_connected_hybrid_layer_8cpp_source.html#l00075">CLFullyConnectedHybridLayer.cpp:75</a></div></div>
<div class="ttc" id="aclassarm__compute_1_1_c_l_g_e_m_m_matrix_accumulate_biases_kernel_html_a78fb5d08e14df2c0b578fd12c7aaedc3"><div class="ttname"><a href="classarm__compute_1_1_c_l_g_e_m_m_matrix_accumulate_biases_kernel.html#a78fb5d08e14df2c0b578fd12c7aaedc3">arm_compute::CLGEMMMatrixAccumulateBiasesKernel::validate</a></div><div class="ttdeci">static Status validate(const ITensorInfo *accum, const ITensorInfo *biases, GPUTarget gpu_target)</div><div class="ttdef"><b>Definition:</b> <a href="_c_l_g_e_m_m_matrix_accumulate_biases_kernel_8cpp_source.html#l00138">CLGEMMMatrixAccumulateBiasesKernel.cpp:138</a></div></div>
<div class="ttc" id="aclassarm__compute_1_1_c_l_multiply_scale_factor_kernel_html_ab3e58bc8d847167f212c9527a7a293f0"><div class="ttname"><a href="classarm__compute_1_1_c_l_multiply_scale_factor_kernel.html#ab3e58bc8d847167f212c9527a7a293f0">arm_compute::CLMultiplyScaleFactorKernel::validate</a></div><div class="ttdeci">static Status validate(const ITensorInfo *input, const ITensorInfo *scale_factor, const ITensorInfo *output)</div><div class="ttdef"><b>Definition:</b> <a href="_c_l_multiply_scale_factor_kernel_8cpp_source.html#l00148">CLMultiplyScaleFactorKernel.cpp:148</a></div></div>
<div class="ttc" id="aclassarm__compute_1_1_c_l_quantization_symmetric_kernel_html_ab9920f5892a8744b5e4759a8ad2ea288"><div class="ttname"><a href="classarm__compute_1_1_c_l_quantization_symmetric_kernel.html#ab9920f5892a8744b5e4759a8ad2ea288">arm_compute::CLQuantizationSymmetricKernel::validate</a></div><div class="ttdeci">static Status validate(const ITensorInfo *input, const ITensorInfo *scale_factor, const ITensorInfo *output)</div><div class="ttdef"><b>Definition:</b> <a href="_c_l_quantization_symmetric_kernel_8cpp_source.html#l00146">CLQuantizationSymmetricKernel.cpp:146</a></div></div>
<div class="ttc" id="aclassarm__compute_1_1_c_l_scale_factor_symm8_kernel_html_a8ec0cdd5608fe71ada812dd43bb640ca"><div class="ttname"><a href="classarm__compute_1_1_c_l_scale_factor_symm8_kernel.html#a8ec0cdd5608fe71ada812dd43bb640ca">arm_compute::CLScaleFactorSymm8Kernel::validate</a></div><div class="ttdeci">static Status validate(const ITensorInfo *input, const ITensorInfo *output)</div><div class="ttdef"><b>Definition:</b> <a href="_c_l_scale_factor_symm8_kernel_8cpp_source.html#l00131">CLScaleFactorSymm8Kernel.cpp:131</a></div></div>
</div><!-- fragment -->
<p class="reference">References <a class="el" href="_c_l_g_e_m_m_matrix_accumulate_biases_kernel_8cpp_source.html#l00138">arm_compute::CLGEMMMatrixAccumulateBiasesKernel::validate()</a>, <a class="el" href="_c_l_scale_factor_symm8_kernel_8cpp_source.html#l00131">arm_compute::CLScaleFactorSymm8Kernel::validate()</a>, <a class="el" href="_c_l_fully_connected_hybrid_layer_8cpp_source.html#l00075">arm_compute::CLFullyConnectedHybridLayerReshapeWeights::validate()</a>, <a class="el" href="_c_l_multiply_scale_factor_kernel_8cpp_source.html#l00148">arm_compute::CLMultiplyScaleFactorKernel::validate()</a>, and <a class="el" href="_c_l_quantization_symmetric_kernel_8cpp_source.html#l00146">arm_compute::CLQuantizationSymmetricKernel::validate()</a>.</p>

<p class="reference">Referenced by <a class="el" href="_c_l_fully_connected_hybrid_layer_8cpp_source.html#l00101">configure()</a>, and <a class="el" href="classarm__compute_1_1_c_l_fully_connected_hybrid_layer.html#afe842e9374ac19e2fa9b7eae8f662dfe">operator=()</a>.</p>

</div>
</div>
<hr/>The documentation for this class was generated from the following files:<ul>
<li>compute/ARMComputeEx/arm_compute/runtime/CL/functions/<a class="el" href="_c_l_fully_connected_hybrid_layer_8h_source.html">CLFullyConnectedHybridLayer.h</a></li>
<li>compute/ARMComputeEx/src/runtime/CL/functions/<a class="el" href="_c_l_fully_connected_hybrid_layer_8cpp_source.html">CLFullyConnectedHybridLayer.cpp</a></li>
</ul>
</div><!-- contents -->
</div><!-- doc-content -->
<!-- start footer part -->
<div id="nav-path" class="navpath"><!-- id is needed for treeview function! -->
  <ul>
    <li class="navelem"><a class="el" href="namespacearm__compute.html">arm_compute</a></li><li class="navelem"><a class="el" href="classarm__compute_1_1_c_l_fully_connected_hybrid_layer.html">CLFullyConnectedHybridLayer</a></li>
    <li class="footer">Generated by <a href="https://www.doxygen.org/index.html"><img class="footer" src="doxygen.svg" width="104" height="31" alt="doxygen"/></a> 1.9.5 </li>
  </ul>
</div>
</body>
</html>
