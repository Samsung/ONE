<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "https://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en-US">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=11"/>
<meta name="generator" content="Doxygen 1.9.5"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<title>ONE - On-device Neural Engine: nnfw::cker::optimized Namespace Reference</title>
<link href="tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="dynsections.js"></script>
<link href="navtree.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="resize.js"></script>
<script type="text/javascript" src="navtreedata.js"></script>
<script type="text/javascript" src="navtree.js"></script>
<link href="search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="search/searchdata.js"></script>
<script type="text/javascript" src="search/search.js"></script>
<link href="doxygen.css" rel="stylesheet" type="text/css" />
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr id="projectrow">
  <td id="projectalign">
   <div id="projectname">ONE - On-device Neural Engine
   </div>
  </td>
 </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.9.5 -->
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:d3d9a9a6595521f9666a5e94cc830dab83b65699&amp;dn=expat.txt MIT */
var searchBox = new SearchBox("searchBox", "search/",'.html');
/* @license-end */
</script>
<script type="text/javascript" src="menudata.js"></script>
<script type="text/javascript" src="menu.js"></script>
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:d3d9a9a6595521f9666a5e94cc830dab83b65699&amp;dn=expat.txt MIT */
$(function() {
  initMenu('',true,false,'search.php','Search');
  $(document).ready(function() { init_search(); });
});
/* @license-end */
</script>
<div id="main-nav"></div>
</div><!-- top -->
<div id="side-nav" class="ui-resizable side-nav-resizable">
  <div id="nav-tree">
    <div id="nav-tree-contents">
      <div id="nav-sync" class="sync"></div>
    </div>
  </div>
  <div id="splitbar" style="-moz-user-select:none;" 
       class="ui-resizable-handle">
  </div>
</div>
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:d3d9a9a6595521f9666a5e94cc830dab83b65699&amp;dn=expat.txt MIT */
$(document).ready(function(){initNavTree('namespacennfw_1_1cker_1_1optimized.html',''); initResizable(); });
/* @license-end */
</script>
<div id="doc-content">
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<div id="MSearchResults">
<div class="SRPage">
<div id="SRIndex">
<div id="SRResults"></div>
<div class="SRStatus" id="Loading">Loading...</div>
<div class="SRStatus" id="Searching">Searching...</div>
<div class="SRStatus" id="NoMatches">No Matches</div>
</div>
</div>
</div>
</div>

<div class="header">
  <div class="summary">
<a href="#namespaces">Namespaces</a> &#124;
<a href="#nested-classes">Data Structures</a> &#124;
<a href="#typedef-members">Typedefs</a> &#124;
<a href="#func-members">Functions</a> &#124;
<a href="#var-members">Variables</a>  </div>
  <div class="headertitle"><div class="title">nnfw::cker::optimized Namespace Reference</div></div>
</div><!--header-->
<div class="contents">
<table class="memberdecls">
<tr class="heading"><td colspan="2"><h2 class="groupheader"><a id="namespaces" name="namespaces"></a>
Namespaces</h2></td></tr>
<tr class="memitem:namespacennfw_1_1cker_1_1optimized_1_1depthwise__conv"><td class="memItemLeft" align="right" valign="top">namespace &#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacennfw_1_1cker_1_1optimized_1_1depthwise__conv.html">depthwise_conv</a></td></tr>
<tr class="separator:"><td class="memSeparator" colspan="2">&#160;</td></tr>
</table><table class="memberdecls">
<tr class="heading"><td colspan="2"><h2 class="groupheader"><a id="nested-classes" name="nested-classes"></a>
Data Structures</h2></td></tr>
<tr class="memitem:"><td class="memItemLeft" align="right" valign="top">struct &#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="structnnfw_1_1cker_1_1optimized_1_1_binary_op_activation_float_max.html">BinaryOpActivationFloatMax</a></td></tr>
<tr class="separator:"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:"><td class="memItemLeft" align="right" valign="top">struct &#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="structnnfw_1_1cker_1_1optimized_1_1_binary_op_activation_float_min_max.html">BinaryOpActivationFloatMinMax</a></td></tr>
<tr class="separator:"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:"><td class="memItemLeft" align="right" valign="top">struct &#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="structnnfw_1_1cker_1_1optimized_1_1_binary_op_activation_float_none.html">BinaryOpActivationFloatNone</a></td></tr>
<tr class="separator:"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:"><td class="memItemLeft" align="right" valign="top">struct &#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="structnnfw_1_1cker_1_1optimized_1_1_binary_op_func_add_float.html">BinaryOpFuncAddFloat</a></td></tr>
<tr class="separator:"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:"><td class="memItemLeft" align="right" valign="top">struct &#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="structnnfw_1_1cker_1_1optimized_1_1_binary_op_func_div_float.html">BinaryOpFuncDivFloat</a></td></tr>
<tr class="separator:"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:"><td class="memItemLeft" align="right" valign="top">struct &#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="structnnfw_1_1cker_1_1optimized_1_1_binary_op_func_mul_float.html">BinaryOpFuncMulFloat</a></td></tr>
<tr class="separator:"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:"><td class="memItemLeft" align="right" valign="top">struct &#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="structnnfw_1_1cker_1_1optimized_1_1_binary_op_func_sub_float.html">BinaryOpFuncSubFloat</a></td></tr>
<tr class="separator:"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:"><td class="memItemLeft" align="right" valign="top">struct &#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="structnnfw_1_1cker_1_1optimized_1_1_binary_op_func_swap_args.html">BinaryOpFuncSwapArgs</a></td></tr>
<tr class="separator:"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:"><td class="memItemLeft" align="right" valign="top">struct &#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="structnnfw_1_1cker_1_1optimized_1_1_float_depthwise_conv_kernel.html">FloatDepthwiseConvKernel</a></td></tr>
<tr class="separator:"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:"><td class="memItemLeft" align="right" valign="top">struct &#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="structnnfw_1_1cker_1_1optimized_1_1_gemmlowp_output_pipeline.html">GemmlowpOutputPipeline</a></td></tr>
<tr class="separator:"><td class="memSeparator" colspan="2">&#160;</td></tr>
</table><table class="memberdecls">
<tr class="heading"><td colspan="2"><h2 class="groupheader"><a id="typedef-members" name="typedef-members"></a>
Typedefs</h2></td></tr>
<tr class="memitem:ad3a3cb812bffb092ce5a4791b841b168"><td class="memItemLeft" align="right" valign="top">using&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacennfw_1_1cker_1_1optimized.html#ad3a3cb812bffb092ce5a4791b841b168">BinaryOpImplFloatFuncs</a> = std::pair&lt; void(*)(int, const <a class="el" href="structnnfw_1_1cker_1_1_binary_arithmetic_op_param.html">BinaryArithmeticOpParam</a> &amp;, const float *, const float *, float *), void(*)(int, const <a class="el" href="structnnfw_1_1cker_1_1_binary_arithmetic_op_param.html">BinaryArithmeticOpParam</a> &amp;, const float, const float *, float *)&gt;</td></tr>
<tr class="separator:ad3a3cb812bffb092ce5a4791b841b168"><td class="memSeparator" colspan="2">&#160;</td></tr>
</table><table class="memberdecls">
<tr class="heading"><td colspan="2"><h2 class="groupheader"><a id="func-members" name="func-members"></a>
Functions</h2></td></tr>
<tr class="memitem:acc41c0b6a568f6a484fbfc9618e0a055"><td class="memTemplParams" colspan="2">template&lt;typename ElementwiseF , typename ScalarBroadcastF , typename T &gt; </td></tr>
<tr class="memitem:acc41c0b6a568f6a484fbfc9618e0a055"><td class="memTemplItemLeft" align="right" valign="top">void&#160;</td><td class="memTemplItemRight" valign="bottom"><a class="el" href="namespacennfw_1_1cker_1_1optimized.html#acc41c0b6a568f6a484fbfc9618e0a055">BinaryBroadcastFiveFold</a> (const <a class="el" href="structnnfw_1_1cker_1_1_binary_arithmetic_op_param.html">BinaryArithmeticOpParam</a> &amp;params, bool switch_inputs, const <a class="el" href="classnnfw_1_1cker_1_1_shape.html">Shape</a> &amp;, const T *unswitched_input1_data, const <a class="el" href="classnnfw_1_1cker_1_1_shape.html">Shape</a> &amp;, const T *unswitched_input2_data, const <a class="el" href="classnnfw_1_1cker_1_1_shape.html">Shape</a> &amp;, T *output_data, ElementwiseF elementwise_f, ScalarBroadcastF scalar_broadcast_f)</td></tr>
<tr class="separator:acc41c0b6a568f6a484fbfc9618e0a055"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a549468359ceb389098998f7af1e959a4"><td class="memTemplParams" colspan="2">template&lt;typename ElementwiseF , typename ScalarBroadcastF , typename T &gt; </td></tr>
<tr class="memitem:a549468359ceb389098998f7af1e959a4"><td class="memTemplItemLeft" align="right" valign="top">void&#160;</td><td class="memTemplItemRight" valign="bottom"><a class="el" href="namespacennfw_1_1cker_1_1optimized.html#a549468359ceb389098998f7af1e959a4">BinaryBroadcastFiveFold</a> (const <a class="el" href="structnnfw_1_1cker_1_1_binary_arithmetic_op_param.html">BinaryArithmeticOpParam</a> &amp;unswitched_params, const <a class="el" href="classnnfw_1_1cker_1_1_shape.html">Shape</a> &amp;, const T *unswitched_input1_data, const <a class="el" href="classnnfw_1_1cker_1_1_shape.html">Shape</a> &amp;, const T *unswitched_input2_data, const <a class="el" href="classnnfw_1_1cker_1_1_shape.html">Shape</a> &amp;, T *output_data, ElementwiseF elementwise_f, ScalarBroadcastF scalar_broadcast_f)</td></tr>
<tr class="separator:a549468359ceb389098998f7af1e959a4"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a1b089b3d4d74b15a41c88996227a23e3"><td class="memTemplParams" colspan="2">template&lt;typename T &gt; </td></tr>
<tr class="memitem:a1b089b3d4d74b15a41c88996227a23e3"><td class="memTemplItemLeft" align="right" valign="top">std::enable_if_t&lt; <a class="el" href="structnnfw_1_1cker_1_1is__quant8.html">is_quant8</a>&lt; T &gt;::value, int32_t &gt;&#160;</td><td class="memTemplItemRight" valign="bottom"><a class="el" href="namespacennfw_1_1cker_1_1optimized.html#a1b089b3d4d74b15a41c88996227a23e3">quant8_sum</a> (const <a class="el" href="structnnfw_1_1cker_1_1_binary_arithmetic_op_param.html">BinaryArithmeticOpParam</a> &amp;params, const T input1_data, const T input2_data)</td></tr>
<tr class="separator:a1b089b3d4d74b15a41c88996227a23e3"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a210f92581f9a80300d9eb4dd4f6957c4"><td class="memItemLeft" align="right" valign="top">void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacennfw_1_1cker_1_1optimized.html#a210f92581f9a80300d9eb4dd4f6957c4">AddElementwise</a> (int size, const <a class="el" href="structnnfw_1_1cker_1_1_binary_arithmetic_op_param.html">BinaryArithmeticOpParam</a> &amp;params, const uint8_t *input1_data, const uint8_t *input2_data, uint8_t *output_data)</td></tr>
<tr class="separator:a210f92581f9a80300d9eb4dd4f6957c4"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a0995e9d972661fccc08859151b5ff109"><td class="memItemLeft" align="right" valign="top">void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacennfw_1_1cker_1_1optimized.html#a0995e9d972661fccc08859151b5ff109">AddElementwise</a> (int size, const <a class="el" href="structnnfw_1_1cker_1_1_binary_arithmetic_op_param.html">BinaryArithmeticOpParam</a> &amp;params, const int8_t *input1_data, const int8_t *input2_data, int8_t *output_data)</td></tr>
<tr class="separator:a0995e9d972661fccc08859151b5ff109"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ac33aef7d437da144093878745494d694"><td class="memTemplParams" colspan="2">template&lt;class OPERATOR , class <a class="el" href="activation__float__helpers_8h.html#abbc420da5dec17216bb014c05ad65304">ACTIVATION</a> &gt; </td></tr>
<tr class="memitem:ac33aef7d437da144093878745494d694"><td class="memTemplItemLeft" align="right" valign="top">void&#160;</td><td class="memTemplItemRight" valign="bottom"><a class="el" href="namespacennfw_1_1cker_1_1optimized.html#ac33aef7d437da144093878745494d694">BinaryOpElementwise</a> (int size, const <a class="el" href="structnnfw_1_1cker_1_1_binary_arithmetic_op_param.html">BinaryArithmeticOpParam</a> &amp;params, const float *input1_data, const float *input2_data, float *output_data)</td></tr>
<tr class="separator:ac33aef7d437da144093878745494d694"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a255edd47f2824083009d823d12646d1f"><td class="memTemplParams" colspan="2">template&lt;class OPERATOR , class <a class="el" href="activation__float__helpers_8h.html#abbc420da5dec17216bb014c05ad65304">ACTIVATION</a> &gt; </td></tr>
<tr class="memitem:a255edd47f2824083009d823d12646d1f"><td class="memTemplItemLeft" align="right" valign="top">void&#160;</td><td class="memTemplItemRight" valign="bottom"><a class="el" href="namespacennfw_1_1cker_1_1optimized.html#a255edd47f2824083009d823d12646d1f">BinaryOpScalarBroadcast</a> (int size, const <a class="el" href="structnnfw_1_1cker_1_1_binary_arithmetic_op_param.html">BinaryArithmeticOpParam</a> &amp;params, const float broadcast_value, const float *input2_data, float *output_data)</td></tr>
<tr class="separator:a255edd47f2824083009d823d12646d1f"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ac1cccd42931df44b8f4e71ce728429eb"><td class="memTemplParams" colspan="2">template&lt;class FUNC &gt; </td></tr>
<tr class="memitem:ac1cccd42931df44b8f4e71ce728429eb"><td class="memTemplItemLeft" align="right" valign="top"><a class="el" href="namespacennfw_1_1cker_1_1optimized.html#ad3a3cb812bffb092ce5a4791b841b168">BinaryOpImplFloatFuncs</a>&#160;</td><td class="memTemplItemRight" valign="bottom"><a class="el" href="namespacennfw_1_1cker_1_1optimized.html#ac1cccd42931df44b8f4e71ce728429eb">getBinaryOpWithActivationImplFloat</a> (const <a class="el" href="structnnfw_1_1cker_1_1_binary_arithmetic_op_param.html">BinaryArithmeticOpParam</a> &amp;params)</td></tr>
<tr class="separator:ac1cccd42931df44b8f4e71ce728429eb"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:af9d3da85d0e72b382c3008baa4d49198"><td class="memTemplParams" colspan="2">template&lt;typename T &gt; </td></tr>
<tr class="memitem:af9d3da85d0e72b382c3008baa4d49198"><td class="memTemplItemLeft" align="right" valign="top">std::enable_if_t&lt; <a class="el" href="structnnfw_1_1cker_1_1is__quant8.html">is_quant8</a>&lt; T &gt;::value &gt;&#160;</td><td class="memTemplItemRight" valign="bottom"><a class="el" href="namespacennfw_1_1cker_1_1optimized.html#af9d3da85d0e72b382c3008baa4d49198">Add</a> (const <a class="el" href="structnnfw_1_1cker_1_1_binary_arithmetic_op_param.html">BinaryArithmeticOpParam</a> &amp;params, const <a class="el" href="classnnfw_1_1cker_1_1_shape.html">Shape</a> &amp;input1_shape, const T *input1_data, const <a class="el" href="classnnfw_1_1cker_1_1_shape.html">Shape</a> &amp;input2_shape, const T *input2_data, const <a class="el" href="classnnfw_1_1cker_1_1_shape.html">Shape</a> &amp;output_shape, T *output_data)</td></tr>
<tr class="separator:af9d3da85d0e72b382c3008baa4d49198"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a4354ea8c863f6038e8a744aee32440ab"><td class="memItemLeft" align="right" valign="top">void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacennfw_1_1cker_1_1optimized.html#a4354ea8c863f6038e8a744aee32440ab">Add</a> (const <a class="el" href="structnnfw_1_1cker_1_1_binary_arithmetic_op_param.html">BinaryArithmeticOpParam</a> &amp;params, const <a class="el" href="classnnfw_1_1cker_1_1_shape.html">Shape</a> &amp;input1_shape, const float *input1_data, const <a class="el" href="classnnfw_1_1cker_1_1_shape.html">Shape</a> &amp;input2_shape, const float *input2_data, const <a class="el" href="classnnfw_1_1cker_1_1_shape.html">Shape</a> &amp;output_shape, float *output_data)</td></tr>
<tr class="separator:a4354ea8c863f6038e8a744aee32440ab"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a400314f9559841f0817833b3e2afcb3f"><td class="memItemLeft" align="right" valign="top">void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacennfw_1_1cker_1_1optimized.html#a400314f9559841f0817833b3e2afcb3f">AddScalarBroadcast</a> (int size, const <a class="el" href="structnnfw_1_1cker_1_1_binary_arithmetic_op_param.html">BinaryArithmeticOpParam</a> &amp;params, uint8_t broadcast_value, const uint8_t *input2_data, uint8_t *output_data)</td></tr>
<tr class="separator:a400314f9559841f0817833b3e2afcb3f"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ac9c61e2ab25f05ac63b562a6f58b1b77"><td class="memItemLeft" align="right" valign="top">void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacennfw_1_1cker_1_1optimized.html#ac9c61e2ab25f05ac63b562a6f58b1b77">AddScalarBroadcast</a> (int size, const <a class="el" href="structnnfw_1_1cker_1_1_binary_arithmetic_op_param.html">BinaryArithmeticOpParam</a> &amp;params, int8_t input1_data, const int8_t *input2_data, int8_t *output_data)</td></tr>
<tr class="separator:ac9c61e2ab25f05ac63b562a6f58b1b77"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a28dc97b9306953b16c0572b361913edd"><td class="memTemplParams" colspan="2">template&lt;typename T &gt; </td></tr>
<tr class="memitem:a28dc97b9306953b16c0572b361913edd"><td class="memTemplItemLeft" align="right" valign="top">std::enable_if_t&lt; <a class="el" href="structnnfw_1_1cker_1_1is__quant8.html">is_quant8</a>&lt; T &gt;::value &gt;&#160;</td><td class="memTemplItemRight" valign="bottom"><a class="el" href="namespacennfw_1_1cker_1_1optimized.html#a28dc97b9306953b16c0572b361913edd">BroadcastAddDispatch</a> (const <a class="el" href="structnnfw_1_1cker_1_1_binary_arithmetic_op_param.html">BinaryArithmeticOpParam</a> &amp;params, const <a class="el" href="classnnfw_1_1cker_1_1_shape.html">Shape</a> &amp;input1_shape, const T *input1_data, const <a class="el" href="classnnfw_1_1cker_1_1_shape.html">Shape</a> &amp;input2_shape, const T *input2_data, const <a class="el" href="classnnfw_1_1cker_1_1_shape.html">Shape</a> &amp;output_shape, T *output_data)</td></tr>
<tr class="separator:a28dc97b9306953b16c0572b361913edd"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:aba80b710dc4100dceaec38389b9aeaa3"><td class="memItemLeft" align="right" valign="top">void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacennfw_1_1cker_1_1optimized.html#aba80b710dc4100dceaec38389b9aeaa3">BroadcastAddDispatch</a> (const <a class="el" href="structnnfw_1_1cker_1_1_binary_arithmetic_op_param.html">BinaryArithmeticOpParam</a> &amp;params, const <a class="el" href="classnnfw_1_1cker_1_1_shape.html">Shape</a> &amp;input1_shape, const float *input1_data, const <a class="el" href="classnnfw_1_1cker_1_1_shape.html">Shape</a> &amp;input2_shape, const float *input2_data, const <a class="el" href="classnnfw_1_1cker_1_1_shape.html">Shape</a> &amp;output_shape, float *output_data)</td></tr>
<tr class="separator:aba80b710dc4100dceaec38389b9aeaa3"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a2fe73b064c2531585c43abd6e9ee929d"><td class="memItemLeft" align="right" valign="top">void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacennfw_1_1cker_1_1optimized.html#a2fe73b064c2531585c43abd6e9ee929d">Sub</a> (const <a class="el" href="structnnfw_1_1cker_1_1_binary_arithmetic_op_param.html">BinaryArithmeticOpParam</a> &amp;params, const <a class="el" href="classnnfw_1_1cker_1_1_shape.html">Shape</a> &amp;input1_shape, const float *input1_data, const <a class="el" href="classnnfw_1_1cker_1_1_shape.html">Shape</a> &amp;input2_shape, const float *input2_data, const <a class="el" href="classnnfw_1_1cker_1_1_shape.html">Shape</a> &amp;output_shape, float *output_data)</td></tr>
<tr class="separator:a2fe73b064c2531585c43abd6e9ee929d"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ab6b4d976d595839e9c5c829c07f82de2"><td class="memItemLeft" align="right" valign="top">void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacennfw_1_1cker_1_1optimized.html#ab6b4d976d595839e9c5c829c07f82de2">BroadcastSubDispatch</a> (const <a class="el" href="structnnfw_1_1cker_1_1_binary_arithmetic_op_param.html">BinaryArithmeticOpParam</a> &amp;params, const <a class="el" href="classnnfw_1_1cker_1_1_shape.html">Shape</a> &amp;input1_shape, const float *input1_data, const <a class="el" href="classnnfw_1_1cker_1_1_shape.html">Shape</a> &amp;input2_shape, const float *input2_data, const <a class="el" href="classnnfw_1_1cker_1_1_shape.html">Shape</a> &amp;output_shape, float *output_data)</td></tr>
<tr class="separator:ab6b4d976d595839e9c5c829c07f82de2"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a19f1d857236b4b11c737cb4fe7cc7567"><td class="memTemplParams" colspan="2">template&lt;typename T &gt; </td></tr>
<tr class="memitem:a19f1d857236b4b11c737cb4fe7cc7567"><td class="memTemplItemLeft" align="right" valign="top">std::enable_if_t&lt; <a class="el" href="structnnfw_1_1cker_1_1is__quant8.html">is_quant8</a>&lt; T &gt;::value, int32_t &gt;&#160;</td><td class="memTemplItemRight" valign="bottom"><a class="el" href="namespacennfw_1_1cker_1_1optimized.html#a19f1d857236b4b11c737cb4fe7cc7567">quant8_mul</a> (const <a class="el" href="structnnfw_1_1cker_1_1_binary_arithmetic_op_param.html">BinaryArithmeticOpParam</a> &amp;params, const T input1_data, const T input2_data)</td></tr>
<tr class="separator:a19f1d857236b4b11c737cb4fe7cc7567"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a38f818fd8d5df7e473fa22d6491d0ad1"><td class="memItemLeft" align="right" valign="top">void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacennfw_1_1cker_1_1optimized.html#a38f818fd8d5df7e473fa22d6491d0ad1">MulElementwise</a> (int size, const <a class="el" href="structnnfw_1_1cker_1_1_binary_arithmetic_op_param.html">BinaryArithmeticOpParam</a> &amp;params, const uint8_t *input1_data, const uint8_t *input2_data, uint8_t *output_data)</td></tr>
<tr class="separator:a38f818fd8d5df7e473fa22d6491d0ad1"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a002839028eca24a863415f9a73e13958"><td class="memItemLeft" align="right" valign="top">void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacennfw_1_1cker_1_1optimized.html#a002839028eca24a863415f9a73e13958">MulElementwise</a> (int size, const <a class="el" href="structnnfw_1_1cker_1_1_binary_arithmetic_op_param.html">BinaryArithmeticOpParam</a> &amp;params, const int8_t *input1_data, const int8_t *input2_data, int8_t *output_data)</td></tr>
<tr class="separator:a002839028eca24a863415f9a73e13958"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a09dafd64d21fcf9e4c9a1b4c735d1b22"><td class="memTemplParams" colspan="2">template&lt;typename T &gt; </td></tr>
<tr class="memitem:a09dafd64d21fcf9e4c9a1b4c735d1b22"><td class="memTemplItemLeft" align="right" valign="top">std::enable_if_t&lt; <a class="el" href="structnnfw_1_1cker_1_1is__quant8.html">is_quant8</a>&lt; T &gt;::value &gt;&#160;</td><td class="memTemplItemRight" valign="bottom"><a class="el" href="namespacennfw_1_1cker_1_1optimized.html#a09dafd64d21fcf9e4c9a1b4c735d1b22">Mul</a> (const <a class="el" href="structnnfw_1_1cker_1_1_binary_arithmetic_op_param.html">BinaryArithmeticOpParam</a> &amp;params, const <a class="el" href="classnnfw_1_1cker_1_1_shape.html">Shape</a> &amp;input1_shape, const T *input1_data, const <a class="el" href="classnnfw_1_1cker_1_1_shape.html">Shape</a> &amp;input2_shape, const T *input2_data, const <a class="el" href="classnnfw_1_1cker_1_1_shape.html">Shape</a> &amp;output_shape, T *output_data)</td></tr>
<tr class="separator:a09dafd64d21fcf9e4c9a1b4c735d1b22"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a4aa4bafb600c8c06ce9dc5217ee5cf22"><td class="memItemLeft" align="right" valign="top">void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacennfw_1_1cker_1_1optimized.html#a4aa4bafb600c8c06ce9dc5217ee5cf22">Mul</a> (const <a class="el" href="structnnfw_1_1cker_1_1_binary_arithmetic_op_param.html">BinaryArithmeticOpParam</a> &amp;params, const <a class="el" href="classnnfw_1_1cker_1_1_shape.html">Shape</a> &amp;input1_shape, const float *input1_data, const <a class="el" href="classnnfw_1_1cker_1_1_shape.html">Shape</a> &amp;input2_shape, const float *input2_data, const <a class="el" href="classnnfw_1_1cker_1_1_shape.html">Shape</a> &amp;output_shape, float *output_data)</td></tr>
<tr class="separator:a4aa4bafb600c8c06ce9dc5217ee5cf22"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a12c1a56c7e65d8f294610e075c9ccc7c"><td class="memItemLeft" align="right" valign="top">void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacennfw_1_1cker_1_1optimized.html#a12c1a56c7e65d8f294610e075c9ccc7c">MulSimpleBroadcast</a> (int size, const <a class="el" href="structnnfw_1_1cker_1_1_binary_arithmetic_op_param.html">BinaryArithmeticOpParam</a> &amp;params, const uint8_t broadcast_value, const uint8_t *input2_data, uint8_t *output_data)</td></tr>
<tr class="separator:a12c1a56c7e65d8f294610e075c9ccc7c"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:af1a0a78101d8a1152b8c8dafe5834292"><td class="memItemLeft" align="right" valign="top">void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacennfw_1_1cker_1_1optimized.html#af1a0a78101d8a1152b8c8dafe5834292">MulSimpleBroadcast</a> (int size, const <a class="el" href="structnnfw_1_1cker_1_1_binary_arithmetic_op_param.html">BinaryArithmeticOpParam</a> &amp;params, const int8_t broadcast_value, const int8_t *input2_data, int8_t *output_data)</td></tr>
<tr class="separator:af1a0a78101d8a1152b8c8dafe5834292"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a5b494d6113f18ea2a286dda9dcc06b5a"><td class="memTemplParams" colspan="2">template&lt;typename T &gt; </td></tr>
<tr class="memitem:a5b494d6113f18ea2a286dda9dcc06b5a"><td class="memTemplItemLeft" align="right" valign="top">std::enable_if_t&lt; <a class="el" href="structnnfw_1_1cker_1_1is__quant8.html">is_quant8</a>&lt; T &gt;::value &gt;&#160;</td><td class="memTemplItemRight" valign="bottom"><a class="el" href="namespacennfw_1_1cker_1_1optimized.html#a5b494d6113f18ea2a286dda9dcc06b5a">BroadcastMulDispatch</a> (const <a class="el" href="structnnfw_1_1cker_1_1_binary_arithmetic_op_param.html">BinaryArithmeticOpParam</a> &amp;params, const <a class="el" href="classnnfw_1_1cker_1_1_shape.html">Shape</a> &amp;input1_shape, const T *input1_data, const <a class="el" href="classnnfw_1_1cker_1_1_shape.html">Shape</a> &amp;input2_shape, const T *input2_data, const <a class="el" href="classnnfw_1_1cker_1_1_shape.html">Shape</a> &amp;output_shape, T *output_data)</td></tr>
<tr class="separator:a5b494d6113f18ea2a286dda9dcc06b5a"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a30d2c6537369842372153972d702867c"><td class="memItemLeft" align="right" valign="top">void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacennfw_1_1cker_1_1optimized.html#a30d2c6537369842372153972d702867c">BroadcastMulDispatch</a> (const <a class="el" href="structnnfw_1_1cker_1_1_binary_arithmetic_op_param.html">BinaryArithmeticOpParam</a> &amp;params, const <a class="el" href="classnnfw_1_1cker_1_1_shape.html">Shape</a> &amp;input1_shape, const float *input1_data, const <a class="el" href="classnnfw_1_1cker_1_1_shape.html">Shape</a> &amp;input2_shape, const float *input2_data, const <a class="el" href="classnnfw_1_1cker_1_1_shape.html">Shape</a> &amp;output_shape, float *output_data)</td></tr>
<tr class="separator:a30d2c6537369842372153972d702867c"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ad0750df2998efe611386c45ffd03b581"><td class="memItemLeft" align="right" valign="top">void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacennfw_1_1cker_1_1optimized.html#ad0750df2998efe611386c45ffd03b581">Div</a> (const <a class="el" href="structnnfw_1_1cker_1_1_binary_arithmetic_op_param.html">BinaryArithmeticOpParam</a> &amp;params, const <a class="el" href="classnnfw_1_1cker_1_1_shape.html">Shape</a> &amp;input1_shape, const float *input1_data, const <a class="el" href="classnnfw_1_1cker_1_1_shape.html">Shape</a> &amp;input2_shape, const float *input2_data, const <a class="el" href="classnnfw_1_1cker_1_1_shape.html">Shape</a> &amp;output_shape, float *output_data)</td></tr>
<tr class="separator:ad0750df2998efe611386c45ffd03b581"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a0665493b253e34ef95e0686933cab6bc"><td class="memItemLeft" align="right" valign="top">void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacennfw_1_1cker_1_1optimized.html#a0665493b253e34ef95e0686933cab6bc">BroadcastDivDispatch</a> (const <a class="el" href="structnnfw_1_1cker_1_1_binary_arithmetic_op_param.html">BinaryArithmeticOpParam</a> &amp;params, const <a class="el" href="classnnfw_1_1cker_1_1_shape.html">Shape</a> &amp;input1_shape, const float *input1_data, const <a class="el" href="classnnfw_1_1cker_1_1_shape.html">Shape</a> &amp;input2_shape, const float *input2_data, const <a class="el" href="classnnfw_1_1cker_1_1_shape.html">Shape</a> &amp;output_shape, float *output_data)</td></tr>
<tr class="separator:a0665493b253e34ef95e0686933cab6bc"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ae5bcc95267018d54f77dd908ef6077ad"><td class="memItemLeft" align="right" valign="top">void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacennfw_1_1cker_1_1optimized.html#ae5bcc95267018d54f77dd908ef6077ad">AddBiasAndEvalActivationFunction</a> (float output_activation_min, float output_activation_max, const <a class="el" href="classnnfw_1_1cker_1_1_shape.html">Shape</a> &amp;bias_shape, const float *bias_data, const <a class="el" href="classnnfw_1_1cker_1_1_shape.html">Shape</a> &amp;array_shape, float *array_data)</td></tr>
<tr class="separator:ae5bcc95267018d54f77dd908ef6077ad"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a201402ddf5f904cc04577124681cf526"><td class="memItemLeft" align="right" valign="top">void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacennfw_1_1cker_1_1optimized.html#a201402ddf5f904cc04577124681cf526">Conv</a> (const <a class="el" href="structnnfw_1_1cker_1_1_conv_params.html">ConvParams</a> &amp;params, const <a class="el" href="classnnfw_1_1cker_1_1_shape.html">Shape</a> &amp;input_shape, const uint8_t *input_data, const <a class="el" href="classnnfw_1_1cker_1_1_shape.html">Shape</a> &amp;filter_shape, const uint8_t *filter_data, const <a class="el" href="classnnfw_1_1cker_1_1_shape.html">Shape</a> &amp;bias_shape, const int32_t *bias_data, const <a class="el" href="classnnfw_1_1cker_1_1_shape.html">Shape</a> &amp;output_shape, uint8_t *output_data, const <a class="el" href="classnnfw_1_1cker_1_1_shape.html">Shape</a> &amp;im2col_shape, uint8_t *im2col_data)</td></tr>
<tr class="separator:a201402ddf5f904cc04577124681cf526"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ac8ea6e7a1b57d1cd9f70bc0035be203c"><td class="memTemplParams" colspan="2">template&lt;bool kAllowStrided, int kFixedInputDepth, int kFixedDepthMultiplier&gt; </td></tr>
<tr class="memitem:ac8ea6e7a1b57d1cd9f70bc0035be203c"><td class="memTemplItemLeft" align="right" valign="top">void&#160;</td><td class="memTemplItemRight" valign="bottom"><a class="el" href="namespacennfw_1_1cker_1_1optimized.html#ac8ea6e7a1b57d1cd9f70bc0035be203c">FloatDepthwiseConvAccumRow</a> (int stride, int dilation_factor, int input_depth, int input_width, const float *input_data, int pad_width, int depth_multiplier, int filter_width, const float *filter_data, int out_x_buffer_start, int out_x_buffer_end, int output_depth, float *acc_buffer)</td></tr>
<tr class="separator:ac8ea6e7a1b57d1cd9f70bc0035be203c"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ad4525396811538b4ed0ec83ded38aadf"><td class="memItemLeft" align="right" valign="top">void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacennfw_1_1cker_1_1optimized.html#ad4525396811538b4ed0ec83ded38aadf">FloatDepthwiseConvAccumRowGeneric</a> (int stride, int dilation_factor, int input_depth, int input_width, const float *input_data, int pad_width, int depth_multiplier, int filter_width, const float *filter_data, int out_x_buffer_start, int out_x_buffer_end, int output_depth, float *acc_buffer)</td></tr>
<tr class="separator:ad4525396811538b4ed0ec83ded38aadf"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a7f4b1eca533bbd4e62a4a44c73f8a803"><td class="memItemLeft" align="right" valign="top">void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacennfw_1_1cker_1_1optimized.html#a7f4b1eca533bbd4e62a4a44c73f8a803">DepthwiseConvInitAccBuffer</a> (int num_output_pixels, int output_depth, const float *bias_data, float *acc_buffer)</td></tr>
<tr class="separator:a7f4b1eca533bbd4e62a4a44c73f8a803"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ad73d8bcc64e3f5565c0a88e353691ba1"><td class="memItemLeft" align="right" valign="top">void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacennfw_1_1cker_1_1optimized.html#ad73d8bcc64e3f5565c0a88e353691ba1">DepthwiseConvImpl</a> (const <a class="el" href="structnnfw_1_1cker_1_1_depthwise_conv_params.html">DepthwiseConvParams</a> &amp;params, const <a class="el" href="classnnfw_1_1cker_1_1_shape.html">Shape</a> &amp;input_shape, const float *input_data, const <a class="el" href="classnnfw_1_1cker_1_1_shape.html">Shape</a> &amp;filter_shape, const float *filter_data, const <a class="el" href="classnnfw_1_1cker_1_1_shape.html">Shape</a> &amp;bias_shape, const float *bias_data, const <a class="el" href="classnnfw_1_1cker_1_1_shape.html">Shape</a> &amp;output_shape, float *output_data, int thread_start, int thread_end, int thread_dim)</td></tr>
<tr class="separator:ad73d8bcc64e3f5565c0a88e353691ba1"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ab9b6acc151e330c9742628a2f37650b6"><td class="memItemLeft" align="right" valign="top">void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacennfw_1_1cker_1_1optimized.html#ab9b6acc151e330c9742628a2f37650b6">DepthwiseConvWithRounding</a> (const <a class="el" href="structnnfw_1_1cker_1_1_depthwise_conv_params.html">DepthwiseConvParams</a> &amp;params, const <a class="el" href="classnnfw_1_1cker_1_1_shape.html">Shape</a> &amp;input_shape, const uint8_t *input_data, const <a class="el" href="classnnfw_1_1cker_1_1_shape.html">Shape</a> &amp;filter_shape, const uint8_t *filter_data, const <a class="el" href="classnnfw_1_1cker_1_1_shape.html">Shape</a> &amp;bias_shape, const int32_t *bias_data, const <a class="el" href="classnnfw_1_1cker_1_1_shape.html">Shape</a> &amp;output_shape, uint8_t *output_data, int thread_start, int thread_end, int thread_dim)</td></tr>
<tr class="separator:ab9b6acc151e330c9742628a2f37650b6"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a7f843a8c10add9f97826e1952b289a07"><td class="memItemLeft" align="right" valign="top">void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacennfw_1_1cker_1_1optimized.html#a7f843a8c10add9f97826e1952b289a07">DepthwiseConvImpl</a> (const <a class="el" href="structnnfw_1_1cker_1_1_depthwise_conv_params.html">DepthwiseConvParams</a> &amp;params, const <a class="el" href="classnnfw_1_1cker_1_1_shape.html">Shape</a> &amp;input_shape, const uint8_t *input_data, const <a class="el" href="classnnfw_1_1cker_1_1_shape.html">Shape</a> &amp;filter_shape, const uint8_t *filter_data, const <a class="el" href="classnnfw_1_1cker_1_1_shape.html">Shape</a> &amp;bias_shape, const int32_t *bias_data, const <a class="el" href="classnnfw_1_1cker_1_1_shape.html">Shape</a> &amp;output_shape, uint8_t *output_data, int thread_start, int thread_end, int thread_dim)</td></tr>
<tr class="separator:a7f843a8c10add9f97826e1952b289a07"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a3faac8ad96bfaec0b409322c20a42230"><td class="memTemplParams" colspan="2">template&lt;typename T &gt; </td></tr>
<tr class="memitem:a3faac8ad96bfaec0b409322c20a42230"><td class="memTemplItemLeft" align="right" valign="top">void&#160;</td><td class="memTemplItemRight" valign="bottom"><a class="el" href="namespacennfw_1_1cker_1_1optimized.html#a3faac8ad96bfaec0b409322c20a42230">ExtractPatchIntoBufferColumn</a> (const <a class="el" href="classnnfw_1_1cker_1_1_shape.html">Shape</a> &amp;input_shape, int w, int h, int b, int kheight, int kwidth, int stride_width, int stride_height, int pad_width, int pad_height, int in_width, int in_height, int in_depth, int single_buffer_length, int buffer_id, const T *in_data, T *conv_buffer_data, uint8_t zero_byte)</td></tr>
<tr class="separator:a3faac8ad96bfaec0b409322c20a42230"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a403e08d865d6f45ba561998b93304d57"><td class="memTemplParams" colspan="2">template&lt;typename T &gt; </td></tr>
<tr class="memitem:a403e08d865d6f45ba561998b93304d57"><td class="memTemplItemLeft" align="right" valign="top">void&#160;</td><td class="memTemplItemRight" valign="bottom"><a class="el" href="namespacennfw_1_1cker_1_1optimized.html#a403e08d865d6f45ba561998b93304d57">DilatedIm2col</a> (const <a class="el" href="structnnfw_1_1cker_1_1_conv_params.html">ConvParams</a> &amp;params, const <a class="el" href="classnnfw_1_1cker_1_1_shape.html">Shape</a> &amp;input_shape, const T *input_data, const <a class="el" href="classnnfw_1_1cker_1_1_shape.html">Shape</a> &amp;filter_shape, const <a class="el" href="classnnfw_1_1cker_1_1_shape.html">Shape</a> &amp;output_shape, T *im2col_data, const int32_t *zero_bytes, const int zero_bytes_len)</td></tr>
<tr class="separator:a403e08d865d6f45ba561998b93304d57"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ac7c3ecad381acad6f04dbc55ed5f750e"><td class="memTemplParams" colspan="2">template&lt;typename T &gt; </td></tr>
<tr class="memitem:ac7c3ecad381acad6f04dbc55ed5f750e"><td class="memTemplItemLeft" align="right" valign="top">void&#160;</td><td class="memTemplItemRight" valign="bottom"><a class="el" href="namespacennfw_1_1cker_1_1optimized.html#ac7c3ecad381acad6f04dbc55ed5f750e">DilatedIm2col</a> (const <a class="el" href="structnnfw_1_1cker_1_1_conv_params.html">ConvParams</a> &amp;params, uint8_t zero_byte, const <a class="el" href="classnnfw_1_1cker_1_1_shape.html">Shape</a> &amp;input_shape, const T *input_data, const <a class="el" href="classnnfw_1_1cker_1_1_shape.html">Shape</a> &amp;filter_shape, const <a class="el" href="classnnfw_1_1cker_1_1_shape.html">Shape</a> &amp;output_shape, T *im2col_data)</td></tr>
<tr class="separator:ac7c3ecad381acad6f04dbc55ed5f750e"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a94dc0ed7df7b941c9a6f0b67a1155e51"><td class="memTemplParams" colspan="2">template&lt;typename T &gt; </td></tr>
<tr class="memitem:a94dc0ed7df7b941c9a6f0b67a1155e51"><td class="memTemplItemLeft" align="right" valign="top">void&#160;</td><td class="memTemplItemRight" valign="bottom"><a class="el" href="namespacennfw_1_1cker_1_1optimized.html#a94dc0ed7df7b941c9a6f0b67a1155e51">Im2col</a> (const <a class="el" href="structnnfw_1_1cker_1_1_conv_params.html">ConvParams</a> &amp;params, int kheight, int kwidth, uint8_t zero_byte, const <a class="el" href="classnnfw_1_1cker_1_1_shape.html">Shape</a> &amp;input_shape, const T *input_data, const <a class="el" href="classnnfw_1_1cker_1_1_shape.html">Shape</a> &amp;output_shape, T *output_data)</td></tr>
<tr class="separator:a94dc0ed7df7b941c9a6f0b67a1155e51"><td class="memSeparator" colspan="2">&#160;</td></tr>
</table><table class="memberdecls">
<tr class="heading"><td colspan="2"><h2 class="groupheader"><a id="var-members" name="var-members"></a>
Variables</h2></td></tr>
<tr class="memitem:a4bac55e13be194606f4691fd24dae393"><td class="memItemLeft" align="right" valign="top">std::mutex&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacennfw_1_1cker_1_1optimized.html#a4bac55e13be194606f4691fd24dae393">_gemmlowp_mutex</a></td></tr>
<tr class="separator:a4bac55e13be194606f4691fd24dae393"><td class="memSeparator" colspan="2">&#160;</td></tr>
</table>
<h2 class="groupheader">Typedef Documentation</h2>
<a id="ad3a3cb812bffb092ce5a4791b841b168" name="ad3a3cb812bffb092ce5a4791b841b168"></a>
<h2 class="memtitle"><span class="permalink"><a href="#ad3a3cb812bffb092ce5a4791b841b168">&#9670;&#160;</a></span>BinaryOpImplFloatFuncs</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">using <a class="el" href="namespacennfw_1_1cker_1_1optimized.html#ad3a3cb812bffb092ce5a4791b841b168">nnfw::cker::optimized::BinaryOpImplFloatFuncs</a> = typedef std::pair&lt;void (*)(int, const <a class="el" href="structnnfw_1_1cker_1_1_binary_arithmetic_op_param.html">BinaryArithmeticOpParam</a> &amp;, const float *, const float *, float *), void (*)(int, const <a class="el" href="structnnfw_1_1cker_1_1_binary_arithmetic_op_param.html">BinaryArithmeticOpParam</a> &amp;, const float, const float *, float *)&gt;</td>
        </tr>
      </table>
</div><div class="memdoc">

<p class="definition">Definition at line <a class="el" href="optimized_2_binary_arithmetic_ops_8h_source.html#l00670">670</a> of file <a class="el" href="optimized_2_binary_arithmetic_ops_8h_source.html">BinaryArithmeticOps.h</a>.</p>

</div>
</div>
<h2 class="groupheader">Function Documentation</h2>
<a id="a4354ea8c863f6038e8a744aee32440ab" name="a4354ea8c863f6038e8a744aee32440ab"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a4354ea8c863f6038e8a744aee32440ab">&#9670;&#160;</a></span>Add() <span class="overload">[1/2]</span></h2>

<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">void nnfw::cker::optimized::Add </td>
          <td>(</td>
          <td class="paramtype">const <a class="el" href="structnnfw_1_1cker_1_1_binary_arithmetic_op_param.html">BinaryArithmeticOpParam</a> &amp;&#160;</td>
          <td class="paramname"><em>params</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const <a class="el" href="classnnfw_1_1cker_1_1_shape.html">Shape</a> &amp;&#160;</td>
          <td class="paramname"><em>input1_shape</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const float *&#160;</td>
          <td class="paramname"><em>input1_data</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const <a class="el" href="classnnfw_1_1cker_1_1_shape.html">Shape</a> &amp;&#160;</td>
          <td class="paramname"><em>input2_shape</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const float *&#160;</td>
          <td class="paramname"><em>input2_data</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const <a class="el" href="classnnfw_1_1cker_1_1_shape.html">Shape</a> &amp;&#160;</td>
          <td class="paramname"><em>output_shape</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">float *&#160;</td>
          <td class="paramname"><em>output_data</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">inline</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p class="definition">Definition at line <a class="el" href="optimized_2_binary_arithmetic_ops_8h_source.html#l00699">699</a> of file <a class="el" href="optimized_2_binary_arithmetic_ops_8h_source.html">BinaryArithmeticOps.h</a>.</p>
<div class="fragment"><div class="line"><span class="lineno">  702</span>{</div>
<div class="line"><span class="lineno">  703</span>  <span class="keyword">const</span> <span class="keywordtype">int</span> flat_size = <a class="code hl_function" href="namespacennfw_1_1cker.html#ab86126de4e835f9499051e43b841abca">MatchingElementsSize</a>(input1_shape, input2_shape, output_shape);</div>
<div class="line"><span class="lineno">  704</span>  <span class="keyword">auto</span> implFuncs = getBinaryOpWithActivationImplFloat&lt;BinaryOpFuncAddFloat&gt;(params);</div>
<div class="line"><span class="lineno">  705</span>  (*implFuncs.first)(flat_size, params, input1_data, input2_data, output_data);</div>
<div class="line"><span class="lineno">  706</span>}</div>
<div class="ttc" id="anamespacennfw_1_1cker_html_ab86126de4e835f9499051e43b841abca"><div class="ttname"><a href="namespacennfw_1_1cker.html#ab86126de4e835f9499051e43b841abca">nnfw::cker::MatchingElementsSize</a></div><div class="ttdeci">int MatchingElementsSize(const Shape &amp;shape, const Shape &amp;check_shape_0, const Shape &amp;check_shape_1)</div><div class="ttdef"><b>Definition:</b> <a href="compute_2cker_2include_2cker_2_shape_8h_source.html#l00337">Shape.h:337</a></div></div>
</div><!-- fragment -->
<p class="reference">References <a class="el" href="compute_2cker_2include_2cker_2_shape_8h_source.html#l00337">nnfw::cker::MatchingElementsSize()</a>.</p>

</div>
</div>
<a id="af9d3da85d0e72b382c3008baa4d49198" name="af9d3da85d0e72b382c3008baa4d49198"></a>
<h2 class="memtitle"><span class="permalink"><a href="#af9d3da85d0e72b382c3008baa4d49198">&#9670;&#160;</a></span>Add() <span class="overload">[2/2]</span></h2>

<div class="memitem">
<div class="memproto">
<div class="memtemplate">
template&lt;typename T &gt; </div>
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">std::enable_if_t&lt; <a class="el" href="structnnfw_1_1cker_1_1is__quant8.html">is_quant8</a>&lt; T &gt;::value &gt; nnfw::cker::optimized::Add </td>
          <td>(</td>
          <td class="paramtype">const <a class="el" href="structnnfw_1_1cker_1_1_binary_arithmetic_op_param.html">BinaryArithmeticOpParam</a> &amp;&#160;</td>
          <td class="paramname"><em>params</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const <a class="el" href="classnnfw_1_1cker_1_1_shape.html">Shape</a> &amp;&#160;</td>
          <td class="paramname"><em>input1_shape</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const T *&#160;</td>
          <td class="paramname"><em>input1_data</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const <a class="el" href="classnnfw_1_1cker_1_1_shape.html">Shape</a> &amp;&#160;</td>
          <td class="paramname"><em>input2_shape</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const T *&#160;</td>
          <td class="paramname"><em>input2_data</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const <a class="el" href="classnnfw_1_1cker_1_1_shape.html">Shape</a> &amp;&#160;</td>
          <td class="paramname"><em>output_shape</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">T *&#160;</td>
          <td class="paramname"><em>output_data</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">inline</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p class="definition">Definition at line <a class="el" href="optimized_2_binary_arithmetic_ops_8h_source.html#l00692">692</a> of file <a class="el" href="optimized_2_binary_arithmetic_ops_8h_source.html">BinaryArithmeticOps.h</a>.</p>
<div class="fragment"><div class="line"><span class="lineno">  694</span>{</div>
<div class="line"><span class="lineno">  695</span>  <span class="keyword">const</span> <span class="keywordtype">int</span> flat_size = <a class="code hl_function" href="namespacennfw_1_1cker.html#ab86126de4e835f9499051e43b841abca">MatchingElementsSize</a>(input1_shape, input2_shape, output_shape);</div>
<div class="line"><span class="lineno">  696</span>  <a class="code hl_function" href="namespacennfw_1_1cker_1_1optimized.html#a210f92581f9a80300d9eb4dd4f6957c4">AddElementwise</a>(flat_size, params, input1_data, input2_data, output_data);</div>
<div class="line"><span class="lineno">  697</span>}</div>
<div class="ttc" id="anamespacennfw_1_1cker_1_1optimized_html_a210f92581f9a80300d9eb4dd4f6957c4"><div class="ttname"><a href="namespacennfw_1_1cker_1_1optimized.html#a210f92581f9a80300d9eb4dd4f6957c4">nnfw::cker::optimized::AddElementwise</a></div><div class="ttdeci">void AddElementwise(int size, const BinaryArithmeticOpParam &amp;params, const uint8_t *input1_data, const uint8_t *input2_data, uint8_t *output_data)</div><div class="ttdef"><b>Definition:</b> <a href="optimized_2_binary_arithmetic_ops_8h_source.html#l00246">BinaryArithmeticOps.h:246</a></div></div>
</div><!-- fragment -->
<p class="reference">References <a class="el" href="optimized_2_binary_arithmetic_ops_8h_source.html#l00246">AddElementwise()</a>, and <a class="el" href="compute_2cker_2include_2cker_2_shape_8h_source.html#l00337">nnfw::cker::MatchingElementsSize()</a>.</p>

<p class="reference">Referenced by <a class="el" href="_binary_arithmetic_ops_8h_source.html#l00204">nnfw::cker::BinaryArithmeticOp()</a>.</p>

</div>
</div>
<a id="ae5bcc95267018d54f77dd908ef6077ad" name="ae5bcc95267018d54f77dd908ef6077ad"></a>
<h2 class="memtitle"><span class="permalink"><a href="#ae5bcc95267018d54f77dd908ef6077ad">&#9670;&#160;</a></span>AddBiasAndEvalActivationFunction()</h2>

<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">void nnfw::cker::optimized::AddBiasAndEvalActivationFunction </td>
          <td>(</td>
          <td class="paramtype">float&#160;</td>
          <td class="paramname"><em>output_activation_min</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">float&#160;</td>
          <td class="paramname"><em>output_activation_max</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const <a class="el" href="classnnfw_1_1cker_1_1_shape.html">Shape</a> &amp;&#160;</td>
          <td class="paramname"><em>bias_shape</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const float *&#160;</td>
          <td class="paramname"><em>bias_data</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const <a class="el" href="classnnfw_1_1cker_1_1_shape.html">Shape</a> &amp;&#160;</td>
          <td class="paramname"><em>array_shape</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">float *&#160;</td>
          <td class="paramname"><em>array_data</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">inline</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p class="definition">Definition at line <a class="el" href="compute_2cker_2include_2cker_2operation_2optimized_2_conv_8h_source.html#l00074">74</a> of file <a class="el" href="compute_2cker_2include_2cker_2operation_2optimized_2_conv_8h_source.html">Conv.h</a>.</p>
<div class="fragment"><div class="line"><span class="lineno">   78</span>{</div>
<div class="line"><span class="lineno">   79</span>  <a class="code hl_function" href="namespacennfw_1_1cker.html#a0b43a0c6f558a6287e5ef43f424a17a6">BiasAndClamp</a>(output_activation_min, output_activation_max, bias_shape.<a class="code hl_function" href="classnnfw_1_1cker_1_1_shape.html#ac7e29fd510111992fbd44906e2080f12">FlatSize</a>(), bias_data,</div>
<div class="line"><span class="lineno">   80</span>               array_shape.<a class="code hl_function" href="classnnfw_1_1cker_1_1_shape.html#ac7e29fd510111992fbd44906e2080f12">FlatSize</a>(), array_data);</div>
<div class="line"><span class="lineno">   81</span>}</div>
<div class="ttc" id="aclassnnfw_1_1cker_1_1_shape_html_ac7e29fd510111992fbd44906e2080f12"><div class="ttname"><a href="classnnfw_1_1cker_1_1_shape.html#ac7e29fd510111992fbd44906e2080f12">nnfw::cker::Shape::FlatSize</a></div><div class="ttdeci">int FlatSize() const</div><div class="ttdef"><b>Definition:</b> <a href="compute_2cker_2include_2cker_2_shape_8h_source.html#l00183">Shape.h:183</a></div></div>
<div class="ttc" id="anamespacennfw_1_1cker_html_a0b43a0c6f558a6287e5ef43f424a17a6"><div class="ttname"><a href="namespacennfw_1_1cker.html#a0b43a0c6f558a6287e5ef43f424a17a6">nnfw::cker::BiasAndClamp</a></div><div class="ttdeci">void BiasAndClamp(float clamp_min, float clamp_max, int bias_size, const float *bias_data, int array_size, float *array_data)</div><div class="ttdef"><b>Definition:</b> <a href="compute_2cker_2include_2cker_2operation_2_common_8h_source.html#l00029">Common.h:29</a></div></div>
</div><!-- fragment -->
<p class="reference">References <a class="el" href="compute_2cker_2include_2cker_2operation_2_common_8h_source.html#l00029">nnfw::cker::BiasAndClamp()</a>, and <a class="el" href="compute_2cker_2include_2cker_2_shape_8h_source.html#l00183">nnfw::cker::Shape::FlatSize()</a>.</p>

<p class="reference">Referenced by <a class="el" href="compute_2cker_2include_2cker_2operation_2optimized_2_conv_8h_source.html#l00254">nnfw::cker::multithreaded::Conv()</a>.</p>

</div>
</div>
<a id="a0995e9d972661fccc08859151b5ff109" name="a0995e9d972661fccc08859151b5ff109"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a0995e9d972661fccc08859151b5ff109">&#9670;&#160;</a></span>AddElementwise() <span class="overload">[1/2]</span></h2>

<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">void nnfw::cker::optimized::AddElementwise </td>
          <td>(</td>
          <td class="paramtype">int&#160;</td>
          <td class="paramname"><em>size</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const <a class="el" href="structnnfw_1_1cker_1_1_binary_arithmetic_op_param.html">BinaryArithmeticOpParam</a> &amp;&#160;</td>
          <td class="paramname"><em>params</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const int8_t *&#160;</td>
          <td class="paramname"><em>input1_data</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const int8_t *&#160;</td>
          <td class="paramname"><em>input2_data</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">int8_t *&#160;</td>
          <td class="paramname"><em>output_data</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">inline</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p class="definition">Definition at line <a class="el" href="optimized_2_binary_arithmetic_ops_8h_source.html#l00322">322</a> of file <a class="el" href="optimized_2_binary_arithmetic_ops_8h_source.html">BinaryArithmeticOps.h</a>.</p>
<div class="fragment"><div class="line"><span class="lineno">  325</span>{</div>
<div class="line"><span class="lineno">  326</span>  <span class="keywordtype">int</span> i = 0;</div>
<div class="line"><span class="lineno">  327</span><span class="preprocessor">#ifdef USE_NEON</span></div>
<div class="line"><span class="lineno">  328</span>  <span class="keyword">const</span> int8x16_t output_activation_min_vector = vdupq_n_s8(params.<a class="code hl_variable" href="structnnfw_1_1cker_1_1_binary_arithmetic_op_param.html#abb8ef3401fcae98626f1da50cda22b2f">quantized_activation_min</a>);</div>
<div class="line"><span class="lineno">  329</span>  <span class="keyword">const</span> int8x16_t output_activation_max_vector = vdupq_n_s8(params.<a class="code hl_variable" href="structnnfw_1_1cker_1_1_binary_arithmetic_op_param.html#a20d919b6418bf302a97df426260bdec0">quantized_activation_max</a>);</div>
<div class="line"><span class="lineno">  330</span> </div>
<div class="line"><span class="lineno">  331</span>  <span class="keyword">const</span> <span class="keywordtype">int</span> input1_left_shift = params.<a class="code hl_variable" href="structnnfw_1_1cker_1_1_binary_arithmetic_op_param.html#ae5b66510e715c8fb127887931de98dd9">left_shift</a> + params.<a class="code hl_variable" href="structnnfw_1_1cker_1_1_binary_arithmetic_op_param.html#a473732b2b25fa319a787fd22411f83b8">input1_shift</a>;</div>
<div class="line"><span class="lineno">  332</span>  <span class="keyword">const</span> <span class="keywordtype">int</span> input2_left_shift = params.<a class="code hl_variable" href="structnnfw_1_1cker_1_1_binary_arithmetic_op_param.html#ae5b66510e715c8fb127887931de98dd9">left_shift</a> + params.<a class="code hl_variable" href="structnnfw_1_1cker_1_1_binary_arithmetic_op_param.html#aaf1857ba76a406a592402a13607421a5">input2_shift</a>;</div>
<div class="line"><span class="lineno">  333</span>  <span class="keyword">const</span> int32x4_t input1_left_dup = vdupq_n_s32(input1_left_shift);</div>
<div class="line"><span class="lineno">  334</span>  <span class="keyword">const</span> int32x4_t input2_left_dup = vdupq_n_s32(input2_left_shift);</div>
<div class="line"><span class="lineno">  335</span> </div>
<div class="line"><span class="lineno">  336</span>  <span class="keyword">const</span> int16x8_t input1_offset_dup = vdupq_n_s16(params.<a class="code hl_variable" href="structnnfw_1_1cker_1_1_binary_arithmetic_op_param.html#a553e59d259a380f2da048553a23af0b0">input1_offset</a>);</div>
<div class="line"><span class="lineno">  337</span>  <span class="keyword">const</span> int16x8_t input2_offset_dup = vdupq_n_s16(params.<a class="code hl_variable" href="structnnfw_1_1cker_1_1_binary_arithmetic_op_param.html#ac4d8a22f96208f1efd75047318475f03">input2_offset</a>);</div>
<div class="line"><span class="lineno">  338</span> </div>
<div class="line"><span class="lineno">  339</span>  <span class="keywordflow">for</span> (; i &lt;= size - 16; i += 16)</div>
<div class="line"><span class="lineno">  340</span>  {</div>
<div class="line"><span class="lineno">  341</span>    <span class="keyword">const</span> int8x16_t input1_val_original = vld1q_s8(input1_data + i);</div>
<div class="line"><span class="lineno">  342</span>    <span class="keyword">const</span> int8x16_t input2_val_original = vld1q_s8(input2_data + i);</div>
<div class="line"><span class="lineno">  343</span> </div>
<div class="line"><span class="lineno">  344</span>    <span class="keyword">const</span> int16x8_t input1_val_s16_high = vmovl_s8(vget_high_s8(input1_val_original));</div>
<div class="line"><span class="lineno">  345</span>    <span class="keyword">const</span> int16x8_t input1_val_s16_low = vmovl_s8(vget_low_s8(input1_val_original));</div>
<div class="line"><span class="lineno">  346</span> </div>
<div class="line"><span class="lineno">  347</span>    <span class="keyword">const</span> int16x8_t input2_val_s16_high = vmovl_s8(vget_high_s8(input2_val_original));</div>
<div class="line"><span class="lineno">  348</span>    <span class="keyword">const</span> int16x8_t input2_val_s16_low = vmovl_s8(vget_low_s8(input2_val_original));</div>
<div class="line"><span class="lineno">  349</span>    <span class="keyword">const</span> int16x8_t input1_val_high = vaddq_s16(input1_val_s16_high, input1_offset_dup);</div>
<div class="line"><span class="lineno">  350</span>    <span class="keyword">const</span> int16x8_t input2_val_high = vaddq_s16(input2_val_s16_high, input2_offset_dup);</div>
<div class="line"><span class="lineno">  351</span>    <span class="keyword">const</span> int16x8_t input1_val_low = vaddq_s16(input1_val_s16_low, input1_offset_dup);</div>
<div class="line"><span class="lineno">  352</span>    <span class="keyword">const</span> int16x8_t input2_val_low = vaddq_s16(input2_val_s16_low, input2_offset_dup);</div>
<div class="line"><span class="lineno">  353</span>    <span class="keyword">const</span> int16x4_t input1_val_high_high = vget_high_s16(input1_val_high);</div>
<div class="line"><span class="lineno">  354</span>    <span class="keyword">const</span> int16x4_t input1_val_high_low = vget_low_s16(input1_val_high);</div>
<div class="line"><span class="lineno">  355</span>    <span class="keyword">const</span> int16x4_t input1_val_low_high = vget_high_s16(input1_val_low);</div>
<div class="line"><span class="lineno">  356</span>    <span class="keyword">const</span> int16x4_t input1_val_low_low = vget_low_s16(input1_val_low);</div>
<div class="line"><span class="lineno">  357</span>    <span class="keyword">const</span> int16x4_t input2_val_high_high = vget_high_s16(input2_val_high);</div>
<div class="line"><span class="lineno">  358</span>    <span class="keyword">const</span> int16x4_t input2_val_high_low = vget_low_s16(input2_val_high);</div>
<div class="line"><span class="lineno">  359</span>    <span class="keyword">const</span> int16x4_t input2_val_low_high = vget_high_s16(input2_val_low);</div>
<div class="line"><span class="lineno">  360</span>    <span class="keyword">const</span> int16x4_t input2_val_low_low = vget_low_s16(input2_val_low);</div>
<div class="line"><span class="lineno">  361</span>    int32x4_t x111 = vmovl_s16(input1_val_low_low);</div>
<div class="line"><span class="lineno">  362</span>    int32x4_t x112 = vmovl_s16(input1_val_low_high);</div>
<div class="line"><span class="lineno">  363</span>    int32x4_t x121 = vmovl_s16(input1_val_high_low);</div>
<div class="line"><span class="lineno">  364</span>    int32x4_t x122 = vmovl_s16(input1_val_high_high);</div>
<div class="line"><span class="lineno">  365</span>    int32x4_t x211 = vmovl_s16(input2_val_low_low);</div>
<div class="line"><span class="lineno">  366</span>    int32x4_t x212 = vmovl_s16(input2_val_low_high);</div>
<div class="line"><span class="lineno">  367</span>    int32x4_t x221 = vmovl_s16(input2_val_high_low);</div>
<div class="line"><span class="lineno">  368</span>    int32x4_t x222 = vmovl_s16(input2_val_high_high);</div>
<div class="line"><span class="lineno">  369</span> </div>
<div class="line"><span class="lineno">  370</span>    x111 = vshlq_s32(x111, input1_left_dup);</div>
<div class="line"><span class="lineno">  371</span>    x112 = vshlq_s32(x112, input1_left_dup);</div>
<div class="line"><span class="lineno">  372</span>    x121 = vshlq_s32(x121, input1_left_dup);</div>
<div class="line"><span class="lineno">  373</span>    x122 = vshlq_s32(x122, input1_left_dup);</div>
<div class="line"><span class="lineno">  374</span>    x211 = vshlq_s32(x211, input2_left_dup);</div>
<div class="line"><span class="lineno">  375</span>    x212 = vshlq_s32(x212, input2_left_dup);</div>
<div class="line"><span class="lineno">  376</span>    x221 = vshlq_s32(x221, input2_left_dup);</div>
<div class="line"><span class="lineno">  377</span>    x222 = vshlq_s32(x222, input2_left_dup);</div>
<div class="line"><span class="lineno">  378</span>    x111 = vqrdmulhq_n_s32(x111, params.<a class="code hl_variable" href="structnnfw_1_1cker_1_1_binary_arithmetic_op_param.html#ae26b1a5367ed8036f623f4116d22b703">input1_multiplier</a>);</div>
<div class="line"><span class="lineno">  379</span>    x112 = vqrdmulhq_n_s32(x112, params.<a class="code hl_variable" href="structnnfw_1_1cker_1_1_binary_arithmetic_op_param.html#ae26b1a5367ed8036f623f4116d22b703">input1_multiplier</a>);</div>
<div class="line"><span class="lineno">  380</span>    x121 = vqrdmulhq_n_s32(x121, params.<a class="code hl_variable" href="structnnfw_1_1cker_1_1_binary_arithmetic_op_param.html#ae26b1a5367ed8036f623f4116d22b703">input1_multiplier</a>);</div>
<div class="line"><span class="lineno">  381</span>    x122 = vqrdmulhq_n_s32(x122, params.<a class="code hl_variable" href="structnnfw_1_1cker_1_1_binary_arithmetic_op_param.html#ae26b1a5367ed8036f623f4116d22b703">input1_multiplier</a>);</div>
<div class="line"><span class="lineno">  382</span>    x211 = vqrdmulhq_n_s32(x211, params.<a class="code hl_variable" href="structnnfw_1_1cker_1_1_binary_arithmetic_op_param.html#ab6b610a4f597712f6dcaf6576c04dc66">input2_multiplier</a>);</div>
<div class="line"><span class="lineno">  383</span>    x212 = vqrdmulhq_n_s32(x212, params.<a class="code hl_variable" href="structnnfw_1_1cker_1_1_binary_arithmetic_op_param.html#ab6b610a4f597712f6dcaf6576c04dc66">input2_multiplier</a>);</div>
<div class="line"><span class="lineno">  384</span>    x221 = vqrdmulhq_n_s32(x221, params.<a class="code hl_variable" href="structnnfw_1_1cker_1_1_binary_arithmetic_op_param.html#ab6b610a4f597712f6dcaf6576c04dc66">input2_multiplier</a>);</div>
<div class="line"><span class="lineno">  385</span>    x222 = vqrdmulhq_n_s32(x222, params.<a class="code hl_variable" href="structnnfw_1_1cker_1_1_binary_arithmetic_op_param.html#ab6b610a4f597712f6dcaf6576c04dc66">input2_multiplier</a>);</div>
<div class="line"><span class="lineno">  386</span>    int32x4_t s11 = vaddq_s32(x111, x211);</div>
<div class="line"><span class="lineno">  387</span>    int32x4_t s12 = vaddq_s32(x112, x212);</div>
<div class="line"><span class="lineno">  388</span>    int32x4_t s21 = vaddq_s32(x121, x221);</div>
<div class="line"><span class="lineno">  389</span>    int32x4_t s22 = vaddq_s32(x122, x222);</div>
<div class="line"><span class="lineno">  390</span>    s11 = vqrdmulhq_n_s32(s11, params.<a class="code hl_variable" href="structnnfw_1_1cker_1_1_binary_arithmetic_op_param.html#adae6f3cd4f5808bde90f4dfd3d915b4c">output_multiplier</a>);</div>
<div class="line"><span class="lineno">  391</span>    s12 = vqrdmulhq_n_s32(s12, params.<a class="code hl_variable" href="structnnfw_1_1cker_1_1_binary_arithmetic_op_param.html#adae6f3cd4f5808bde90f4dfd3d915b4c">output_multiplier</a>);</div>
<div class="line"><span class="lineno">  392</span>    s21 = vqrdmulhq_n_s32(s21, params.<a class="code hl_variable" href="structnnfw_1_1cker_1_1_binary_arithmetic_op_param.html#adae6f3cd4f5808bde90f4dfd3d915b4c">output_multiplier</a>);</div>
<div class="line"><span class="lineno">  393</span>    s22 = vqrdmulhq_n_s32(s22, params.<a class="code hl_variable" href="structnnfw_1_1cker_1_1_binary_arithmetic_op_param.html#adae6f3cd4f5808bde90f4dfd3d915b4c">output_multiplier</a>);</div>
<div class="line"><span class="lineno">  394</span>    <span class="keyword">using </span>gemmlowp::RoundingDivideByPOT;</div>
<div class="line"><span class="lineno">  395</span>    s11 = RoundingDivideByPOT(s11, -params.<a class="code hl_variable" href="structnnfw_1_1cker_1_1_binary_arithmetic_op_param.html#a1c0adc1575a15790a35479c5fa5093a4">output_shift</a>);</div>
<div class="line"><span class="lineno">  396</span>    s12 = RoundingDivideByPOT(s12, -params.<a class="code hl_variable" href="structnnfw_1_1cker_1_1_binary_arithmetic_op_param.html#a1c0adc1575a15790a35479c5fa5093a4">output_shift</a>);</div>
<div class="line"><span class="lineno">  397</span>    s21 = RoundingDivideByPOT(s21, -params.<a class="code hl_variable" href="structnnfw_1_1cker_1_1_binary_arithmetic_op_param.html#a1c0adc1575a15790a35479c5fa5093a4">output_shift</a>);</div>
<div class="line"><span class="lineno">  398</span>    s22 = RoundingDivideByPOT(s22, -params.<a class="code hl_variable" href="structnnfw_1_1cker_1_1_binary_arithmetic_op_param.html#a1c0adc1575a15790a35479c5fa5093a4">output_shift</a>);</div>
<div class="line"><span class="lineno">  399</span>    <span class="keyword">const</span> int16x4_t s11_narrowed = vmovn_s32(s11);</div>
<div class="line"><span class="lineno">  400</span>    <span class="keyword">const</span> int16x4_t s12_narrowed = vmovn_s32(s12);</div>
<div class="line"><span class="lineno">  401</span>    <span class="keyword">const</span> int16x4_t s21_narrowed = vmovn_s32(s21);</div>
<div class="line"><span class="lineno">  402</span>    <span class="keyword">const</span> int16x4_t s22_narrowed = vmovn_s32(s22);</div>
<div class="line"><span class="lineno">  403</span>    <span class="keyword">const</span> int16x8_t s1 =</div>
<div class="line"><span class="lineno">  404</span>      vaddq_s16(vcombine_s16(s11_narrowed, s12_narrowed), vdupq_n_s16(params.<a class="code hl_variable" href="structnnfw_1_1cker_1_1_binary_arithmetic_op_param.html#aa5bf6b7c6e3772bd04264f6729915729">output_offset</a>));</div>
<div class="line"><span class="lineno">  405</span>    <span class="keyword">const</span> int16x8_t s2 =</div>
<div class="line"><span class="lineno">  406</span>      vaddq_s16(vcombine_s16(s21_narrowed, s22_narrowed), vdupq_n_s16(params.<a class="code hl_variable" href="structnnfw_1_1cker_1_1_binary_arithmetic_op_param.html#aa5bf6b7c6e3772bd04264f6729915729">output_offset</a>));</div>
<div class="line"><span class="lineno">  407</span>    <span class="keyword">const</span> int8x16_t s = vcombine_s8(vqmovn_s16(s1), vqmovn_s16(s2));</div>
<div class="line"><span class="lineno">  408</span> </div>
<div class="line"><span class="lineno">  409</span>    <span class="keyword">const</span> int8x16_t clamped =</div>
<div class="line"><span class="lineno">  410</span>      vmaxq_s8(output_activation_min_vector, vminq_s8(output_activation_max_vector, s));</div>
<div class="line"><span class="lineno">  411</span>    vst1q_s8(output_data + i, clamped);</div>
<div class="line"><span class="lineno">  412</span>  }</div>
<div class="line"><span class="lineno">  413</span><span class="preprocessor">#endif </span><span class="comment">// NEON</span></div>
<div class="line"><span class="lineno">  414</span> </div>
<div class="line"><span class="lineno">  415</span>  <span class="keywordflow">for</span> (; i &lt; size; ++i)</div>
<div class="line"><span class="lineno">  416</span>  {</div>
<div class="line"><span class="lineno">  417</span>    <span class="keyword">const</span> int32_t input1_val = params.<a class="code hl_variable" href="structnnfw_1_1cker_1_1_binary_arithmetic_op_param.html#a553e59d259a380f2da048553a23af0b0">input1_offset</a> + input1_data[i];</div>
<div class="line"><span class="lineno">  418</span>    <span class="keyword">const</span> int32_t input2_val = params.<a class="code hl_variable" href="structnnfw_1_1cker_1_1_binary_arithmetic_op_param.html#ac4d8a22f96208f1efd75047318475f03">input2_offset</a> + input2_data[i];</div>
<div class="line"><span class="lineno">  419</span>    <span class="keyword">const</span> int32_t shifted_input1_val = input1_val * (1 &lt;&lt; params.<a class="code hl_variable" href="structnnfw_1_1cker_1_1_binary_arithmetic_op_param.html#ae5b66510e715c8fb127887931de98dd9">left_shift</a>);</div>
<div class="line"><span class="lineno">  420</span>    <span class="keyword">const</span> int32_t shifted_input2_val = input2_val * (1 &lt;&lt; params.<a class="code hl_variable" href="structnnfw_1_1cker_1_1_binary_arithmetic_op_param.html#ae5b66510e715c8fb127887931de98dd9">left_shift</a>);</div>
<div class="line"><span class="lineno">  421</span>    <span class="keyword">const</span> int32_t scaled_input1_val = <a class="code hl_function" href="namespacennfw_1_1cker.html#afe95d2a91cb367a4b9be585d4ff4b90f">MultiplyByQuantizedMultiplierSmallerThanOneExp</a>(</div>
<div class="line"><span class="lineno">  422</span>      shifted_input1_val, params.<a class="code hl_variable" href="structnnfw_1_1cker_1_1_binary_arithmetic_op_param.html#ae26b1a5367ed8036f623f4116d22b703">input1_multiplier</a>, params.<a class="code hl_variable" href="structnnfw_1_1cker_1_1_binary_arithmetic_op_param.html#a473732b2b25fa319a787fd22411f83b8">input1_shift</a>);</div>
<div class="line"><span class="lineno">  423</span>    <span class="keyword">const</span> int32_t scaled_input2_val = <a class="code hl_function" href="namespacennfw_1_1cker.html#afe95d2a91cb367a4b9be585d4ff4b90f">MultiplyByQuantizedMultiplierSmallerThanOneExp</a>(</div>
<div class="line"><span class="lineno">  424</span>      shifted_input2_val, params.<a class="code hl_variable" href="structnnfw_1_1cker_1_1_binary_arithmetic_op_param.html#ab6b610a4f597712f6dcaf6576c04dc66">input2_multiplier</a>, params.<a class="code hl_variable" href="structnnfw_1_1cker_1_1_binary_arithmetic_op_param.html#aaf1857ba76a406a592402a13607421a5">input2_shift</a>);</div>
<div class="line"><span class="lineno">  425</span>    <span class="keyword">const</span> int32_t raw_sum = scaled_input1_val + scaled_input2_val;</div>
<div class="line"><span class="lineno">  426</span>    <span class="keyword">const</span> int32_t raw_output = <a class="code hl_function" href="namespacennfw_1_1cker.html#afe95d2a91cb367a4b9be585d4ff4b90f">MultiplyByQuantizedMultiplierSmallerThanOneExp</a>(</div>
<div class="line"><span class="lineno">  427</span>                                 raw_sum, params.<a class="code hl_variable" href="structnnfw_1_1cker_1_1_binary_arithmetic_op_param.html#adae6f3cd4f5808bde90f4dfd3d915b4c">output_multiplier</a>, params.<a class="code hl_variable" href="structnnfw_1_1cker_1_1_binary_arithmetic_op_param.html#a1c0adc1575a15790a35479c5fa5093a4">output_shift</a>) +</div>
<div class="line"><span class="lineno">  428</span>                               params.<a class="code hl_variable" href="structnnfw_1_1cker_1_1_binary_arithmetic_op_param.html#aa5bf6b7c6e3772bd04264f6729915729">output_offset</a>;</div>
<div class="line"><span class="lineno">  429</span>    <span class="keyword">const</span> int32_t clamped_output = std::min(params.<a class="code hl_variable" href="structnnfw_1_1cker_1_1_binary_arithmetic_op_param.html#a20d919b6418bf302a97df426260bdec0">quantized_activation_max</a>,</div>
<div class="line"><span class="lineno">  430</span>                                            std::max(params.<a class="code hl_variable" href="structnnfw_1_1cker_1_1_binary_arithmetic_op_param.html#abb8ef3401fcae98626f1da50cda22b2f">quantized_activation_min</a>, raw_output));</div>
<div class="line"><span class="lineno">  431</span>    output_data[i] = <span class="keyword">static_cast&lt;</span>int8_t<span class="keyword">&gt;</span>(clamped_output);</div>
<div class="line"><span class="lineno">  432</span>  }</div>
<div class="line"><span class="lineno">  433</span>}</div>
<div class="ttc" id="anamespacennfw_1_1cker_html_afe95d2a91cb367a4b9be585d4ff4b90f"><div class="ttname"><a href="namespacennfw_1_1cker.html#afe95d2a91cb367a4b9be585d4ff4b90f">nnfw::cker::MultiplyByQuantizedMultiplierSmallerThanOneExp</a></div><div class="ttdeci">int32_t MultiplyByQuantizedMultiplierSmallerThanOneExp(int32_t x, int32_t quantized_multiplier, int left_shift)</div><div class="ttdef"><b>Definition:</b> <a href="compute_2cker_2include_2cker_2_utils_8h_source.html#l00108">Utils.h:108</a></div></div>
<div class="ttc" id="astructnnfw_1_1cker_1_1_binary_arithmetic_op_param_html_a1c0adc1575a15790a35479c5fa5093a4"><div class="ttname"><a href="structnnfw_1_1cker_1_1_binary_arithmetic_op_param.html#a1c0adc1575a15790a35479c5fa5093a4">nnfw::cker::BinaryArithmeticOpParam::output_shift</a></div><div class="ttdeci">int32_t output_shift</div><div class="ttdef"><b>Definition:</b> <a href="compute_2cker_2include_2cker_2_types_8h_source.html#l00181">Types.h:181</a></div></div>
<div class="ttc" id="astructnnfw_1_1cker_1_1_binary_arithmetic_op_param_html_a20d919b6418bf302a97df426260bdec0"><div class="ttname"><a href="structnnfw_1_1cker_1_1_binary_arithmetic_op_param.html#a20d919b6418bf302a97df426260bdec0">nnfw::cker::BinaryArithmeticOpParam::quantized_activation_max</a></div><div class="ttdeci">int32_t quantized_activation_max</div><div class="ttdef"><b>Definition:</b> <a href="compute_2cker_2include_2cker_2_types_8h_source.html#l00190">Types.h:190</a></div></div>
<div class="ttc" id="astructnnfw_1_1cker_1_1_binary_arithmetic_op_param_html_a473732b2b25fa319a787fd22411f83b8"><div class="ttname"><a href="structnnfw_1_1cker_1_1_binary_arithmetic_op_param.html#a473732b2b25fa319a787fd22411f83b8">nnfw::cker::BinaryArithmeticOpParam::input1_shift</a></div><div class="ttdeci">int32_t input1_shift</div><div class="ttdef"><b>Definition:</b> <a href="compute_2cker_2include_2cker_2_types_8h_source.html#l00185">Types.h:185</a></div></div>
<div class="ttc" id="astructnnfw_1_1cker_1_1_binary_arithmetic_op_param_html_a553e59d259a380f2da048553a23af0b0"><div class="ttname"><a href="structnnfw_1_1cker_1_1_binary_arithmetic_op_param.html#a553e59d259a380f2da048553a23af0b0">nnfw::cker::BinaryArithmeticOpParam::input1_offset</a></div><div class="ttdeci">int32_t input1_offset</div><div class="ttdef"><b>Definition:</b> <a href="compute_2cker_2include_2cker_2_types_8h_source.html#l00177">Types.h:177</a></div></div>
<div class="ttc" id="astructnnfw_1_1cker_1_1_binary_arithmetic_op_param_html_aa5bf6b7c6e3772bd04264f6729915729"><div class="ttname"><a href="structnnfw_1_1cker_1_1_binary_arithmetic_op_param.html#aa5bf6b7c6e3772bd04264f6729915729">nnfw::cker::BinaryArithmeticOpParam::output_offset</a></div><div class="ttdeci">int32_t output_offset</div><div class="ttdef"><b>Definition:</b> <a href="compute_2cker_2include_2cker_2_types_8h_source.html#l00179">Types.h:179</a></div></div>
<div class="ttc" id="astructnnfw_1_1cker_1_1_binary_arithmetic_op_param_html_aaf1857ba76a406a592402a13607421a5"><div class="ttname"><a href="structnnfw_1_1cker_1_1_binary_arithmetic_op_param.html#aaf1857ba76a406a592402a13607421a5">nnfw::cker::BinaryArithmeticOpParam::input2_shift</a></div><div class="ttdeci">int32_t input2_shift</div><div class="ttdef"><b>Definition:</b> <a href="compute_2cker_2include_2cker_2_types_8h_source.html#l00187">Types.h:187</a></div></div>
<div class="ttc" id="astructnnfw_1_1cker_1_1_binary_arithmetic_op_param_html_ab6b610a4f597712f6dcaf6576c04dc66"><div class="ttname"><a href="structnnfw_1_1cker_1_1_binary_arithmetic_op_param.html#ab6b610a4f597712f6dcaf6576c04dc66">nnfw::cker::BinaryArithmeticOpParam::input2_multiplier</a></div><div class="ttdeci">int32_t input2_multiplier</div><div class="ttdef"><b>Definition:</b> <a href="compute_2cker_2include_2cker_2_types_8h_source.html#l00186">Types.h:186</a></div></div>
<div class="ttc" id="astructnnfw_1_1cker_1_1_binary_arithmetic_op_param_html_abb8ef3401fcae98626f1da50cda22b2f"><div class="ttname"><a href="structnnfw_1_1cker_1_1_binary_arithmetic_op_param.html#abb8ef3401fcae98626f1da50cda22b2f">nnfw::cker::BinaryArithmeticOpParam::quantized_activation_min</a></div><div class="ttdeci">int32_t quantized_activation_min</div><div class="ttdef"><b>Definition:</b> <a href="compute_2cker_2include_2cker_2_types_8h_source.html#l00189">Types.h:189</a></div></div>
<div class="ttc" id="astructnnfw_1_1cker_1_1_binary_arithmetic_op_param_html_ac4d8a22f96208f1efd75047318475f03"><div class="ttname"><a href="structnnfw_1_1cker_1_1_binary_arithmetic_op_param.html#ac4d8a22f96208f1efd75047318475f03">nnfw::cker::BinaryArithmeticOpParam::input2_offset</a></div><div class="ttdeci">int32_t input2_offset</div><div class="ttdef"><b>Definition:</b> <a href="compute_2cker_2include_2cker_2_types_8h_source.html#l00178">Types.h:178</a></div></div>
<div class="ttc" id="astructnnfw_1_1cker_1_1_binary_arithmetic_op_param_html_adae6f3cd4f5808bde90f4dfd3d915b4c"><div class="ttname"><a href="structnnfw_1_1cker_1_1_binary_arithmetic_op_param.html#adae6f3cd4f5808bde90f4dfd3d915b4c">nnfw::cker::BinaryArithmeticOpParam::output_multiplier</a></div><div class="ttdeci">int32_t output_multiplier</div><div class="ttdef"><b>Definition:</b> <a href="compute_2cker_2include_2cker_2_types_8h_source.html#l00180">Types.h:180</a></div></div>
<div class="ttc" id="astructnnfw_1_1cker_1_1_binary_arithmetic_op_param_html_ae26b1a5367ed8036f623f4116d22b703"><div class="ttname"><a href="structnnfw_1_1cker_1_1_binary_arithmetic_op_param.html#ae26b1a5367ed8036f623f4116d22b703">nnfw::cker::BinaryArithmeticOpParam::input1_multiplier</a></div><div class="ttdeci">int32_t input1_multiplier</div><div class="ttdef"><b>Definition:</b> <a href="compute_2cker_2include_2cker_2_types_8h_source.html#l00184">Types.h:184</a></div></div>
<div class="ttc" id="astructnnfw_1_1cker_1_1_binary_arithmetic_op_param_html_ae5b66510e715c8fb127887931de98dd9"><div class="ttname"><a href="structnnfw_1_1cker_1_1_binary_arithmetic_op_param.html#ae5b66510e715c8fb127887931de98dd9">nnfw::cker::BinaryArithmeticOpParam::left_shift</a></div><div class="ttdeci">int32_t left_shift</div><div class="ttdef"><b>Definition:</b> <a href="compute_2cker_2include_2cker_2_types_8h_source.html#l00183">Types.h:183</a></div></div>
</div><!-- fragment -->
<p class="reference">References <a class="el" href="compute_2cker_2include_2cker_2_types_8h_source.html#l00184">nnfw::cker::BinaryArithmeticOpParam::input1_multiplier</a>, <a class="el" href="compute_2cker_2include_2cker_2_types_8h_source.html#l00177">nnfw::cker::BinaryArithmeticOpParam::input1_offset</a>, <a class="el" href="compute_2cker_2include_2cker_2_types_8h_source.html#l00185">nnfw::cker::BinaryArithmeticOpParam::input1_shift</a>, <a class="el" href="compute_2cker_2include_2cker_2_types_8h_source.html#l00186">nnfw::cker::BinaryArithmeticOpParam::input2_multiplier</a>, <a class="el" href="compute_2cker_2include_2cker_2_types_8h_source.html#l00178">nnfw::cker::BinaryArithmeticOpParam::input2_offset</a>, <a class="el" href="compute_2cker_2include_2cker_2_types_8h_source.html#l00187">nnfw::cker::BinaryArithmeticOpParam::input2_shift</a>, <a class="el" href="compute_2cker_2include_2cker_2_types_8h_source.html#l00183">nnfw::cker::BinaryArithmeticOpParam::left_shift</a>, <a class="el" href="compute_2cker_2include_2cker_2_utils_8h_source.html#l00108">nnfw::cker::MultiplyByQuantizedMultiplierSmallerThanOneExp()</a>, <a class="el" href="compute_2cker_2include_2cker_2_types_8h_source.html#l00180">nnfw::cker::BinaryArithmeticOpParam::output_multiplier</a>, <a class="el" href="compute_2cker_2include_2cker_2_types_8h_source.html#l00179">nnfw::cker::BinaryArithmeticOpParam::output_offset</a>, <a class="el" href="compute_2cker_2include_2cker_2_types_8h_source.html#l00181">nnfw::cker::BinaryArithmeticOpParam::output_shift</a>, <a class="el" href="compute_2cker_2include_2cker_2_types_8h_source.html#l00190">nnfw::cker::BinaryArithmeticOpParam::quantized_activation_max</a>, and <a class="el" href="compute_2cker_2include_2cker_2_types_8h_source.html#l00189">nnfw::cker::BinaryArithmeticOpParam::quantized_activation_min</a>.</p>

</div>
</div>
<a id="a210f92581f9a80300d9eb4dd4f6957c4" name="a210f92581f9a80300d9eb4dd4f6957c4"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a210f92581f9a80300d9eb4dd4f6957c4">&#9670;&#160;</a></span>AddElementwise() <span class="overload">[2/2]</span></h2>

<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">void nnfw::cker::optimized::AddElementwise </td>
          <td>(</td>
          <td class="paramtype">int&#160;</td>
          <td class="paramname"><em>size</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const <a class="el" href="structnnfw_1_1cker_1_1_binary_arithmetic_op_param.html">BinaryArithmeticOpParam</a> &amp;&#160;</td>
          <td class="paramname"><em>params</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const uint8_t *&#160;</td>
          <td class="paramname"><em>input1_data</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const uint8_t *&#160;</td>
          <td class="paramname"><em>input2_data</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">uint8_t *&#160;</td>
          <td class="paramname"><em>output_data</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">inline</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p class="definition">Definition at line <a class="el" href="optimized_2_binary_arithmetic_ops_8h_source.html#l00246">246</a> of file <a class="el" href="optimized_2_binary_arithmetic_ops_8h_source.html">BinaryArithmeticOps.h</a>.</p>
<div class="fragment"><div class="line"><span class="lineno">  249</span>{</div>
<div class="line"><span class="lineno">  250</span>  <span class="keywordtype">int</span> i = 0;</div>
<div class="line"><span class="lineno">  251</span> </div>
<div class="line"><span class="lineno">  252</span><span class="preprocessor">#ifdef USE_NEON</span></div>
<div class="line"><span class="lineno">  253</span>  <span class="keyword">const</span> uint8x8_t output_activation_min_vector = vdup_n_u8(params.<a class="code hl_variable" href="structnnfw_1_1cker_1_1_binary_arithmetic_op_param.html#abb8ef3401fcae98626f1da50cda22b2f">quantized_activation_min</a>);</div>
<div class="line"><span class="lineno">  254</span>  <span class="keyword">const</span> uint8x8_t output_activation_max_vector = vdup_n_u8(params.<a class="code hl_variable" href="structnnfw_1_1cker_1_1_binary_arithmetic_op_param.html#a20d919b6418bf302a97df426260bdec0">quantized_activation_max</a>);</div>
<div class="line"><span class="lineno">  255</span>  <span class="keywordflow">for</span> (; i &lt;= size - 8; i += 8)</div>
<div class="line"><span class="lineno">  256</span>  {</div>
<div class="line"><span class="lineno">  257</span>    <span class="keyword">const</span> uint8x8_t input1_val_original = vld1_u8(input1_data + i);</div>
<div class="line"><span class="lineno">  258</span>    <span class="keyword">const</span> uint8x8_t input2_val_original = vld1_u8(input2_data + i);</div>
<div class="line"><span class="lineno">  259</span>    <span class="keyword">const</span> int16x8_t input1_val_s16 = vreinterpretq_s16_u16(vmovl_u8(input1_val_original));</div>
<div class="line"><span class="lineno">  260</span>    <span class="keyword">const</span> int16x8_t input2_val_s16 = vreinterpretq_s16_u16(vmovl_u8(input2_val_original));</div>
<div class="line"><span class="lineno">  261</span>    <span class="keyword">const</span> int16x8_t input1_val = vaddq_s16(input1_val_s16, vdupq_n_s16(params.<a class="code hl_variable" href="structnnfw_1_1cker_1_1_binary_arithmetic_op_param.html#a553e59d259a380f2da048553a23af0b0">input1_offset</a>));</div>
<div class="line"><span class="lineno">  262</span>    <span class="keyword">const</span> int16x8_t input2_val = vaddq_s16(input2_val_s16, vdupq_n_s16(params.<a class="code hl_variable" href="structnnfw_1_1cker_1_1_binary_arithmetic_op_param.html#ac4d8a22f96208f1efd75047318475f03">input2_offset</a>));</div>
<div class="line"><span class="lineno">  263</span>    <span class="keyword">const</span> int16x4_t input1_val_high = vget_high_s16(input1_val);</div>
<div class="line"><span class="lineno">  264</span>    <span class="keyword">const</span> int16x4_t input1_val_low = vget_low_s16(input1_val);</div>
<div class="line"><span class="lineno">  265</span>    <span class="keyword">const</span> int16x4_t input2_val_high = vget_high_s16(input2_val);</div>
<div class="line"><span class="lineno">  266</span>    <span class="keyword">const</span> int16x4_t input2_val_low = vget_low_s16(input2_val);</div>
<div class="line"><span class="lineno">  267</span>    int32x4_t x11 = vmovl_s16(input1_val_low);</div>
<div class="line"><span class="lineno">  268</span>    int32x4_t x12 = vmovl_s16(input1_val_high);</div>
<div class="line"><span class="lineno">  269</span>    int32x4_t x21 = vmovl_s16(input2_val_low);</div>
<div class="line"><span class="lineno">  270</span>    int32x4_t x22 = vmovl_s16(input2_val_high);</div>
<div class="line"><span class="lineno">  271</span>    <span class="keyword">const</span> int32x4_t left_shift_dup = vdupq_n_s32(params.<a class="code hl_variable" href="structnnfw_1_1cker_1_1_binary_arithmetic_op_param.html#ae5b66510e715c8fb127887931de98dd9">left_shift</a>);</div>
<div class="line"><span class="lineno">  272</span>    x11 = vshlq_s32(x11, left_shift_dup);</div>
<div class="line"><span class="lineno">  273</span>    x12 = vshlq_s32(x12, left_shift_dup);</div>
<div class="line"><span class="lineno">  274</span>    x21 = vshlq_s32(x21, left_shift_dup);</div>
<div class="line"><span class="lineno">  275</span>    x22 = vshlq_s32(x22, left_shift_dup);</div>
<div class="line"><span class="lineno">  276</span>    x11 = vqrdmulhq_n_s32(x11, params.<a class="code hl_variable" href="structnnfw_1_1cker_1_1_binary_arithmetic_op_param.html#ae26b1a5367ed8036f623f4116d22b703">input1_multiplier</a>);</div>
<div class="line"><span class="lineno">  277</span>    x12 = vqrdmulhq_n_s32(x12, params.<a class="code hl_variable" href="structnnfw_1_1cker_1_1_binary_arithmetic_op_param.html#ae26b1a5367ed8036f623f4116d22b703">input1_multiplier</a>);</div>
<div class="line"><span class="lineno">  278</span>    x21 = vqrdmulhq_n_s32(x21, params.<a class="code hl_variable" href="structnnfw_1_1cker_1_1_binary_arithmetic_op_param.html#ab6b610a4f597712f6dcaf6576c04dc66">input2_multiplier</a>);</div>
<div class="line"><span class="lineno">  279</span>    x22 = vqrdmulhq_n_s32(x22, params.<a class="code hl_variable" href="structnnfw_1_1cker_1_1_binary_arithmetic_op_param.html#ab6b610a4f597712f6dcaf6576c04dc66">input2_multiplier</a>);</div>
<div class="line"><span class="lineno">  280</span>    <span class="keyword">const</span> int32x4_t input1_shift_dup = vdupq_n_s32(params.<a class="code hl_variable" href="structnnfw_1_1cker_1_1_binary_arithmetic_op_param.html#a473732b2b25fa319a787fd22411f83b8">input1_shift</a>);</div>
<div class="line"><span class="lineno">  281</span>    <span class="keyword">const</span> int32x4_t input2_shift_dup = vdupq_n_s32(params.<a class="code hl_variable" href="structnnfw_1_1cker_1_1_binary_arithmetic_op_param.html#aaf1857ba76a406a592402a13607421a5">input2_shift</a>);</div>
<div class="line"><span class="lineno">  282</span>    x11 = vshlq_s32(x11, input1_shift_dup);</div>
<div class="line"><span class="lineno">  283</span>    x12 = vshlq_s32(x12, input1_shift_dup);</div>
<div class="line"><span class="lineno">  284</span>    x21 = vshlq_s32(x21, input2_shift_dup);</div>
<div class="line"><span class="lineno">  285</span>    x22 = vshlq_s32(x22, input2_shift_dup);</div>
<div class="line"><span class="lineno">  286</span>    int32x4_t s1 = vaddq_s32(x11, x21);</div>
<div class="line"><span class="lineno">  287</span>    int32x4_t s2 = vaddq_s32(x12, x22);</div>
<div class="line"><span class="lineno">  288</span>    s1 = vqrdmulhq_n_s32(s1, params.<a class="code hl_variable" href="structnnfw_1_1cker_1_1_binary_arithmetic_op_param.html#adae6f3cd4f5808bde90f4dfd3d915b4c">output_multiplier</a>);</div>
<div class="line"><span class="lineno">  289</span>    s2 = vqrdmulhq_n_s32(s2, params.<a class="code hl_variable" href="structnnfw_1_1cker_1_1_binary_arithmetic_op_param.html#adae6f3cd4f5808bde90f4dfd3d915b4c">output_multiplier</a>);</div>
<div class="line"><span class="lineno">  290</span>    <span class="keyword">using </span>gemmlowp::RoundingDivideByPOT;</div>
<div class="line"><span class="lineno">  291</span>    s1 = RoundingDivideByPOT(s1, -params.<a class="code hl_variable" href="structnnfw_1_1cker_1_1_binary_arithmetic_op_param.html#a1c0adc1575a15790a35479c5fa5093a4">output_shift</a>);</div>
<div class="line"><span class="lineno">  292</span>    s2 = RoundingDivideByPOT(s2, -params.<a class="code hl_variable" href="structnnfw_1_1cker_1_1_binary_arithmetic_op_param.html#a1c0adc1575a15790a35479c5fa5093a4">output_shift</a>);</div>
<div class="line"><span class="lineno">  293</span>    <span class="keyword">const</span> int16x4_t s1_narrowed = vmovn_s32(s1);</div>
<div class="line"><span class="lineno">  294</span>    <span class="keyword">const</span> int16x4_t s2_narrowed = vmovn_s32(s2);</div>
<div class="line"><span class="lineno">  295</span>    <span class="keyword">const</span> int16x8_t s =</div>
<div class="line"><span class="lineno">  296</span>      vaddq_s16(vcombine_s16(s1_narrowed, s2_narrowed), vdupq_n_s16(params.<a class="code hl_variable" href="structnnfw_1_1cker_1_1_binary_arithmetic_op_param.html#aa5bf6b7c6e3772bd04264f6729915729">output_offset</a>));</div>
<div class="line"><span class="lineno">  297</span>    <span class="keyword">const</span> uint8x8_t clamped =</div>
<div class="line"><span class="lineno">  298</span>      vmax_u8(output_activation_min_vector, vmin_u8(output_activation_max_vector, vqmovun_s16(s)));</div>
<div class="line"><span class="lineno">  299</span>    vst1_u8(output_data + i, clamped);</div>
<div class="line"><span class="lineno">  300</span>  }</div>
<div class="line"><span class="lineno">  301</span><span class="preprocessor">#endif </span><span class="comment">// NEON</span></div>
<div class="line"><span class="lineno">  302</span>  <span class="keywordflow">for</span> (; i &lt; size; ++i)</div>
<div class="line"><span class="lineno">  303</span>  {</div>
<div class="line"><span class="lineno">  304</span>    <span class="keyword">const</span> int32_t input1_val = params.<a class="code hl_variable" href="structnnfw_1_1cker_1_1_binary_arithmetic_op_param.html#a553e59d259a380f2da048553a23af0b0">input1_offset</a> + input1_data[i];</div>
<div class="line"><span class="lineno">  305</span>    <span class="keyword">const</span> int32_t input2_val = params.<a class="code hl_variable" href="structnnfw_1_1cker_1_1_binary_arithmetic_op_param.html#ac4d8a22f96208f1efd75047318475f03">input2_offset</a> + input2_data[i];</div>
<div class="line"><span class="lineno">  306</span>    <span class="keyword">const</span> int32_t shifted_input1_val = input1_val * (1 &lt;&lt; params.<a class="code hl_variable" href="structnnfw_1_1cker_1_1_binary_arithmetic_op_param.html#ae5b66510e715c8fb127887931de98dd9">left_shift</a>);</div>
<div class="line"><span class="lineno">  307</span>    <span class="keyword">const</span> int32_t shifted_input2_val = input2_val * (1 &lt;&lt; params.<a class="code hl_variable" href="structnnfw_1_1cker_1_1_binary_arithmetic_op_param.html#ae5b66510e715c8fb127887931de98dd9">left_shift</a>);</div>
<div class="line"><span class="lineno">  308</span>    <span class="keyword">const</span> int32_t scaled_input1_val = <a class="code hl_function" href="namespacennfw_1_1cker.html#afe95d2a91cb367a4b9be585d4ff4b90f">MultiplyByQuantizedMultiplierSmallerThanOneExp</a>(</div>
<div class="line"><span class="lineno">  309</span>      shifted_input1_val, params.<a class="code hl_variable" href="structnnfw_1_1cker_1_1_binary_arithmetic_op_param.html#ae26b1a5367ed8036f623f4116d22b703">input1_multiplier</a>, params.<a class="code hl_variable" href="structnnfw_1_1cker_1_1_binary_arithmetic_op_param.html#a473732b2b25fa319a787fd22411f83b8">input1_shift</a>);</div>
<div class="line"><span class="lineno">  310</span>    <span class="keyword">const</span> int32_t scaled_input2_val = <a class="code hl_function" href="namespacennfw_1_1cker.html#afe95d2a91cb367a4b9be585d4ff4b90f">MultiplyByQuantizedMultiplierSmallerThanOneExp</a>(</div>
<div class="line"><span class="lineno">  311</span>      shifted_input2_val, params.<a class="code hl_variable" href="structnnfw_1_1cker_1_1_binary_arithmetic_op_param.html#ab6b610a4f597712f6dcaf6576c04dc66">input2_multiplier</a>, params.<a class="code hl_variable" href="structnnfw_1_1cker_1_1_binary_arithmetic_op_param.html#aaf1857ba76a406a592402a13607421a5">input2_shift</a>);</div>
<div class="line"><span class="lineno">  312</span>    <span class="keyword">const</span> int32_t raw_sum = scaled_input1_val + scaled_input2_val;</div>
<div class="line"><span class="lineno">  313</span>    <span class="keyword">const</span> int32_t raw_output = <a class="code hl_function" href="namespacennfw_1_1cker.html#afe95d2a91cb367a4b9be585d4ff4b90f">MultiplyByQuantizedMultiplierSmallerThanOneExp</a>(</div>
<div class="line"><span class="lineno">  314</span>                                 raw_sum, params.<a class="code hl_variable" href="structnnfw_1_1cker_1_1_binary_arithmetic_op_param.html#adae6f3cd4f5808bde90f4dfd3d915b4c">output_multiplier</a>, params.<a class="code hl_variable" href="structnnfw_1_1cker_1_1_binary_arithmetic_op_param.html#a1c0adc1575a15790a35479c5fa5093a4">output_shift</a>) +</div>
<div class="line"><span class="lineno">  315</span>                               params.<a class="code hl_variable" href="structnnfw_1_1cker_1_1_binary_arithmetic_op_param.html#aa5bf6b7c6e3772bd04264f6729915729">output_offset</a>;</div>
<div class="line"><span class="lineno">  316</span>    <span class="keyword">const</span> int32_t clamped_output = std::min(params.<a class="code hl_variable" href="structnnfw_1_1cker_1_1_binary_arithmetic_op_param.html#a20d919b6418bf302a97df426260bdec0">quantized_activation_max</a>,</div>
<div class="line"><span class="lineno">  317</span>                                            std::max(params.<a class="code hl_variable" href="structnnfw_1_1cker_1_1_binary_arithmetic_op_param.html#abb8ef3401fcae98626f1da50cda22b2f">quantized_activation_min</a>, raw_output));</div>
<div class="line"><span class="lineno">  318</span>    output_data[i] = <span class="keyword">static_cast&lt;</span>uint8_t<span class="keyword">&gt;</span>(clamped_output);</div>
<div class="line"><span class="lineno">  319</span>  }</div>
<div class="line"><span class="lineno">  320</span>}</div>
</div><!-- fragment -->
<p class="reference">References <a class="el" href="compute_2cker_2include_2cker_2_types_8h_source.html#l00184">nnfw::cker::BinaryArithmeticOpParam::input1_multiplier</a>, <a class="el" href="compute_2cker_2include_2cker_2_types_8h_source.html#l00177">nnfw::cker::BinaryArithmeticOpParam::input1_offset</a>, <a class="el" href="compute_2cker_2include_2cker_2_types_8h_source.html#l00185">nnfw::cker::BinaryArithmeticOpParam::input1_shift</a>, <a class="el" href="compute_2cker_2include_2cker_2_types_8h_source.html#l00186">nnfw::cker::BinaryArithmeticOpParam::input2_multiplier</a>, <a class="el" href="compute_2cker_2include_2cker_2_types_8h_source.html#l00178">nnfw::cker::BinaryArithmeticOpParam::input2_offset</a>, <a class="el" href="compute_2cker_2include_2cker_2_types_8h_source.html#l00187">nnfw::cker::BinaryArithmeticOpParam::input2_shift</a>, <a class="el" href="compute_2cker_2include_2cker_2_types_8h_source.html#l00183">nnfw::cker::BinaryArithmeticOpParam::left_shift</a>, <a class="el" href="compute_2cker_2include_2cker_2_utils_8h_source.html#l00108">nnfw::cker::MultiplyByQuantizedMultiplierSmallerThanOneExp()</a>, <a class="el" href="compute_2cker_2include_2cker_2_types_8h_source.html#l00180">nnfw::cker::BinaryArithmeticOpParam::output_multiplier</a>, <a class="el" href="compute_2cker_2include_2cker_2_types_8h_source.html#l00179">nnfw::cker::BinaryArithmeticOpParam::output_offset</a>, <a class="el" href="compute_2cker_2include_2cker_2_types_8h_source.html#l00181">nnfw::cker::BinaryArithmeticOpParam::output_shift</a>, <a class="el" href="compute_2cker_2include_2cker_2_types_8h_source.html#l00190">nnfw::cker::BinaryArithmeticOpParam::quantized_activation_max</a>, and <a class="el" href="compute_2cker_2include_2cker_2_types_8h_source.html#l00189">nnfw::cker::BinaryArithmeticOpParam::quantized_activation_min</a>.</p>

<p class="reference">Referenced by <a class="el" href="optimized_2_binary_arithmetic_ops_8h_source.html#l00692">Add()</a>, and <a class="el" href="optimized_2_binary_arithmetic_ops_8h_source.html#l00812">BroadcastAddDispatch()</a>.</p>

</div>
</div>
<a id="ac9c61e2ab25f05ac63b562a6f58b1b77" name="ac9c61e2ab25f05ac63b562a6f58b1b77"></a>
<h2 class="memtitle"><span class="permalink"><a href="#ac9c61e2ab25f05ac63b562a6f58b1b77">&#9670;&#160;</a></span>AddScalarBroadcast() <span class="overload">[1/2]</span></h2>

<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">void nnfw::cker::optimized::AddScalarBroadcast </td>
          <td>(</td>
          <td class="paramtype">int&#160;</td>
          <td class="paramname"><em>size</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const <a class="el" href="structnnfw_1_1cker_1_1_binary_arithmetic_op_param.html">BinaryArithmeticOpParam</a> &amp;&#160;</td>
          <td class="paramname"><em>params</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">int8_t&#160;</td>
          <td class="paramname"><em>input1_data</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const int8_t *&#160;</td>
          <td class="paramname"><em>input2_data</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">int8_t *&#160;</td>
          <td class="paramname"><em>output_data</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">inline</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p class="definition">Definition at line <a class="el" href="optimized_2_binary_arithmetic_ops_8h_source.html#l00727">727</a> of file <a class="el" href="optimized_2_binary_arithmetic_ops_8h_source.html">BinaryArithmeticOps.h</a>.</p>
<div class="fragment"><div class="line"><span class="lineno">  729</span>{</div>
<div class="line"><span class="lineno">  730</span>  <span class="keyword">using </span>gemmlowp::RoundingDivideByPOT;</div>
<div class="line"><span class="lineno">  731</span>  <span class="keywordtype">int</span> i = 0;</div>
<div class="line"><span class="lineno">  732</span><span class="preprocessor">#ifdef USE_NEON</span></div>
<div class="line"><span class="lineno">  733</span>  <span class="keyword">const</span> int32x4_t left_shift_dup = vdupq_n_s32(params.<a class="code hl_variable" href="structnnfw_1_1cker_1_1_binary_arithmetic_op_param.html#ae5b66510e715c8fb127887931de98dd9">left_shift</a>);</div>
<div class="line"><span class="lineno">  734</span>  <span class="keyword">const</span> int8x8_t output_activation_min_vector = vdup_n_s8(params.<a class="code hl_variable" href="structnnfw_1_1cker_1_1_binary_arithmetic_op_param.html#abb8ef3401fcae98626f1da50cda22b2f">quantized_activation_min</a>);</div>
<div class="line"><span class="lineno">  735</span>  <span class="keyword">const</span> int8x8_t output_activation_max_vector = vdup_n_s8(params.<a class="code hl_variable" href="structnnfw_1_1cker_1_1_binary_arithmetic_op_param.html#a20d919b6418bf302a97df426260bdec0">quantized_activation_max</a>);</div>
<div class="line"><span class="lineno">  736</span> </div>
<div class="line"><span class="lineno">  737</span>  <span class="comment">// Process broadcast scalar.</span></div>
<div class="line"><span class="lineno">  738</span>  <span class="keyword">const</span> int8x8_t input1_val_original = vdup_n_s8(input1_data);</div>
<div class="line"><span class="lineno">  739</span>  <span class="keyword">const</span> int16x8_t input1_val_s16 = vmovl_s8(input1_val_original);</div>
<div class="line"><span class="lineno">  740</span>  <span class="keyword">const</span> int16x8_t input1_val = vaddq_s16(input1_val_s16, vdupq_n_s16(params.<a class="code hl_variable" href="structnnfw_1_1cker_1_1_binary_arithmetic_op_param.html#a553e59d259a380f2da048553a23af0b0">input1_offset</a>));</div>
<div class="line"><span class="lineno">  741</span>  <span class="keyword">const</span> int16x4_t input1_val_high = vget_high_s16(input1_val);</div>
<div class="line"><span class="lineno">  742</span>  <span class="keyword">const</span> int16x4_t input1_val_low = vget_low_s16(input1_val);</div>
<div class="line"><span class="lineno">  743</span>  int32x4_t x11 = vmovl_s16(input1_val_low);</div>
<div class="line"><span class="lineno">  744</span>  int32x4_t x12 = vmovl_s16(input1_val_high);</div>
<div class="line"><span class="lineno">  745</span>  x11 = vshlq_s32(x11, left_shift_dup);</div>
<div class="line"><span class="lineno">  746</span>  x12 = vshlq_s32(x12, left_shift_dup);</div>
<div class="line"><span class="lineno">  747</span>  x11 = vqrdmulhq_n_s32(x11, params.<a class="code hl_variable" href="structnnfw_1_1cker_1_1_binary_arithmetic_op_param.html#ae26b1a5367ed8036f623f4116d22b703">input1_multiplier</a>);</div>
<div class="line"><span class="lineno">  748</span>  x12 = vqrdmulhq_n_s32(x12, params.<a class="code hl_variable" href="structnnfw_1_1cker_1_1_binary_arithmetic_op_param.html#ae26b1a5367ed8036f623f4116d22b703">input1_multiplier</a>);</div>
<div class="line"><span class="lineno">  749</span>  <span class="keyword">const</span> int32x4_t input1_shift_dup = vdupq_n_s32(params.<a class="code hl_variable" href="structnnfw_1_1cker_1_1_binary_arithmetic_op_param.html#a473732b2b25fa319a787fd22411f83b8">input1_shift</a>);</div>
<div class="line"><span class="lineno">  750</span>  x11 = vshlq_s32(x11, input1_shift_dup);</div>
<div class="line"><span class="lineno">  751</span>  x12 = vshlq_s32(x12, input1_shift_dup);</div>
<div class="line"><span class="lineno">  752</span> </div>
<div class="line"><span class="lineno">  753</span>  <span class="keywordflow">for</span> (; i &lt;= size - 8; i += 8)</div>
<div class="line"><span class="lineno">  754</span>  {</div>
<div class="line"><span class="lineno">  755</span>    <span class="keyword">const</span> int8x8_t input2_val_original = vld1_s8(input2_data + i);</div>
<div class="line"><span class="lineno">  756</span>    <span class="keyword">const</span> int16x8_t input2_val_s16 = vmovl_s8(input2_val_original);</div>
<div class="line"><span class="lineno">  757</span>    <span class="keyword">const</span> int16x8_t input2_val = vaddq_s16(input2_val_s16, vdupq_n_s16(params.<a class="code hl_variable" href="structnnfw_1_1cker_1_1_binary_arithmetic_op_param.html#ac4d8a22f96208f1efd75047318475f03">input2_offset</a>));</div>
<div class="line"><span class="lineno">  758</span>    <span class="keyword">const</span> int16x4_t input2_val_high = vget_high_s16(input2_val);</div>
<div class="line"><span class="lineno">  759</span>    <span class="keyword">const</span> int16x4_t input2_val_low = vget_low_s16(input2_val);</div>
<div class="line"><span class="lineno">  760</span>    int32x4_t x21 = vmovl_s16(input2_val_low);</div>
<div class="line"><span class="lineno">  761</span>    int32x4_t x22 = vmovl_s16(input2_val_high);</div>
<div class="line"><span class="lineno">  762</span>    x21 = vshlq_s32(x21, left_shift_dup);</div>
<div class="line"><span class="lineno">  763</span>    x22 = vshlq_s32(x22, left_shift_dup);</div>
<div class="line"><span class="lineno">  764</span>    x21 = vqrdmulhq_n_s32(x21, params.<a class="code hl_variable" href="structnnfw_1_1cker_1_1_binary_arithmetic_op_param.html#ab6b610a4f597712f6dcaf6576c04dc66">input2_multiplier</a>);</div>
<div class="line"><span class="lineno">  765</span>    x22 = vqrdmulhq_n_s32(x22, params.<a class="code hl_variable" href="structnnfw_1_1cker_1_1_binary_arithmetic_op_param.html#ab6b610a4f597712f6dcaf6576c04dc66">input2_multiplier</a>);</div>
<div class="line"><span class="lineno">  766</span>    <span class="keyword">const</span> int32x4_t input2_shift_dup = vdupq_n_s32(params.<a class="code hl_variable" href="structnnfw_1_1cker_1_1_binary_arithmetic_op_param.html#aaf1857ba76a406a592402a13607421a5">input2_shift</a>);</div>
<div class="line"><span class="lineno">  767</span>    x21 = vshlq_s32(x21, input2_shift_dup);</div>
<div class="line"><span class="lineno">  768</span>    x22 = vshlq_s32(x22, input2_shift_dup);</div>
<div class="line"><span class="lineno">  769</span>    int32x4_t s1 = vaddq_s32(x11, x21);</div>
<div class="line"><span class="lineno">  770</span>    int32x4_t s2 = vaddq_s32(x12, x22);</div>
<div class="line"><span class="lineno">  771</span>    s1 = vqrdmulhq_n_s32(s1, params.<a class="code hl_variable" href="structnnfw_1_1cker_1_1_binary_arithmetic_op_param.html#adae6f3cd4f5808bde90f4dfd3d915b4c">output_multiplier</a>);</div>
<div class="line"><span class="lineno">  772</span>    s2 = vqrdmulhq_n_s32(s2, params.<a class="code hl_variable" href="structnnfw_1_1cker_1_1_binary_arithmetic_op_param.html#adae6f3cd4f5808bde90f4dfd3d915b4c">output_multiplier</a>);</div>
<div class="line"><span class="lineno">  773</span>    s1 = RoundingDivideByPOT(s1, -params.<a class="code hl_variable" href="structnnfw_1_1cker_1_1_binary_arithmetic_op_param.html#a1c0adc1575a15790a35479c5fa5093a4">output_shift</a>);</div>
<div class="line"><span class="lineno">  774</span>    s2 = RoundingDivideByPOT(s2, -params.<a class="code hl_variable" href="structnnfw_1_1cker_1_1_binary_arithmetic_op_param.html#a1c0adc1575a15790a35479c5fa5093a4">output_shift</a>);</div>
<div class="line"><span class="lineno">  775</span>    <span class="keyword">const</span> int16x4_t s1_narrowed = vmovn_s32(s1);</div>
<div class="line"><span class="lineno">  776</span>    <span class="keyword">const</span> int16x4_t s2_narrowed = vmovn_s32(s2);</div>
<div class="line"><span class="lineno">  777</span>    <span class="keyword">const</span> int16x8_t s =</div>
<div class="line"><span class="lineno">  778</span>      vaddq_s16(vcombine_s16(s1_narrowed, s2_narrowed), vdupq_n_s16(params.<a class="code hl_variable" href="structnnfw_1_1cker_1_1_binary_arithmetic_op_param.html#aa5bf6b7c6e3772bd04264f6729915729">output_offset</a>));</div>
<div class="line"><span class="lineno">  779</span>    <span class="keyword">const</span> int8x8_t clamped =</div>
<div class="line"><span class="lineno">  780</span>      vmax_s8(output_activation_min_vector, vmin_s8(output_activation_max_vector, vqmovn_s16(s)));</div>
<div class="line"><span class="lineno">  781</span>    vst1_s8(output_data + i, clamped);</div>
<div class="line"><span class="lineno">  782</span>  }</div>
<div class="line"><span class="lineno">  783</span><span class="preprocessor">#endif </span><span class="comment">// NEON</span></div>
<div class="line"><span class="lineno">  784</span> </div>
<div class="line"><span class="lineno">  785</span>  <span class="keywordflow">if</span> (i &lt; size)</div>
<div class="line"><span class="lineno">  786</span>  {</div>
<div class="line"><span class="lineno">  787</span>    <span class="comment">// Process broadcast scalar.</span></div>
<div class="line"><span class="lineno">  788</span>    <span class="keyword">const</span> int32_t input1_val = params.<a class="code hl_variable" href="structnnfw_1_1cker_1_1_binary_arithmetic_op_param.html#a553e59d259a380f2da048553a23af0b0">input1_offset</a> + input1_data;</div>
<div class="line"><span class="lineno">  789</span>    <span class="keyword">const</span> int32_t shifted_input1_val = input1_val * (1 &lt;&lt; params.<a class="code hl_variable" href="structnnfw_1_1cker_1_1_binary_arithmetic_op_param.html#ae5b66510e715c8fb127887931de98dd9">left_shift</a>);</div>
<div class="line"><span class="lineno">  790</span>    <span class="keyword">const</span> int32_t scaled_input1_val = <a class="code hl_function" href="namespacennfw_1_1cker.html#afe95d2a91cb367a4b9be585d4ff4b90f">MultiplyByQuantizedMultiplierSmallerThanOneExp</a>(</div>
<div class="line"><span class="lineno">  791</span>      shifted_input1_val, params.<a class="code hl_variable" href="structnnfw_1_1cker_1_1_binary_arithmetic_op_param.html#ae26b1a5367ed8036f623f4116d22b703">input1_multiplier</a>, params.<a class="code hl_variable" href="structnnfw_1_1cker_1_1_binary_arithmetic_op_param.html#a473732b2b25fa319a787fd22411f83b8">input1_shift</a>);</div>
<div class="line"><span class="lineno">  792</span> </div>
<div class="line"><span class="lineno">  793</span>    <span class="keywordflow">for</span> (; i &lt; size; ++i)</div>
<div class="line"><span class="lineno">  794</span>    {</div>
<div class="line"><span class="lineno">  795</span>      <span class="keyword">const</span> int32_t input2_val = params.<a class="code hl_variable" href="structnnfw_1_1cker_1_1_binary_arithmetic_op_param.html#ac4d8a22f96208f1efd75047318475f03">input2_offset</a> + input2_data[i];</div>
<div class="line"><span class="lineno">  796</span>      <span class="keyword">const</span> int32_t shifted_input2_val = input2_val * (1 &lt;&lt; params.<a class="code hl_variable" href="structnnfw_1_1cker_1_1_binary_arithmetic_op_param.html#ae5b66510e715c8fb127887931de98dd9">left_shift</a>);</div>
<div class="line"><span class="lineno">  797</span>      <span class="keyword">const</span> int32_t scaled_input2_val = <a class="code hl_function" href="namespacennfw_1_1cker.html#afe95d2a91cb367a4b9be585d4ff4b90f">MultiplyByQuantizedMultiplierSmallerThanOneExp</a>(</div>
<div class="line"><span class="lineno">  798</span>        shifted_input2_val, params.<a class="code hl_variable" href="structnnfw_1_1cker_1_1_binary_arithmetic_op_param.html#ab6b610a4f597712f6dcaf6576c04dc66">input2_multiplier</a>, params.<a class="code hl_variable" href="structnnfw_1_1cker_1_1_binary_arithmetic_op_param.html#aaf1857ba76a406a592402a13607421a5">input2_shift</a>);</div>
<div class="line"><span class="lineno">  799</span>      <span class="keyword">const</span> int32_t raw_sum = scaled_input1_val + scaled_input2_val;</div>
<div class="line"><span class="lineno">  800</span>      <span class="keyword">const</span> int32_t raw_output = <a class="code hl_function" href="namespacennfw_1_1cker.html#afe95d2a91cb367a4b9be585d4ff4b90f">MultiplyByQuantizedMultiplierSmallerThanOneExp</a>(</div>
<div class="line"><span class="lineno">  801</span>                                   raw_sum, params.<a class="code hl_variable" href="structnnfw_1_1cker_1_1_binary_arithmetic_op_param.html#adae6f3cd4f5808bde90f4dfd3d915b4c">output_multiplier</a>, params.<a class="code hl_variable" href="structnnfw_1_1cker_1_1_binary_arithmetic_op_param.html#a1c0adc1575a15790a35479c5fa5093a4">output_shift</a>) +</div>
<div class="line"><span class="lineno">  802</span>                                 params.<a class="code hl_variable" href="structnnfw_1_1cker_1_1_binary_arithmetic_op_param.html#aa5bf6b7c6e3772bd04264f6729915729">output_offset</a>;</div>
<div class="line"><span class="lineno">  803</span>      <span class="keyword">const</span> int32_t clamped_output = std::min(</div>
<div class="line"><span class="lineno">  804</span>        params.<a class="code hl_variable" href="structnnfw_1_1cker_1_1_binary_arithmetic_op_param.html#a20d919b6418bf302a97df426260bdec0">quantized_activation_max</a>, std::max(params.<a class="code hl_variable" href="structnnfw_1_1cker_1_1_binary_arithmetic_op_param.html#abb8ef3401fcae98626f1da50cda22b2f">quantized_activation_min</a>, raw_output));</div>
<div class="line"><span class="lineno">  805</span>      output_data[i] = <span class="keyword">static_cast&lt;</span>int8_t<span class="keyword">&gt;</span>(clamped_output);</div>
<div class="line"><span class="lineno">  806</span>    }</div>
<div class="line"><span class="lineno">  807</span>  }</div>
<div class="line"><span class="lineno">  808</span>}</div>
</div><!-- fragment -->
<p class="reference">References <a class="el" href="compute_2cker_2include_2cker_2_types_8h_source.html#l00184">nnfw::cker::BinaryArithmeticOpParam::input1_multiplier</a>, <a class="el" href="compute_2cker_2include_2cker_2_types_8h_source.html#l00177">nnfw::cker::BinaryArithmeticOpParam::input1_offset</a>, <a class="el" href="compute_2cker_2include_2cker_2_types_8h_source.html#l00185">nnfw::cker::BinaryArithmeticOpParam::input1_shift</a>, <a class="el" href="compute_2cker_2include_2cker_2_types_8h_source.html#l00186">nnfw::cker::BinaryArithmeticOpParam::input2_multiplier</a>, <a class="el" href="compute_2cker_2include_2cker_2_types_8h_source.html#l00178">nnfw::cker::BinaryArithmeticOpParam::input2_offset</a>, <a class="el" href="compute_2cker_2include_2cker_2_types_8h_source.html#l00187">nnfw::cker::BinaryArithmeticOpParam::input2_shift</a>, <a class="el" href="compute_2cker_2include_2cker_2_types_8h_source.html#l00183">nnfw::cker::BinaryArithmeticOpParam::left_shift</a>, <a class="el" href="compute_2cker_2include_2cker_2_utils_8h_source.html#l00108">nnfw::cker::MultiplyByQuantizedMultiplierSmallerThanOneExp()</a>, <a class="el" href="compute_2cker_2include_2cker_2_types_8h_source.html#l00180">nnfw::cker::BinaryArithmeticOpParam::output_multiplier</a>, <a class="el" href="compute_2cker_2include_2cker_2_types_8h_source.html#l00179">nnfw::cker::BinaryArithmeticOpParam::output_offset</a>, <a class="el" href="compute_2cker_2include_2cker_2_types_8h_source.html#l00181">nnfw::cker::BinaryArithmeticOpParam::output_shift</a>, <a class="el" href="compute_2cker_2include_2cker_2_types_8h_source.html#l00190">nnfw::cker::BinaryArithmeticOpParam::quantized_activation_max</a>, and <a class="el" href="compute_2cker_2include_2cker_2_types_8h_source.html#l00189">nnfw::cker::BinaryArithmeticOpParam::quantized_activation_min</a>.</p>

</div>
</div>
<a id="a400314f9559841f0817833b3e2afcb3f" name="a400314f9559841f0817833b3e2afcb3f"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a400314f9559841f0817833b3e2afcb3f">&#9670;&#160;</a></span>AddScalarBroadcast() <span class="overload">[2/2]</span></h2>

<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">void nnfw::cker::optimized::AddScalarBroadcast </td>
          <td>(</td>
          <td class="paramtype">int&#160;</td>
          <td class="paramname"><em>size</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const <a class="el" href="structnnfw_1_1cker_1_1_binary_arithmetic_op_param.html">BinaryArithmeticOpParam</a> &amp;&#160;</td>
          <td class="paramname"><em>params</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">uint8_t&#160;</td>
          <td class="paramname"><em>broadcast_value</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const uint8_t *&#160;</td>
          <td class="paramname"><em>input2_data</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">uint8_t *&#160;</td>
          <td class="paramname"><em>output_data</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">inline</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p class="definition">Definition at line <a class="el" href="optimized_2_binary_arithmetic_ops_8h_source.html#l00711">711</a> of file <a class="el" href="optimized_2_binary_arithmetic_ops_8h_source.html">BinaryArithmeticOps.h</a>.</p>
<div class="fragment"><div class="line"><span class="lineno">  714</span>{</div>
<div class="line"><span class="lineno">  715</span>  <span class="keywordtype">int</span> i = 0;</div>
<div class="line"><span class="lineno">  716</span>  int32_t clamped_output;</div>
<div class="line"><span class="lineno">  717</span>  <span class="keywordflow">for</span> (; i &lt; size; ++i)</div>
<div class="line"><span class="lineno">  718</span>  {</div>
<div class="line"><span class="lineno">  719</span>    clamped_output = <a class="code hl_function" href="namespacennfw_1_1cker_1_1optimized.html#a1b089b3d4d74b15a41c88996227a23e3">quant8_sum</a>(params, broadcast_value, input2_data[i]);</div>
<div class="line"><span class="lineno">  720</span>    output_data[i] = <span class="keyword">static_cast&lt;</span>uint8_t<span class="keyword">&gt;</span>(clamped_output);</div>
<div class="line"><span class="lineno">  721</span>  }</div>
<div class="line"><span class="lineno">  722</span>}</div>
<div class="ttc" id="anamespacennfw_1_1cker_1_1optimized_html_a1b089b3d4d74b15a41c88996227a23e3"><div class="ttname"><a href="namespacennfw_1_1cker_1_1optimized.html#a1b089b3d4d74b15a41c88996227a23e3">nnfw::cker::optimized::quant8_sum</a></div><div class="ttdeci">std::enable_if_t&lt; is_quant8&lt; T &gt;::value, int32_t &gt; quant8_sum(const BinaryArithmeticOpParam &amp;params, const T input1_data, const T input2_data)</div><div class="ttdef"><b>Definition:</b> <a href="optimized_2_binary_arithmetic_ops_8h_source.html#l00227">BinaryArithmeticOps.h:227</a></div></div>
</div><!-- fragment -->
<p class="reference">References <a class="el" href="optimized_2_binary_arithmetic_ops_8h_source.html#l00227">quant8_sum()</a>.</p>

<p class="reference">Referenced by <a class="el" href="optimized_2_binary_arithmetic_ops_8h_source.html#l00812">BroadcastAddDispatch()</a>.</p>

</div>
</div>
<a id="acc41c0b6a568f6a484fbfc9618e0a055" name="acc41c0b6a568f6a484fbfc9618e0a055"></a>
<h2 class="memtitle"><span class="permalink"><a href="#acc41c0b6a568f6a484fbfc9618e0a055">&#9670;&#160;</a></span>BinaryBroadcastFiveFold() <span class="overload">[1/2]</span></h2>

<div class="memitem">
<div class="memproto">
<div class="memtemplate">
template&lt;typename ElementwiseF , typename ScalarBroadcastF , typename T &gt; </div>
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">void nnfw::cker::optimized::BinaryBroadcastFiveFold </td>
          <td>(</td>
          <td class="paramtype">const <a class="el" href="structnnfw_1_1cker_1_1_binary_arithmetic_op_param.html">BinaryArithmeticOpParam</a> &amp;&#160;</td>
          <td class="paramname"><em>params</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">bool&#160;</td>
          <td class="paramname"><em>switch_inputs</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const <a class="el" href="classnnfw_1_1cker_1_1_shape.html">Shape</a> &amp;&#160;</td>
          <td class="paramname">, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const T *&#160;</td>
          <td class="paramname"><em>unswitched_input1_data</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const <a class="el" href="classnnfw_1_1cker_1_1_shape.html">Shape</a> &amp;&#160;</td>
          <td class="paramname">, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const T *&#160;</td>
          <td class="paramname"><em>unswitched_input2_data</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const <a class="el" href="classnnfw_1_1cker_1_1_shape.html">Shape</a> &amp;&#160;</td>
          <td class="paramname">, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">T *&#160;</td>
          <td class="paramname"><em>output_data</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">ElementwiseF&#160;</td>
          <td class="paramname"><em>elementwise_f</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">ScalarBroadcastF&#160;</td>
          <td class="paramname"><em>scalar_broadcast_f</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">inline</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p class="definition">Definition at line <a class="el" href="optimized_2_binary_arithmetic_ops_8h_source.html#l00040">40</a> of file <a class="el" href="optimized_2_binary_arithmetic_ops_8h_source.html">BinaryArithmeticOps.h</a>.</p>
<div class="fragment"><div class="line"><span class="lineno">   47</span>{</div>
<div class="line"><span class="lineno">   48</span>  <span class="keyword">const</span> T *input1_data = switch_inputs ? unswitched_input2_data : unswitched_input1_data;</div>
<div class="line"><span class="lineno">   49</span>  <span class="keyword">const</span> T *input2_data = switch_inputs ? unswitched_input1_data : unswitched_input2_data;</div>
<div class="line"><span class="lineno">   50</span> </div>
<div class="line"><span class="lineno">   51</span>  <span class="comment">// Fivefold nested loops. The second input resets its position for each</span></div>
<div class="line"><span class="lineno">   52</span>  <span class="comment">// iteration of the second loop. The first input resets its position at the</span></div>
<div class="line"><span class="lineno">   53</span>  <span class="comment">// beginning of the fourth loop. The innermost loop is an elementwise add of</span></div>
<div class="line"><span class="lineno">   54</span>  <span class="comment">// sections of the arrays.</span></div>
<div class="line"><span class="lineno">   55</span>  T *output_data_ptr = output_data;</div>
<div class="line"><span class="lineno">   56</span>  <span class="keyword">const</span> T *input1_data_ptr = input1_data;</div>
<div class="line"><span class="lineno">   57</span>  <span class="keyword">const</span> T *input2_data_reset = input2_data;</div>
<div class="line"><span class="lineno">   58</span>  <span class="comment">// In the fivefold pattern, y0, y2 and y4 are not broadcast, and so shared</span></div>
<div class="line"><span class="lineno">   59</span>  <span class="comment">// between input shapes. y3 for input 1 is always broadcast, and so the</span></div>
<div class="line"><span class="lineno">   60</span>  <span class="comment">// dimension there is 1, whereas optionally y1 might be broadcast for input 2.</span></div>
<div class="line"><span class="lineno">   61</span>  <span class="comment">// Put another way,</span></div>
<div class="line"><span class="lineno">   62</span>  <span class="comment">// input1.shape.FlatSize = y0 * y1 * y2 * y4,</span></div>
<div class="line"><span class="lineno">   63</span>  <span class="comment">// input2.shape.FlatSize = y0 * y2 * y3 * y4.</span></div>
<div class="line"><span class="lineno">   64</span>  <span class="keywordtype">int</span> y0 = params.<a class="code hl_variable" href="structnnfw_1_1cker_1_1_binary_arithmetic_op_param.html#af5001901a39c73ed102fd42b617e8955">broadcast_shape</a>[0];</div>
<div class="line"><span class="lineno">   65</span>  <span class="keywordtype">int</span> y1 = params.<a class="code hl_variable" href="structnnfw_1_1cker_1_1_binary_arithmetic_op_param.html#af5001901a39c73ed102fd42b617e8955">broadcast_shape</a>[1];</div>
<div class="line"><span class="lineno">   66</span>  <span class="keywordtype">int</span> y2 = params.<a class="code hl_variable" href="structnnfw_1_1cker_1_1_binary_arithmetic_op_param.html#af5001901a39c73ed102fd42b617e8955">broadcast_shape</a>[2];</div>
<div class="line"><span class="lineno">   67</span>  <span class="keywordtype">int</span> y3 = params.<a class="code hl_variable" href="structnnfw_1_1cker_1_1_binary_arithmetic_op_param.html#af5001901a39c73ed102fd42b617e8955">broadcast_shape</a>[3];</div>
<div class="line"><span class="lineno">   68</span>  <span class="keywordtype">int</span> y4 = params.<a class="code hl_variable" href="structnnfw_1_1cker_1_1_binary_arithmetic_op_param.html#af5001901a39c73ed102fd42b617e8955">broadcast_shape</a>[4];</div>
<div class="line"><span class="lineno">   69</span>  <span class="keywordflow">if</span> (y4 &gt; 1)</div>
<div class="line"><span class="lineno">   70</span>  {</div>
<div class="line"><span class="lineno">   71</span>    <span class="comment">// General fivefold pattern, with y4 &gt; 1 so there is a non-broadcast inner</span></div>
<div class="line"><span class="lineno">   72</span>    <span class="comment">// dimension.</span></div>
<div class="line"><span class="lineno">   73</span>    <span class="keywordflow">for</span> (<span class="keywordtype">int</span> i0 = 0; i0 &lt; y0; ++i0)</div>
<div class="line"><span class="lineno">   74</span>    {</div>
<div class="line"><span class="lineno">   75</span>      <span class="keyword">const</span> T *input2_data_ptr = <span class="keyword">nullptr</span>;</div>
<div class="line"><span class="lineno">   76</span>      <span class="keywordflow">for</span> (<span class="keywordtype">int</span> i1 = 0; i1 &lt; y1; ++i1)</div>
<div class="line"><span class="lineno">   77</span>      {</div>
<div class="line"><span class="lineno">   78</span>        input2_data_ptr = input2_data_reset;</div>
<div class="line"><span class="lineno">   79</span>        <span class="keywordflow">for</span> (<span class="keywordtype">int</span> i2 = 0; i2 &lt; y2; ++i2)</div>
<div class="line"><span class="lineno">   80</span>        {</div>
<div class="line"><span class="lineno">   81</span>          <span class="keywordflow">for</span> (<span class="keywordtype">int</span> i3 = 0; i3 &lt; y3; ++i3)</div>
<div class="line"><span class="lineno">   82</span>          {</div>
<div class="line"><span class="lineno">   83</span>            elementwise_f(y4, params, input1_data_ptr, input2_data_ptr, output_data_ptr);</div>
<div class="line"><span class="lineno">   84</span>            input2_data_ptr += y4;</div>
<div class="line"><span class="lineno">   85</span>            output_data_ptr += y4;</div>
<div class="line"><span class="lineno">   86</span>          }</div>
<div class="line"><span class="lineno">   87</span>          <span class="comment">// We have broadcast y4 of input1 data y3 times, and now move on.</span></div>
<div class="line"><span class="lineno">   88</span>          input1_data_ptr += y4;</div>
<div class="line"><span class="lineno">   89</span>        }</div>
<div class="line"><span class="lineno">   90</span>      }</div>
<div class="line"><span class="lineno">   91</span>      <span class="comment">// We have broadcast y2*y3*y4 of input2 data y1 times, and now move on.</span></div>
<div class="line"><span class="lineno">   92</span>      input2_data_reset = input2_data_ptr;</div>
<div class="line"><span class="lineno">   93</span>    }</div>
<div class="line"><span class="lineno">   94</span>  }</div>
<div class="line"><span class="lineno">   95</span>  <span class="keywordflow">else</span></div>
<div class="line"><span class="lineno">   96</span>  {</div>
<div class="line"><span class="lineno">   97</span>    <span class="comment">// Special case of y4 == 1, in which the innermost loop is a single element</span></div>
<div class="line"><span class="lineno">   98</span>    <span class="comment">// and can be combined with the next (y3) as an inner broadcast.</span></div>
<div class="line"><span class="lineno">   99</span>    <span class="comment">//</span></div>
<div class="line"><span class="lineno">  100</span>    <span class="comment">// Note that this handles the case of pure scalar broadcast when</span></div>
<div class="line"><span class="lineno">  101</span>    <span class="comment">// y0 == y1 == y2 == 1. With low overhead it handles cases such as scalar</span></div>
<div class="line"><span class="lineno">  102</span>    <span class="comment">// broadcast with batch (as y2 &gt; 1).</span></div>
<div class="line"><span class="lineno">  103</span>    <span class="comment">//</span></div>
<div class="line"><span class="lineno">  104</span>    <span class="comment">// NOTE The process is the same as the above general case except simplified</span></div>
<div class="line"><span class="lineno">  105</span>    <span class="comment">// for y4 == 1 and the loop over y3 is contained within the</span></div>
<div class="line"><span class="lineno">  106</span>    <span class="comment">// AddScalarBroadcast function.</span></div>
<div class="line"><span class="lineno">  107</span>    <span class="keywordflow">for</span> (<span class="keywordtype">int</span> i0 = 0; i0 &lt; y0; ++i0)</div>
<div class="line"><span class="lineno">  108</span>    {</div>
<div class="line"><span class="lineno">  109</span>      <span class="keyword">const</span> T *input2_data_ptr = <span class="keyword">nullptr</span>;</div>
<div class="line"><span class="lineno">  110</span>      <span class="keywordflow">for</span> (<span class="keywordtype">int</span> i1 = 0; i1 &lt; y1; ++i1)</div>
<div class="line"><span class="lineno">  111</span>      {</div>
<div class="line"><span class="lineno">  112</span>        input2_data_ptr = input2_data_reset;</div>
<div class="line"><span class="lineno">  113</span>        <span class="keywordflow">for</span> (<span class="keywordtype">int</span> i2 = 0; i2 &lt; y2; ++i2)</div>
<div class="line"><span class="lineno">  114</span>        {</div>
<div class="line"><span class="lineno">  115</span>          scalar_broadcast_f(y3, params, *input1_data_ptr, input2_data_ptr, output_data_ptr);</div>
<div class="line"><span class="lineno">  116</span>          input2_data_ptr += y3;</div>
<div class="line"><span class="lineno">  117</span>          output_data_ptr += y3;</div>
<div class="line"><span class="lineno">  118</span>          input1_data_ptr += 1;</div>
<div class="line"><span class="lineno">  119</span>        }</div>
<div class="line"><span class="lineno">  120</span>      }</div>
<div class="line"><span class="lineno">  121</span>      input2_data_reset = input2_data_ptr;</div>
<div class="line"><span class="lineno">  122</span>    }</div>
<div class="line"><span class="lineno">  123</span>  }</div>
<div class="line"><span class="lineno">  124</span>}</div>
<div class="ttc" id="astructnnfw_1_1cker_1_1_binary_arithmetic_op_param_html_af5001901a39c73ed102fd42b617e8955"><div class="ttname"><a href="structnnfw_1_1cker_1_1_binary_arithmetic_op_param.html#af5001901a39c73ed102fd42b617e8955">nnfw::cker::BinaryArithmeticOpParam::broadcast_shape</a></div><div class="ttdeci">int broadcast_shape[5]</div><div class="ttdef"><b>Definition:</b> <a href="compute_2cker_2include_2cker_2_types_8h_source.html#l00204">Types.h:204</a></div></div>
</div><!-- fragment -->
<p class="reference">References <a class="el" href="compute_2cker_2include_2cker_2_types_8h_source.html#l00204">nnfw::cker::BinaryArithmeticOpParam::broadcast_shape</a>.</p>

<p class="reference">Referenced by <a class="el" href="optimized_2_binary_arithmetic_ops_8h_source.html#l00812">BroadcastAddDispatch()</a>, <a class="el" href="optimized_2_binary_arithmetic_ops_8h_source.html#l01234">BroadcastDivDispatch()</a>, <a class="el" href="optimized_2_binary_arithmetic_ops_8h_source.html#l01177">BroadcastMulDispatch()</a>, and <a class="el" href="optimized_2_binary_arithmetic_ops_8h_source.html#l00867">BroadcastSubDispatch()</a>.</p>

</div>
</div>
<a id="a549468359ceb389098998f7af1e959a4" name="a549468359ceb389098998f7af1e959a4"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a549468359ceb389098998f7af1e959a4">&#9670;&#160;</a></span>BinaryBroadcastFiveFold() <span class="overload">[2/2]</span></h2>

<div class="memitem">
<div class="memproto">
<div class="memtemplate">
template&lt;typename ElementwiseF , typename ScalarBroadcastF , typename T &gt; </div>
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">void nnfw::cker::optimized::BinaryBroadcastFiveFold </td>
          <td>(</td>
          <td class="paramtype">const <a class="el" href="structnnfw_1_1cker_1_1_binary_arithmetic_op_param.html">BinaryArithmeticOpParam</a> &amp;&#160;</td>
          <td class="paramname"><em>unswitched_params</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const <a class="el" href="classnnfw_1_1cker_1_1_shape.html">Shape</a> &amp;&#160;</td>
          <td class="paramname">, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const T *&#160;</td>
          <td class="paramname"><em>unswitched_input1_data</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const <a class="el" href="classnnfw_1_1cker_1_1_shape.html">Shape</a> &amp;&#160;</td>
          <td class="paramname">, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const T *&#160;</td>
          <td class="paramname"><em>unswitched_input2_data</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const <a class="el" href="classnnfw_1_1cker_1_1_shape.html">Shape</a> &amp;&#160;</td>
          <td class="paramname">, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">T *&#160;</td>
          <td class="paramname"><em>output_data</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">ElementwiseF&#160;</td>
          <td class="paramname"><em>elementwise_f</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">ScalarBroadcastF&#160;</td>
          <td class="paramname"><em>scalar_broadcast_f</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">inline</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p class="definition">Definition at line <a class="el" href="optimized_2_binary_arithmetic_ops_8h_source.html#l00128">128</a> of file <a class="el" href="optimized_2_binary_arithmetic_ops_8h_source.html">BinaryArithmeticOps.h</a>.</p>
<div class="fragment"><div class="line"><span class="lineno">  135</span>{</div>
<div class="line"><span class="lineno">  136</span>  <a class="code hl_struct" href="structnnfw_1_1cker_1_1_binary_arithmetic_op_param.html">BinaryArithmeticOpParam</a> switched_params = unswitched_params;</div>
<div class="line"><span class="lineno">  137</span>  switched_params.<a class="code hl_variable" href="structnnfw_1_1cker_1_1_binary_arithmetic_op_param.html#a553e59d259a380f2da048553a23af0b0">input1_offset</a> = unswitched_params.<a class="code hl_variable" href="structnnfw_1_1cker_1_1_binary_arithmetic_op_param.html#ac4d8a22f96208f1efd75047318475f03">input2_offset</a>;</div>
<div class="line"><span class="lineno">  138</span>  switched_params.<a class="code hl_variable" href="structnnfw_1_1cker_1_1_binary_arithmetic_op_param.html#ae26b1a5367ed8036f623f4116d22b703">input1_multiplier</a> = unswitched_params.<a class="code hl_variable" href="structnnfw_1_1cker_1_1_binary_arithmetic_op_param.html#ab6b610a4f597712f6dcaf6576c04dc66">input2_multiplier</a>;</div>
<div class="line"><span class="lineno">  139</span>  switched_params.<a class="code hl_variable" href="structnnfw_1_1cker_1_1_binary_arithmetic_op_param.html#a473732b2b25fa319a787fd22411f83b8">input1_shift</a> = unswitched_params.<a class="code hl_variable" href="structnnfw_1_1cker_1_1_binary_arithmetic_op_param.html#aaf1857ba76a406a592402a13607421a5">input2_shift</a>;</div>
<div class="line"><span class="lineno">  140</span>  switched_params.<a class="code hl_variable" href="structnnfw_1_1cker_1_1_binary_arithmetic_op_param.html#ac4d8a22f96208f1efd75047318475f03">input2_offset</a> = unswitched_params.<a class="code hl_variable" href="structnnfw_1_1cker_1_1_binary_arithmetic_op_param.html#a553e59d259a380f2da048553a23af0b0">input1_offset</a>;</div>
<div class="line"><span class="lineno">  141</span>  switched_params.<a class="code hl_variable" href="structnnfw_1_1cker_1_1_binary_arithmetic_op_param.html#ab6b610a4f597712f6dcaf6576c04dc66">input2_multiplier</a> = unswitched_params.<a class="code hl_variable" href="structnnfw_1_1cker_1_1_binary_arithmetic_op_param.html#ae26b1a5367ed8036f623f4116d22b703">input1_multiplier</a>;</div>
<div class="line"><span class="lineno">  142</span>  switched_params.<a class="code hl_variable" href="structnnfw_1_1cker_1_1_binary_arithmetic_op_param.html#aaf1857ba76a406a592402a13607421a5">input2_shift</a> = unswitched_params.<a class="code hl_variable" href="structnnfw_1_1cker_1_1_binary_arithmetic_op_param.html#a473732b2b25fa319a787fd22411f83b8">input1_shift</a>;</div>
<div class="line"><span class="lineno">  143</span> </div>
<div class="line"><span class="lineno">  144</span>  <span class="keyword">const</span> <span class="keywordtype">bool</span> use_unswitched =</div>
<div class="line"><span class="lineno">  145</span>    unswitched_params.<a class="code hl_variable" href="structnnfw_1_1cker_1_1_binary_arithmetic_op_param.html#aa40d8c9eb2a4cea35596c77395fae910">broadcast_category</a> == BroadcastableOpCategory::kFirstInputBroadcastsFast;</div>
<div class="line"><span class="lineno">  146</span> </div>
<div class="line"><span class="lineno">  147</span>  <span class="keyword">const</span> <a class="code hl_struct" href="structnnfw_1_1cker_1_1_binary_arithmetic_op_param.html">BinaryArithmeticOpParam</a> &amp;params = use_unswitched ? unswitched_params : switched_params;</div>
<div class="line"><span class="lineno">  148</span>  <span class="keyword">const</span> T *input1_data = use_unswitched ? unswitched_input1_data : unswitched_input2_data;</div>
<div class="line"><span class="lineno">  149</span>  <span class="keyword">const</span> T *input2_data = use_unswitched ? unswitched_input2_data : unswitched_input1_data;</div>
<div class="line"><span class="lineno">  150</span> </div>
<div class="line"><span class="lineno">  151</span>  <span class="comment">// Fivefold nested loops. The second input resets its position for each</span></div>
<div class="line"><span class="lineno">  152</span>  <span class="comment">// iteration of the second loop. The first input resets its position at the</span></div>
<div class="line"><span class="lineno">  153</span>  <span class="comment">// beginning of the fourth loop. The innermost loop is an elementwise add of</span></div>
<div class="line"><span class="lineno">  154</span>  <span class="comment">// sections of the arrays.</span></div>
<div class="line"><span class="lineno">  155</span>  T *output_data_ptr = output_data;</div>
<div class="line"><span class="lineno">  156</span>  <span class="keyword">const</span> T *input1_data_ptr = input1_data;</div>
<div class="line"><span class="lineno">  157</span>  <span class="keyword">const</span> T *input2_data_reset = input2_data;</div>
<div class="line"><span class="lineno">  158</span>  <span class="comment">// In the fivefold pattern, y0, y2 and y4 are not broadcast, and so shared</span></div>
<div class="line"><span class="lineno">  159</span>  <span class="comment">// between input shapes. y3 for input 1 is always broadcast, and so the</span></div>
<div class="line"><span class="lineno">  160</span>  <span class="comment">// dimension there is 1, whereas optionally y1 might be broadcast for</span></div>
<div class="line"><span class="lineno">  161</span>  <span class="comment">// input 2. Put another way, input1.shape.FlatSize = y0 * y1 * y2 * y4,</span></div>
<div class="line"><span class="lineno">  162</span>  <span class="comment">// input2.shape.FlatSize = y0 * y2 * y3 * y4.</span></div>
<div class="line"><span class="lineno">  163</span>  <span class="keywordtype">int</span> y0 = params.<a class="code hl_variable" href="structnnfw_1_1cker_1_1_binary_arithmetic_op_param.html#af5001901a39c73ed102fd42b617e8955">broadcast_shape</a>[0];</div>
<div class="line"><span class="lineno">  164</span>  <span class="keywordtype">int</span> y1 = params.<a class="code hl_variable" href="structnnfw_1_1cker_1_1_binary_arithmetic_op_param.html#af5001901a39c73ed102fd42b617e8955">broadcast_shape</a>[1];</div>
<div class="line"><span class="lineno">  165</span>  <span class="keywordtype">int</span> y2 = params.<a class="code hl_variable" href="structnnfw_1_1cker_1_1_binary_arithmetic_op_param.html#af5001901a39c73ed102fd42b617e8955">broadcast_shape</a>[2];</div>
<div class="line"><span class="lineno">  166</span>  <span class="keywordtype">int</span> y3 = params.<a class="code hl_variable" href="structnnfw_1_1cker_1_1_binary_arithmetic_op_param.html#af5001901a39c73ed102fd42b617e8955">broadcast_shape</a>[3];</div>
<div class="line"><span class="lineno">  167</span>  <span class="keywordtype">int</span> y4 = params.<a class="code hl_variable" href="structnnfw_1_1cker_1_1_binary_arithmetic_op_param.html#af5001901a39c73ed102fd42b617e8955">broadcast_shape</a>[4];</div>
<div class="line"><span class="lineno">  168</span>  <span class="keywordflow">if</span> (y4 &gt; 1)</div>
<div class="line"><span class="lineno">  169</span>  {</div>
<div class="line"><span class="lineno">  170</span>    <span class="comment">// General fivefold pattern, with y4 &gt; 1 so there is a non-broadcast inner</span></div>
<div class="line"><span class="lineno">  171</span>    <span class="comment">// dimension.</span></div>
<div class="line"><span class="lineno">  172</span>    <span class="keywordflow">for</span> (<span class="keywordtype">int</span> i0 = 0; i0 &lt; y0; ++i0)</div>
<div class="line"><span class="lineno">  173</span>    {</div>
<div class="line"><span class="lineno">  174</span>      <span class="keyword">const</span> T *input2_data_ptr = <span class="keyword">nullptr</span>;</div>
<div class="line"><span class="lineno">  175</span>      <span class="keywordflow">for</span> (<span class="keywordtype">int</span> i1 = 0; i1 &lt; y1; ++i1)</div>
<div class="line"><span class="lineno">  176</span>      {</div>
<div class="line"><span class="lineno">  177</span>        input2_data_ptr = input2_data_reset;</div>
<div class="line"><span class="lineno">  178</span>        <span class="keywordflow">for</span> (<span class="keywordtype">int</span> i2 = 0; i2 &lt; y2; ++i2)</div>
<div class="line"><span class="lineno">  179</span>        {</div>
<div class="line"><span class="lineno">  180</span>          <span class="keywordflow">for</span> (<span class="keywordtype">int</span> i3 = 0; i3 &lt; y3; ++i3)</div>
<div class="line"><span class="lineno">  181</span>          {</div>
<div class="line"><span class="lineno">  182</span>            elementwise_f(y4, params, input1_data_ptr, input2_data_ptr, output_data_ptr);</div>
<div class="line"><span class="lineno">  183</span>            input2_data_ptr += y4;</div>
<div class="line"><span class="lineno">  184</span>            output_data_ptr += y4;</div>
<div class="line"><span class="lineno">  185</span>          }</div>
<div class="line"><span class="lineno">  186</span>          <span class="comment">// We have broadcast y4 of input1 data y3 times, and now move on.</span></div>
<div class="line"><span class="lineno">  187</span>          input1_data_ptr += y4;</div>
<div class="line"><span class="lineno">  188</span>        }</div>
<div class="line"><span class="lineno">  189</span>      }</div>
<div class="line"><span class="lineno">  190</span>      <span class="comment">// We have broadcast y2*y3*y4 of input2 data y1 times, and now move on.</span></div>
<div class="line"><span class="lineno">  191</span>      input2_data_reset = input2_data_ptr;</div>
<div class="line"><span class="lineno">  192</span>    }</div>
<div class="line"><span class="lineno">  193</span>  }</div>
<div class="line"><span class="lineno">  194</span>  <span class="keywordflow">else</span></div>
<div class="line"><span class="lineno">  195</span>  {</div>
<div class="line"><span class="lineno">  196</span>    <span class="comment">// Special case of y4 == 1, in which the innermost loop is a single</span></div>
<div class="line"><span class="lineno">  197</span>    <span class="comment">// element and can be combined with the next (y3) as an inner broadcast.</span></div>
<div class="line"><span class="lineno">  198</span>    <span class="comment">//</span></div>
<div class="line"><span class="lineno">  199</span>    <span class="comment">// Note that this handles the case of pure scalar broadcast when</span></div>
<div class="line"><span class="lineno">  200</span>    <span class="comment">// y0 == y1 == y2 == 1. With low overhead it handles cases such as scalar</span></div>
<div class="line"><span class="lineno">  201</span>    <span class="comment">// broadcast with batch (as y2 &gt; 1).</span></div>
<div class="line"><span class="lineno">  202</span>    <span class="comment">//</span></div>
<div class="line"><span class="lineno">  203</span>    <span class="comment">// NOTE The process is the same as the above general case except</span></div>
<div class="line"><span class="lineno">  204</span>    <span class="comment">// simplified for y4 == 1 and the loop over y3 is contained within the</span></div>
<div class="line"><span class="lineno">  205</span>    <span class="comment">// AddScalarBroadcast function.</span></div>
<div class="line"><span class="lineno">  206</span>    <span class="keywordflow">for</span> (<span class="keywordtype">int</span> i0 = 0; i0 &lt; y0; ++i0)</div>
<div class="line"><span class="lineno">  207</span>    {</div>
<div class="line"><span class="lineno">  208</span>      <span class="keyword">const</span> T *input2_data_ptr = <span class="keyword">nullptr</span>;</div>
<div class="line"><span class="lineno">  209</span>      <span class="keywordflow">for</span> (<span class="keywordtype">int</span> i1 = 0; i1 &lt; y1; ++i1)</div>
<div class="line"><span class="lineno">  210</span>      {</div>
<div class="line"><span class="lineno">  211</span>        input2_data_ptr = input2_data_reset;</div>
<div class="line"><span class="lineno">  212</span>        <span class="keywordflow">for</span> (<span class="keywordtype">int</span> i2 = 0; i2 &lt; y2; ++i2)</div>
<div class="line"><span class="lineno">  213</span>        {</div>
<div class="line"><span class="lineno">  214</span>          scalar_broadcast_f(y3, params, *input1_data_ptr, input2_data_ptr, output_data_ptr);</div>
<div class="line"><span class="lineno">  215</span>          input2_data_ptr += y3;</div>
<div class="line"><span class="lineno">  216</span>          output_data_ptr += y3;</div>
<div class="line"><span class="lineno">  217</span>          input1_data_ptr += 1;</div>
<div class="line"><span class="lineno">  218</span>        }</div>
<div class="line"><span class="lineno">  219</span>      }</div>
<div class="line"><span class="lineno">  220</span>      input2_data_reset = input2_data_ptr;</div>
<div class="line"><span class="lineno">  221</span>    }</div>
<div class="line"><span class="lineno">  222</span>  }</div>
<div class="line"><span class="lineno">  223</span>}</div>
<div class="ttc" id="astructnnfw_1_1cker_1_1_binary_arithmetic_op_param_html"><div class="ttname"><a href="structnnfw_1_1cker_1_1_binary_arithmetic_op_param.html">nnfw::cker::BinaryArithmeticOpParam</a></div><div class="ttdef"><b>Definition:</b> <a href="compute_2cker_2include_2cker_2_types_8h_source.html#l00172">Types.h:173</a></div></div>
<div class="ttc" id="astructnnfw_1_1cker_1_1_binary_arithmetic_op_param_html_aa40d8c9eb2a4cea35596c77395fae910"><div class="ttname"><a href="structnnfw_1_1cker_1_1_binary_arithmetic_op_param.html#aa40d8c9eb2a4cea35596c77395fae910">nnfw::cker::BinaryArithmeticOpParam::broadcast_category</a></div><div class="ttdeci">BroadcastableOpCategory broadcast_category</div><div class="ttdef"><b>Definition:</b> <a href="compute_2cker_2include_2cker_2_types_8h_source.html#l00175">Types.h:175</a></div></div>
</div><!-- fragment -->
<p class="reference">References <a class="el" href="compute_2cker_2include_2cker_2_types_8h_source.html#l00175">nnfw::cker::BinaryArithmeticOpParam::broadcast_category</a>, <a class="el" href="compute_2cker_2include_2cker_2_types_8h_source.html#l00204">nnfw::cker::BinaryArithmeticOpParam::broadcast_shape</a>, <a class="el" href="compute_2cker_2include_2cker_2_types_8h_source.html#l00184">nnfw::cker::BinaryArithmeticOpParam::input1_multiplier</a>, <a class="el" href="compute_2cker_2include_2cker_2_types_8h_source.html#l00177">nnfw::cker::BinaryArithmeticOpParam::input1_offset</a>, <a class="el" href="compute_2cker_2include_2cker_2_types_8h_source.html#l00185">nnfw::cker::BinaryArithmeticOpParam::input1_shift</a>, <a class="el" href="compute_2cker_2include_2cker_2_types_8h_source.html#l00186">nnfw::cker::BinaryArithmeticOpParam::input2_multiplier</a>, <a class="el" href="compute_2cker_2include_2cker_2_types_8h_source.html#l00178">nnfw::cker::BinaryArithmeticOpParam::input2_offset</a>, <a class="el" href="compute_2cker_2include_2cker_2_types_8h_source.html#l00187">nnfw::cker::BinaryArithmeticOpParam::input2_shift</a>, and <a class="el" href="namespacennfw_1_1cker.html#a098b43a9a087b975a86725a93009c82eacbb6717a21700840a8cc79eb660284d5">nnfw::cker::kFirstInputBroadcastsFast</a>.</p>

</div>
</div>
<a id="ac33aef7d437da144093878745494d694" name="ac33aef7d437da144093878745494d694"></a>
<h2 class="memtitle"><span class="permalink"><a href="#ac33aef7d437da144093878745494d694">&#9670;&#160;</a></span>BinaryOpElementwise()</h2>

<div class="memitem">
<div class="memproto">
<div class="memtemplate">
template&lt;class OPERATOR , class <a class="el" href="activation__float__helpers_8h.html#abbc420da5dec17216bb014c05ad65304">ACTIVATION</a> &gt; </div>
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">void nnfw::cker::optimized::BinaryOpElementwise </td>
          <td>(</td>
          <td class="paramtype">int&#160;</td>
          <td class="paramname"><em>size</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const <a class="el" href="structnnfw_1_1cker_1_1_binary_arithmetic_op_param.html">BinaryArithmeticOpParam</a> &amp;&#160;</td>
          <td class="paramname"><em>params</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const float *&#160;</td>
          <td class="paramname"><em>input1_data</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const float *&#160;</td>
          <td class="paramname"><em>input2_data</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">float *&#160;</td>
          <td class="paramname"><em>output_data</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">inline</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p class="definition">Definition at line <a class="el" href="optimized_2_binary_arithmetic_ops_8h_source.html#l00562">562</a> of file <a class="el" href="optimized_2_binary_arithmetic_ops_8h_source.html">BinaryArithmeticOps.h</a>.</p>
<div class="fragment"><div class="line"><span class="lineno">  565</span>{</div>
<div class="line"><span class="lineno">  566</span>  <span class="keywordtype">int</span> i = 0;</div>
<div class="line"><span class="lineno">  567</span> </div>
<div class="line"><span class="lineno">  568</span><span class="preprocessor">#ifdef USE_NEON</span></div>
<div class="line"><span class="lineno">  569</span>  <span class="keyword">const</span> <span class="keyword">auto</span> activation_min = vdupq_n_f32(params.<a class="code hl_variable" href="structnnfw_1_1cker_1_1_binary_arithmetic_op_param.html#af87d13f6f63a9d5eeb175c2db94bae92">float_activation_min</a>);</div>
<div class="line"><span class="lineno">  570</span>  <span class="keyword">const</span> <span class="keyword">auto</span> activation_max = vdupq_n_f32(params.<a class="code hl_variable" href="structnnfw_1_1cker_1_1_binary_arithmetic_op_param.html#ac8fdfaa8e3b43c341a0128857d3bcf20">float_activation_max</a>);</div>
<div class="line"><span class="lineno">  571</span>  <span class="keywordflow">for</span> (; i &lt;= size - 16; i += 16)</div>
<div class="line"><span class="lineno">  572</span>  {</div>
<div class="line"><span class="lineno">  573</span>    <span class="keyword">auto</span> a10 = vld1q_f32(input1_data + i);</div>
<div class="line"><span class="lineno">  574</span>    <span class="keyword">auto</span> a11 = vld1q_f32(input1_data + i + 4);</div>
<div class="line"><span class="lineno">  575</span>    <span class="keyword">auto</span> a12 = vld1q_f32(input1_data + i + 8);</div>
<div class="line"><span class="lineno">  576</span>    <span class="keyword">auto</span> a13 = vld1q_f32(input1_data + i + 12);</div>
<div class="line"><span class="lineno">  577</span>    <span class="keyword">auto</span> a20 = vld1q_f32(input2_data + i);</div>
<div class="line"><span class="lineno">  578</span>    <span class="keyword">auto</span> a21 = vld1q_f32(input2_data + i + 4);</div>
<div class="line"><span class="lineno">  579</span>    <span class="keyword">auto</span> a22 = vld1q_f32(input2_data + i + 8);</div>
<div class="line"><span class="lineno">  580</span>    <span class="keyword">auto</span> a23 = vld1q_f32(input2_data + i + 12);</div>
<div class="line"><span class="lineno">  581</span>    <span class="keyword">auto</span> x0 = OPERATOR::calculate(a10, a20);</div>
<div class="line"><span class="lineno">  582</span>    <span class="keyword">auto</span> x1 = OPERATOR::calculate(a11, a21);</div>
<div class="line"><span class="lineno">  583</span>    <span class="keyword">auto</span> x2 = OPERATOR::calculate(a12, a22);</div>
<div class="line"><span class="lineno">  584</span>    <span class="keyword">auto</span> x3 = OPERATOR::calculate(a13, a23);</div>
<div class="line"><span class="lineno">  585</span>    x0 = ACTIVATION::applyFloor(x0, activation_min);</div>
<div class="line"><span class="lineno">  586</span>    x1 = ACTIVATION::applyFloor(x1, activation_min);</div>
<div class="line"><span class="lineno">  587</span>    x2 = ACTIVATION::applyFloor(x2, activation_min);</div>
<div class="line"><span class="lineno">  588</span>    x3 = ACTIVATION::applyFloor(x3, activation_min);</div>
<div class="line"><span class="lineno">  589</span>    x0 = ACTIVATION::applyCeiling(x0, activation_max);</div>
<div class="line"><span class="lineno">  590</span>    x1 = ACTIVATION::applyCeiling(x1, activation_max);</div>
<div class="line"><span class="lineno">  591</span>    x2 = ACTIVATION::applyCeiling(x2, activation_max);</div>
<div class="line"><span class="lineno">  592</span>    x3 = ACTIVATION::applyCeiling(x3, activation_max);</div>
<div class="line"><span class="lineno">  593</span>    vst1q_f32(output_data + i, x0);</div>
<div class="line"><span class="lineno">  594</span>    vst1q_f32(output_data + i + 4, x1);</div>
<div class="line"><span class="lineno">  595</span>    vst1q_f32(output_data + i + 8, x2);</div>
<div class="line"><span class="lineno">  596</span>    vst1q_f32(output_data + i + 12, x3);</div>
<div class="line"><span class="lineno">  597</span>  }</div>
<div class="line"><span class="lineno">  598</span>  <span class="keywordflow">for</span> (; i &lt;= size - 4; i += 4)</div>
<div class="line"><span class="lineno">  599</span>  {</div>
<div class="line"><span class="lineno">  600</span>    <span class="keyword">auto</span> a1 = vld1q_f32(input1_data + i);</div>
<div class="line"><span class="lineno">  601</span>    <span class="keyword">auto</span> a2 = vld1q_f32(input2_data + i);</div>
<div class="line"><span class="lineno">  602</span>    <span class="keyword">auto</span> x = OPERATOR::calculate(a1, a2); <span class="comment">// vaddq</span></div>
<div class="line"><span class="lineno">  603</span>    <span class="keyword">auto</span> x_clamped =</div>
<div class="line"><span class="lineno">  604</span>      ACTIVATION::applyCeiling(ACTIVATION::applyFloor(x, activation_min), activation_max);</div>
<div class="line"><span class="lineno">  605</span>    vst1q_f32(output_data + i, x_clamped);</div>
<div class="line"><span class="lineno">  606</span>  }</div>
<div class="line"><span class="lineno">  607</span><span class="preprocessor">#endif </span><span class="comment">// USE_NEON</span></div>
<div class="line"><span class="lineno">  608</span>  <span class="keywordflow">for</span> (; i &lt; size; i++)</div>
<div class="line"><span class="lineno">  609</span>  {</div>
<div class="line"><span class="lineno">  610</span>    <span class="keyword">auto</span> x = OPERATOR::calculate(input1_data[i], input2_data[i]);</div>
<div class="line"><span class="lineno">  611</span>    output_data[i] = ACTIVATION::applyCeiling(</div>
<div class="line"><span class="lineno">  612</span>      ACTIVATION::applyFloor(x, params.<a class="code hl_variable" href="structnnfw_1_1cker_1_1_binary_arithmetic_op_param.html#af87d13f6f63a9d5eeb175c2db94bae92">float_activation_min</a>), params.<a class="code hl_variable" href="structnnfw_1_1cker_1_1_binary_arithmetic_op_param.html#ac8fdfaa8e3b43c341a0128857d3bcf20">float_activation_max</a>);</div>
<div class="line"><span class="lineno">  613</span>  }</div>
<div class="line"><span class="lineno">  614</span>}</div>
<div class="ttc" id="astructnnfw_1_1cker_1_1_binary_arithmetic_op_param_html_ac8fdfaa8e3b43c341a0128857d3bcf20"><div class="ttname"><a href="structnnfw_1_1cker_1_1_binary_arithmetic_op_param.html#ac8fdfaa8e3b43c341a0128857d3bcf20">nnfw::cker::BinaryArithmeticOpParam::float_activation_max</a></div><div class="ttdeci">float float_activation_max</div><div class="ttdef"><b>Definition:</b> <a href="compute_2cker_2include_2cker_2_types_8h_source.html#l00193">Types.h:193</a></div></div>
<div class="ttc" id="astructnnfw_1_1cker_1_1_binary_arithmetic_op_param_html_af87d13f6f63a9d5eeb175c2db94bae92"><div class="ttname"><a href="structnnfw_1_1cker_1_1_binary_arithmetic_op_param.html#af87d13f6f63a9d5eeb175c2db94bae92">nnfw::cker::BinaryArithmeticOpParam::float_activation_min</a></div><div class="ttdeci">float float_activation_min</div><div class="ttdef"><b>Definition:</b> <a href="compute_2cker_2include_2cker_2_types_8h_source.html#l00192">Types.h:192</a></div></div>
</div><!-- fragment -->
<p class="reference">References <a class="el" href="compute_2cker_2include_2cker_2_types_8h_source.html#l00193">nnfw::cker::BinaryArithmeticOpParam::float_activation_max</a>, and <a class="el" href="compute_2cker_2include_2cker_2_types_8h_source.html#l00192">nnfw::cker::BinaryArithmeticOpParam::float_activation_min</a>.</p>

</div>
</div>
<a id="a255edd47f2824083009d823d12646d1f" name="a255edd47f2824083009d823d12646d1f"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a255edd47f2824083009d823d12646d1f">&#9670;&#160;</a></span>BinaryOpScalarBroadcast()</h2>

<div class="memitem">
<div class="memproto">
<div class="memtemplate">
template&lt;class OPERATOR , class <a class="el" href="activation__float__helpers_8h.html#abbc420da5dec17216bb014c05ad65304">ACTIVATION</a> &gt; </div>
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">void nnfw::cker::optimized::BinaryOpScalarBroadcast </td>
          <td>(</td>
          <td class="paramtype">int&#160;</td>
          <td class="paramname"><em>size</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const <a class="el" href="structnnfw_1_1cker_1_1_binary_arithmetic_op_param.html">BinaryArithmeticOpParam</a> &amp;&#160;</td>
          <td class="paramname"><em>params</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const float&#160;</td>
          <td class="paramname"><em>broadcast_value</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const float *&#160;</td>
          <td class="paramname"><em>input2_data</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">float *&#160;</td>
          <td class="paramname"><em>output_data</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">inline</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p class="definition">Definition at line <a class="el" href="optimized_2_binary_arithmetic_ops_8h_source.html#l00620">620</a> of file <a class="el" href="optimized_2_binary_arithmetic_ops_8h_source.html">BinaryArithmeticOps.h</a>.</p>
<div class="fragment"><div class="line"><span class="lineno">  623</span>{</div>
<div class="line"><span class="lineno">  624</span>  <span class="keywordtype">int</span> i = 0;</div>
<div class="line"><span class="lineno">  625</span> </div>
<div class="line"><span class="lineno">  626</span><span class="preprocessor">#ifdef USE_NEON</span></div>
<div class="line"><span class="lineno">  627</span>  <span class="keyword">const</span> <span class="keyword">auto</span> activation_min = vdupq_n_f32(params.<a class="code hl_variable" href="structnnfw_1_1cker_1_1_binary_arithmetic_op_param.html#af87d13f6f63a9d5eeb175c2db94bae92">float_activation_min</a>);</div>
<div class="line"><span class="lineno">  628</span>  <span class="keyword">const</span> <span class="keyword">auto</span> activation_max = vdupq_n_f32(params.<a class="code hl_variable" href="structnnfw_1_1cker_1_1_binary_arithmetic_op_param.html#ac8fdfaa8e3b43c341a0128857d3bcf20">float_activation_max</a>);</div>
<div class="line"><span class="lineno">  629</span>  <span class="keyword">const</span> <span class="keyword">auto</span> broadcast_value_dup = vdupq_n_f32(broadcast_value);</div>
<div class="line"><span class="lineno">  630</span>  <span class="keywordflow">for</span> (; i &lt;= size - 16; i += 16)</div>
<div class="line"><span class="lineno">  631</span>  {</div>
<div class="line"><span class="lineno">  632</span>    <span class="keyword">auto</span> a20 = vld1q_f32(input2_data + i);</div>
<div class="line"><span class="lineno">  633</span>    <span class="keyword">auto</span> a21 = vld1q_f32(input2_data + i + 4);</div>
<div class="line"><span class="lineno">  634</span>    <span class="keyword">auto</span> a22 = vld1q_f32(input2_data + i + 8);</div>
<div class="line"><span class="lineno">  635</span>    <span class="keyword">auto</span> a23 = vld1q_f32(input2_data + i + 12);</div>
<div class="line"><span class="lineno">  636</span>    <span class="keyword">auto</span> x0 = OPERATOR::calculate(broadcast_value_dup, a20);</div>
<div class="line"><span class="lineno">  637</span>    <span class="keyword">auto</span> x1 = OPERATOR::calculate(broadcast_value_dup, a21);</div>
<div class="line"><span class="lineno">  638</span>    <span class="keyword">auto</span> x2 = OPERATOR::calculate(broadcast_value_dup, a22);</div>
<div class="line"><span class="lineno">  639</span>    <span class="keyword">auto</span> x3 = OPERATOR::calculate(broadcast_value_dup, a23);</div>
<div class="line"><span class="lineno">  640</span>    x0 = ACTIVATION::applyFloor(x0, activation_min);</div>
<div class="line"><span class="lineno">  641</span>    x1 = ACTIVATION::applyFloor(x1, activation_min);</div>
<div class="line"><span class="lineno">  642</span>    x2 = ACTIVATION::applyFloor(x2, activation_min);</div>
<div class="line"><span class="lineno">  643</span>    x3 = ACTIVATION::applyFloor(x3, activation_min);</div>
<div class="line"><span class="lineno">  644</span>    x0 = ACTIVATION::applyCeiling(x0, activation_max);</div>
<div class="line"><span class="lineno">  645</span>    x1 = ACTIVATION::applyCeiling(x1, activation_max);</div>
<div class="line"><span class="lineno">  646</span>    x2 = ACTIVATION::applyCeiling(x2, activation_max);</div>
<div class="line"><span class="lineno">  647</span>    x3 = ACTIVATION::applyCeiling(x3, activation_max);</div>
<div class="line"><span class="lineno">  648</span>    vst1q_f32(output_data + i, x0);</div>
<div class="line"><span class="lineno">  649</span>    vst1q_f32(output_data + i + 4, x1);</div>
<div class="line"><span class="lineno">  650</span>    vst1q_f32(output_data + i + 8, x2);</div>
<div class="line"><span class="lineno">  651</span>    vst1q_f32(output_data + i + 12, x3);</div>
<div class="line"><span class="lineno">  652</span>  }</div>
<div class="line"><span class="lineno">  653</span>  <span class="keywordflow">for</span> (; i &lt;= size - 4; i += 4)</div>
<div class="line"><span class="lineno">  654</span>  {</div>
<div class="line"><span class="lineno">  655</span>    <span class="keyword">auto</span> a2 = vld1q_f32(input2_data + i);</div>
<div class="line"><span class="lineno">  656</span>    <span class="keyword">auto</span> x = OPERATOR::calculate(broadcast_value_dup, a2);</div>
<div class="line"><span class="lineno">  657</span>    <span class="keyword">auto</span> x_clamped =</div>
<div class="line"><span class="lineno">  658</span>      ACTIVATION::applyCeiling(ACTIVATION::applyFloor(x, activation_min), activation_max);</div>
<div class="line"><span class="lineno">  659</span>    vst1q_f32(output_data + i, x_clamped);</div>
<div class="line"><span class="lineno">  660</span>  }</div>
<div class="line"><span class="lineno">  661</span><span class="preprocessor">#endif </span><span class="comment">// USE_NEON</span></div>
<div class="line"><span class="lineno">  662</span>  <span class="keywordflow">for</span> (; i &lt; size; i++)</div>
<div class="line"><span class="lineno">  663</span>  {</div>
<div class="line"><span class="lineno">  664</span>    <span class="keyword">auto</span> x = OPERATOR::calculate(broadcast_value, input2_data[i]);</div>
<div class="line"><span class="lineno">  665</span>    output_data[i] = ACTIVATION::applyCeiling(</div>
<div class="line"><span class="lineno">  666</span>      ACTIVATION::applyFloor(x, params.<a class="code hl_variable" href="structnnfw_1_1cker_1_1_binary_arithmetic_op_param.html#af87d13f6f63a9d5eeb175c2db94bae92">float_activation_min</a>), params.<a class="code hl_variable" href="structnnfw_1_1cker_1_1_binary_arithmetic_op_param.html#ac8fdfaa8e3b43c341a0128857d3bcf20">float_activation_max</a>);</div>
<div class="line"><span class="lineno">  667</span>  }</div>
<div class="line"><span class="lineno">  668</span>}</div>
</div><!-- fragment -->
<p class="reference">References <a class="el" href="compute_2cker_2include_2cker_2_types_8h_source.html#l00193">nnfw::cker::BinaryArithmeticOpParam::float_activation_max</a>, and <a class="el" href="compute_2cker_2include_2cker_2_types_8h_source.html#l00192">nnfw::cker::BinaryArithmeticOpParam::float_activation_min</a>.</p>

</div>
</div>
<a id="aba80b710dc4100dceaec38389b9aeaa3" name="aba80b710dc4100dceaec38389b9aeaa3"></a>
<h2 class="memtitle"><span class="permalink"><a href="#aba80b710dc4100dceaec38389b9aeaa3">&#9670;&#160;</a></span>BroadcastAddDispatch() <span class="overload">[1/2]</span></h2>

<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">void nnfw::cker::optimized::BroadcastAddDispatch </td>
          <td>(</td>
          <td class="paramtype">const <a class="el" href="structnnfw_1_1cker_1_1_binary_arithmetic_op_param.html">BinaryArithmeticOpParam</a> &amp;&#160;</td>
          <td class="paramname"><em>params</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const <a class="el" href="classnnfw_1_1cker_1_1_shape.html">Shape</a> &amp;&#160;</td>
          <td class="paramname"><em>input1_shape</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const float *&#160;</td>
          <td class="paramname"><em>input1_data</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const <a class="el" href="classnnfw_1_1cker_1_1_shape.html">Shape</a> &amp;&#160;</td>
          <td class="paramname"><em>input2_shape</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const float *&#160;</td>
          <td class="paramname"><em>input2_data</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const <a class="el" href="classnnfw_1_1cker_1_1_shape.html">Shape</a> &amp;&#160;</td>
          <td class="paramname"><em>output_shape</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">float *&#160;</td>
          <td class="paramname"><em>output_data</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">inline</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p class="definition">Definition at line <a class="el" href="optimized_2_binary_arithmetic_ops_8h_source.html#l00835">835</a> of file <a class="el" href="optimized_2_binary_arithmetic_ops_8h_source.html">BinaryArithmeticOps.h</a>.</p>
<div class="fragment"><div class="line"><span class="lineno">  839</span>{</div>
<div class="line"><span class="lineno">  840</span>  <span class="keywordflow">if</span> (params.<a class="code hl_variable" href="structnnfw_1_1cker_1_1_binary_arithmetic_op_param.html#aa40d8c9eb2a4cea35596c77395fae910">broadcast_category</a> == BroadcastableOpCategory::kGenericBroadcast)</div>
<div class="line"><span class="lineno">  841</span>  {</div>
<div class="line"><span class="lineno">  842</span>    <span class="keyword">const</span> std::function&lt;float(<span class="keyword">const</span> <span class="keywordtype">float</span> &amp;, <span class="keyword">const</span> <span class="keywordtype">float</span> &amp;)&gt; fn =</div>
<div class="line"><span class="lineno">  843</span>      [](<span class="keyword">const</span> <span class="keywordtype">float</span> &amp;a, <span class="keyword">const</span> <span class="keywordtype">float</span> &amp;b) -&gt; <span class="keywordtype">float</span> { <span class="keywordflow">return</span> a + b; };</div>
<div class="line"><span class="lineno">  844</span>    reference::BroadcastBinaryArithmeticOpSlow(params, input1_shape, input1_data, input2_shape,</div>
<div class="line"><span class="lineno">  845</span>                                               input2_data, output_shape, output_data, fn);</div>
<div class="line"><span class="lineno">  846</span>  }</div>
<div class="line"><span class="lineno">  847</span>  <span class="keywordflow">else</span></div>
<div class="line"><span class="lineno">  848</span>  {</div>
<div class="line"><span class="lineno">  849</span>    <span class="keyword">auto</span> implFuncs = getBinaryOpWithActivationImplFloat&lt;BinaryOpFuncAddFloat&gt;(params);</div>
<div class="line"><span class="lineno">  850</span> </div>
<div class="line"><span class="lineno">  851</span>    BinaryBroadcastFiveFold(</div>
<div class="line"><span class="lineno">  852</span>      params, params.<a class="code hl_variable" href="structnnfw_1_1cker_1_1_binary_arithmetic_op_param.html#aa40d8c9eb2a4cea35596c77395fae910">broadcast_category</a> == BroadcastableOpCategory::kSecondInputBroadcastsFast,</div>
<div class="line"><span class="lineno">  853</span>      input1_shape, input1_data, input2_shape, input2_data, output_shape, output_data,</div>
<div class="line"><span class="lineno">  854</span>      implFuncs.first, implFuncs.second);</div>
<div class="line"><span class="lineno">  855</span>  }</div>
<div class="line"><span class="lineno">  856</span>}</div>
</div><!-- fragment -->
<p class="reference">References <a class="el" href="optimized_2_binary_arithmetic_ops_8h_source.html#l00040">BinaryBroadcastFiveFold()</a>, <a class="el" href="compute_2cker_2include_2cker_2_types_8h_source.html#l00175">nnfw::cker::BinaryArithmeticOpParam::broadcast_category</a>, <a class="el" href="reference_2_binary_arithmetic_ops_8h_source.html#l00065">nnfw::cker::reference::BroadcastBinaryArithmeticOpSlow()</a>, <a class="el" href="namespacennfw_1_1cker.html#a098b43a9a087b975a86725a93009c82ea2b25dfc070122a240f4d1155b9cc6d55">nnfw::cker::kGenericBroadcast</a>, and <a class="el" href="namespacennfw_1_1cker.html#a098b43a9a087b975a86725a93009c82ea617cfcafa34e51f4c5f637e365f400c3">nnfw::cker::kSecondInputBroadcastsFast</a>.</p>

</div>
</div>
<a id="a28dc97b9306953b16c0572b361913edd" name="a28dc97b9306953b16c0572b361913edd"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a28dc97b9306953b16c0572b361913edd">&#9670;&#160;</a></span>BroadcastAddDispatch() <span class="overload">[2/2]</span></h2>

<div class="memitem">
<div class="memproto">
<div class="memtemplate">
template&lt;typename T &gt; </div>
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">std::enable_if_t&lt; <a class="el" href="structnnfw_1_1cker_1_1is__quant8.html">is_quant8</a>&lt; T &gt;::value &gt; nnfw::cker::optimized::BroadcastAddDispatch </td>
          <td>(</td>
          <td class="paramtype">const <a class="el" href="structnnfw_1_1cker_1_1_binary_arithmetic_op_param.html">BinaryArithmeticOpParam</a> &amp;&#160;</td>
          <td class="paramname"><em>params</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const <a class="el" href="classnnfw_1_1cker_1_1_shape.html">Shape</a> &amp;&#160;</td>
          <td class="paramname"><em>input1_shape</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const T *&#160;</td>
          <td class="paramname"><em>input1_data</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const <a class="el" href="classnnfw_1_1cker_1_1_shape.html">Shape</a> &amp;&#160;</td>
          <td class="paramname"><em>input2_shape</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const T *&#160;</td>
          <td class="paramname"><em>input2_data</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const <a class="el" href="classnnfw_1_1cker_1_1_shape.html">Shape</a> &amp;&#160;</td>
          <td class="paramname"><em>output_shape</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">T *&#160;</td>
          <td class="paramname"><em>output_data</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">inline</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p class="definition">Definition at line <a class="el" href="optimized_2_binary_arithmetic_ops_8h_source.html#l00812">812</a> of file <a class="el" href="optimized_2_binary_arithmetic_ops_8h_source.html">BinaryArithmeticOps.h</a>.</p>
<div class="fragment"><div class="line"><span class="lineno">  815</span>{</div>
<div class="line"><span class="lineno">  816</span>  <span class="keywordflow">if</span> (params.<a class="code hl_variable" href="structnnfw_1_1cker_1_1_binary_arithmetic_op_param.html#aa40d8c9eb2a4cea35596c77395fae910">broadcast_category</a> == BroadcastableOpCategory::kGenericBroadcast)</div>
<div class="line"><span class="lineno">  817</span>  {</div>
<div class="line"><span class="lineno">  818</span>    <span class="keyword">const</span> std::function&lt;T(<span class="keyword">const</span> <a class="code hl_struct" href="structnnfw_1_1cker_1_1_binary_arithmetic_op_param.html">BinaryArithmeticOpParam</a> &amp;, <span class="keyword">const</span> T &amp;, <span class="keyword">const</span> T &amp;)&gt; fn =</div>
<div class="line"><span class="lineno">  819</span>      [](<span class="keyword">const</span> <a class="code hl_struct" href="structnnfw_1_1cker_1_1_binary_arithmetic_op_param.html">BinaryArithmeticOpParam</a> &amp;params, <span class="keyword">const</span> T &amp;a, <span class="keyword">const</span> T &amp;b) {</div>
<div class="line"><span class="lineno">  820</span>        <span class="keywordflow">return</span> <span class="keyword">static_cast&lt;</span>T<span class="keyword">&gt;</span>(<a class="code hl_function" href="namespacennfw_1_1cker_1_1optimized.html#a1b089b3d4d74b15a41c88996227a23e3">quant8_sum</a>(params, a, b));</div>
<div class="line"><span class="lineno">  821</span>      };</div>
<div class="line"><span class="lineno">  822</span>    reference::BroadcastBinaryArithmeticOpSlow(params, input1_shape, input1_data, input2_shape,</div>
<div class="line"><span class="lineno">  823</span>                                               input2_data, output_shape, output_data, fn);</div>
<div class="line"><span class="lineno">  824</span>    <span class="keywordflow">return</span>;</div>
<div class="line"><span class="lineno">  825</span>  }</div>
<div class="line"><span class="lineno">  826</span> </div>
<div class="line"><span class="lineno">  827</span>  BinaryBroadcastFiveFold(</div>
<div class="line"><span class="lineno">  828</span>    params, input1_shape, input1_data, input2_shape, input2_data, output_shape, output_data,</div>
<div class="line"><span class="lineno">  829</span>    <span class="keyword">static_cast&lt;</span><span class="keywordtype">void</span> (*)(<span class="keywordtype">int</span>, <span class="keyword">const </span>BinaryArithmeticOpParam &amp;, <span class="keyword">const </span>T *, <span class="keyword">const </span>T *, T *)<span class="keyword">&gt;</span>(</div>
<div class="line"><span class="lineno">  830</span>      AddElementwise),</div>
<div class="line"><span class="lineno">  831</span>    <span class="keyword">static_cast&lt;</span><span class="keywordtype">void</span> (*)(<span class="keywordtype">int</span>, <span class="keyword">const </span>BinaryArithmeticOpParam &amp;, T, <span class="keyword">const </span>T *, T *)<span class="keyword">&gt;</span>(</div>
<div class="line"><span class="lineno">  832</span>      AddScalarBroadcast));</div>
<div class="line"><span class="lineno">  833</span>}</div>
</div><!-- fragment -->
<p class="reference">References <a class="el" href="optimized_2_binary_arithmetic_ops_8h_source.html#l00246">AddElementwise()</a>, <a class="el" href="optimized_2_binary_arithmetic_ops_8h_source.html#l00711">AddScalarBroadcast()</a>, <a class="el" href="optimized_2_binary_arithmetic_ops_8h_source.html#l00040">BinaryBroadcastFiveFold()</a>, <a class="el" href="compute_2cker_2include_2cker_2_types_8h_source.html#l00175">nnfw::cker::BinaryArithmeticOpParam::broadcast_category</a>, <a class="el" href="reference_2_binary_arithmetic_ops_8h_source.html#l00065">nnfw::cker::reference::BroadcastBinaryArithmeticOpSlow()</a>, <a class="el" href="namespacennfw_1_1cker.html#a098b43a9a087b975a86725a93009c82ea2b25dfc070122a240f4d1155b9cc6d55">nnfw::cker::kGenericBroadcast</a>, and <a class="el" href="optimized_2_binary_arithmetic_ops_8h_source.html#l00227">quant8_sum()</a>.</p>

<p class="reference">Referenced by <a class="el" href="_binary_arithmetic_ops_8h_source.html#l00271">nnfw::cker::BroadcastBinaryArithmeticOp()</a>.</p>

</div>
</div>
<a id="a0665493b253e34ef95e0686933cab6bc" name="a0665493b253e34ef95e0686933cab6bc"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a0665493b253e34ef95e0686933cab6bc">&#9670;&#160;</a></span>BroadcastDivDispatch()</h2>

<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">void nnfw::cker::optimized::BroadcastDivDispatch </td>
          <td>(</td>
          <td class="paramtype">const <a class="el" href="structnnfw_1_1cker_1_1_binary_arithmetic_op_param.html">BinaryArithmeticOpParam</a> &amp;&#160;</td>
          <td class="paramname"><em>params</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const <a class="el" href="classnnfw_1_1cker_1_1_shape.html">Shape</a> &amp;&#160;</td>
          <td class="paramname"><em>input1_shape</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const float *&#160;</td>
          <td class="paramname"><em>input1_data</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const <a class="el" href="classnnfw_1_1cker_1_1_shape.html">Shape</a> &amp;&#160;</td>
          <td class="paramname"><em>input2_shape</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const float *&#160;</td>
          <td class="paramname"><em>input2_data</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const <a class="el" href="classnnfw_1_1cker_1_1_shape.html">Shape</a> &amp;&#160;</td>
          <td class="paramname"><em>output_shape</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">float *&#160;</td>
          <td class="paramname"><em>output_data</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">inline</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p class="definition">Definition at line <a class="el" href="optimized_2_binary_arithmetic_ops_8h_source.html#l01234">1234</a> of file <a class="el" href="optimized_2_binary_arithmetic_ops_8h_source.html">BinaryArithmeticOps.h</a>.</p>
<div class="fragment"><div class="line"><span class="lineno"> 1238</span>{</div>
<div class="line"><span class="lineno"> 1239</span><span class="preprocessor">#ifdef __aarch64__</span></div>
<div class="line"><span class="lineno"> 1240</span>  <span class="keywordflow">if</span> (params.<a class="code hl_variable" href="structnnfw_1_1cker_1_1_binary_arithmetic_op_param.html#aa40d8c9eb2a4cea35596c77395fae910">broadcast_category</a> == BroadcastableOpCategory::kFirstInputBroadcastsFast)</div>
<div class="line"><span class="lineno"> 1241</span>  {</div>
<div class="line"><span class="lineno"> 1242</span>    <span class="keyword">auto</span> implFuncs = getBinaryOpWithActivationImplFloat&lt;BinaryOpFuncDivFloat&gt;(params);</div>
<div class="line"><span class="lineno"> 1243</span>    <a class="code hl_function" href="namespacennfw_1_1cker_1_1optimized.html#acc41c0b6a568f6a484fbfc9618e0a055">BinaryBroadcastFiveFold</a>(params, <span class="keyword">false</span>, input1_shape, input1_data, input2_shape, input2_data,</div>
<div class="line"><span class="lineno"> 1244</span>                            output_shape, output_data, implFuncs.first, implFuncs.second);</div>
<div class="line"><span class="lineno"> 1245</span>  }</div>
<div class="line"><span class="lineno"> 1246</span>  <span class="keywordflow">else</span> <span class="keywordflow">if</span> (params.<a class="code hl_variable" href="structnnfw_1_1cker_1_1_binary_arithmetic_op_param.html#aa40d8c9eb2a4cea35596c77395fae910">broadcast_category</a> == BroadcastableOpCategory::kSecondInputBroadcastsFast)</div>
<div class="line"><span class="lineno"> 1247</span>  {</div>
<div class="line"><span class="lineno"> 1248</span>    <span class="keyword">auto</span> implFuncs =</div>
<div class="line"><span class="lineno"> 1249</span>      getBinaryOpWithActivationImplFloat&lt;BinaryOpFuncSwapArgs&lt;BinaryOpFuncDivFloat&gt;&gt;(params);</div>
<div class="line"><span class="lineno"> 1250</span>    BinaryBroadcastFiveFold(params, <span class="keyword">true</span>, input1_shape, input1_data, input2_shape, input2_data,</div>
<div class="line"><span class="lineno"> 1251</span>                            output_shape, output_data, implFuncs.first, implFuncs.second);</div>
<div class="line"><span class="lineno"> 1252</span>  }</div>
<div class="line"><span class="lineno"> 1253</span>  <span class="keywordflow">else</span></div>
<div class="line"><span class="lineno"> 1254</span><span class="preprocessor">#endif </span><span class="comment">// __aarch64__</span></div>
<div class="line"><span class="lineno"> 1255</span>  {</div>
<div class="line"><span class="lineno"> 1256</span>    <span class="keyword">const</span> std::function&lt;float(<span class="keyword">const</span> <span class="keywordtype">float</span> &amp;, <span class="keyword">const</span> <span class="keywordtype">float</span> &amp;)&gt; fn =</div>
<div class="line"><span class="lineno"> 1257</span>      [](<span class="keyword">const</span> <span class="keywordtype">float</span> &amp;a, <span class="keyword">const</span> <span class="keywordtype">float</span> &amp;b) -&gt; <span class="keywordtype">float</span> { <span class="keywordflow">return</span> a / b; };</div>
<div class="line"><span class="lineno"> 1258</span>    reference::BroadcastBinaryArithmeticOpSlow(params, input1_shape, input1_data, input2_shape,</div>
<div class="line"><span class="lineno"> 1259</span>                                               input2_data, output_shape, output_data, fn);</div>
<div class="line"><span class="lineno"> 1260</span>  }</div>
<div class="line"><span class="lineno"> 1261</span>}</div>
<div class="ttc" id="anamespacennfw_1_1cker_1_1optimized_html_acc41c0b6a568f6a484fbfc9618e0a055"><div class="ttname"><a href="namespacennfw_1_1cker_1_1optimized.html#acc41c0b6a568f6a484fbfc9618e0a055">nnfw::cker::optimized::BinaryBroadcastFiveFold</a></div><div class="ttdeci">void BinaryBroadcastFiveFold(const BinaryArithmeticOpParam &amp;params, bool switch_inputs, const Shape &amp;, const T *unswitched_input1_data, const Shape &amp;, const T *unswitched_input2_data, const Shape &amp;, T *output_data, ElementwiseF elementwise_f, ScalarBroadcastF scalar_broadcast_f)</div><div class="ttdef"><b>Definition:</b> <a href="optimized_2_binary_arithmetic_ops_8h_source.html#l00040">BinaryArithmeticOps.h:40</a></div></div>
</div><!-- fragment -->
<p class="reference">References <a class="el" href="optimized_2_binary_arithmetic_ops_8h_source.html#l00040">BinaryBroadcastFiveFold()</a>, <a class="el" href="compute_2cker_2include_2cker_2_types_8h_source.html#l00175">nnfw::cker::BinaryArithmeticOpParam::broadcast_category</a>, <a class="el" href="reference_2_binary_arithmetic_ops_8h_source.html#l00065">nnfw::cker::reference::BroadcastBinaryArithmeticOpSlow()</a>, <a class="el" href="namespacennfw_1_1cker.html#a098b43a9a087b975a86725a93009c82eacbb6717a21700840a8cc79eb660284d5">nnfw::cker::kFirstInputBroadcastsFast</a>, and <a class="el" href="namespacennfw_1_1cker.html#a098b43a9a087b975a86725a93009c82ea617cfcafa34e51f4c5f637e365f400c3">nnfw::cker::kSecondInputBroadcastsFast</a>.</p>

<p class="reference">Referenced by <a class="el" href="_binary_arithmetic_ops_8h_source.html#l00296">nnfw::cker::BroadcastBinaryArithmeticOp()</a>.</p>

</div>
</div>
<a id="a30d2c6537369842372153972d702867c" name="a30d2c6537369842372153972d702867c"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a30d2c6537369842372153972d702867c">&#9670;&#160;</a></span>BroadcastMulDispatch() <span class="overload">[1/2]</span></h2>

<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">void nnfw::cker::optimized::BroadcastMulDispatch </td>
          <td>(</td>
          <td class="paramtype">const <a class="el" href="structnnfw_1_1cker_1_1_binary_arithmetic_op_param.html">BinaryArithmeticOpParam</a> &amp;&#160;</td>
          <td class="paramname"><em>params</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const <a class="el" href="classnnfw_1_1cker_1_1_shape.html">Shape</a> &amp;&#160;</td>
          <td class="paramname"><em>input1_shape</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const float *&#160;</td>
          <td class="paramname"><em>input1_data</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const <a class="el" href="classnnfw_1_1cker_1_1_shape.html">Shape</a> &amp;&#160;</td>
          <td class="paramname"><em>input2_shape</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const float *&#160;</td>
          <td class="paramname"><em>input2_data</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const <a class="el" href="classnnfw_1_1cker_1_1_shape.html">Shape</a> &amp;&#160;</td>
          <td class="paramname"><em>output_shape</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">float *&#160;</td>
          <td class="paramname"><em>output_data</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">inline</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p class="definition">Definition at line <a class="el" href="optimized_2_binary_arithmetic_ops_8h_source.html#l01199">1199</a> of file <a class="el" href="optimized_2_binary_arithmetic_ops_8h_source.html">BinaryArithmeticOps.h</a>.</p>
<div class="fragment"><div class="line"><span class="lineno"> 1203</span>{</div>
<div class="line"><span class="lineno"> 1204</span>  <span class="keywordflow">if</span> (params.<a class="code hl_variable" href="structnnfw_1_1cker_1_1_binary_arithmetic_op_param.html#aa40d8c9eb2a4cea35596c77395fae910">broadcast_category</a> == BroadcastableOpCategory::kGenericBroadcast)</div>
<div class="line"><span class="lineno"> 1205</span>  {</div>
<div class="line"><span class="lineno"> 1206</span>    <span class="comment">// TODO: Use GetBinaryArithmeticFn</span></div>
<div class="line"><span class="lineno"> 1207</span>    <span class="keyword">const</span> std::function&lt;float(<span class="keyword">const</span> <span class="keywordtype">float</span> &amp;, <span class="keyword">const</span> <span class="keywordtype">float</span> &amp;)&gt; fn =</div>
<div class="line"><span class="lineno"> 1208</span>      [](<span class="keyword">const</span> <span class="keywordtype">float</span> &amp;a, <span class="keyword">const</span> <span class="keywordtype">float</span> &amp;b) -&gt; <span class="keywordtype">float</span> { <span class="keywordflow">return</span> a * b; };</div>
<div class="line"><span class="lineno"> 1209</span>    reference::BroadcastBinaryArithmeticOpSlow(params, input1_shape, input1_data, input2_shape,</div>
<div class="line"><span class="lineno"> 1210</span>                                               input2_data, output_shape, output_data, fn);</div>
<div class="line"><span class="lineno"> 1211</span>    <span class="keywordflow">return</span>;</div>
<div class="line"><span class="lineno"> 1212</span>  }</div>
<div class="line"><span class="lineno"> 1213</span>  <span class="keyword">auto</span> implFuncs = getBinaryOpWithActivationImplFloat&lt;BinaryOpFuncMulFloat&gt;(params);</div>
<div class="line"><span class="lineno"> 1214</span>  BinaryBroadcastFiveFold(params, input1_shape, input1_data, input2_shape, input2_data,</div>
<div class="line"><span class="lineno"> 1215</span>                          output_shape, output_data, implFuncs.first, implFuncs.second);</div>
<div class="line"><span class="lineno"> 1216</span>}</div>
</div><!-- fragment -->
<p class="reference">References <a class="el" href="optimized_2_binary_arithmetic_ops_8h_source.html#l00040">BinaryBroadcastFiveFold()</a>, <a class="el" href="compute_2cker_2include_2cker_2_types_8h_source.html#l00175">nnfw::cker::BinaryArithmeticOpParam::broadcast_category</a>, <a class="el" href="reference_2_binary_arithmetic_ops_8h_source.html#l00065">nnfw::cker::reference::BroadcastBinaryArithmeticOpSlow()</a>, and <a class="el" href="namespacennfw_1_1cker.html#a098b43a9a087b975a86725a93009c82ea2b25dfc070122a240f4d1155b9cc6d55">nnfw::cker::kGenericBroadcast</a>.</p>

</div>
</div>
<a id="a5b494d6113f18ea2a286dda9dcc06b5a" name="a5b494d6113f18ea2a286dda9dcc06b5a"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a5b494d6113f18ea2a286dda9dcc06b5a">&#9670;&#160;</a></span>BroadcastMulDispatch() <span class="overload">[2/2]</span></h2>

<div class="memitem">
<div class="memproto">
<div class="memtemplate">
template&lt;typename T &gt; </div>
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">std::enable_if_t&lt; <a class="el" href="structnnfw_1_1cker_1_1is__quant8.html">is_quant8</a>&lt; T &gt;::value &gt; nnfw::cker::optimized::BroadcastMulDispatch </td>
          <td>(</td>
          <td class="paramtype">const <a class="el" href="structnnfw_1_1cker_1_1_binary_arithmetic_op_param.html">BinaryArithmeticOpParam</a> &amp;&#160;</td>
          <td class="paramname"><em>params</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const <a class="el" href="classnnfw_1_1cker_1_1_shape.html">Shape</a> &amp;&#160;</td>
          <td class="paramname"><em>input1_shape</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const T *&#160;</td>
          <td class="paramname"><em>input1_data</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const <a class="el" href="classnnfw_1_1cker_1_1_shape.html">Shape</a> &amp;&#160;</td>
          <td class="paramname"><em>input2_shape</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const T *&#160;</td>
          <td class="paramname"><em>input2_data</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const <a class="el" href="classnnfw_1_1cker_1_1_shape.html">Shape</a> &amp;&#160;</td>
          <td class="paramname"><em>output_shape</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">T *&#160;</td>
          <td class="paramname"><em>output_data</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">inline</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p class="definition">Definition at line <a class="el" href="optimized_2_binary_arithmetic_ops_8h_source.html#l01177">1177</a> of file <a class="el" href="optimized_2_binary_arithmetic_ops_8h_source.html">BinaryArithmeticOps.h</a>.</p>
<div class="fragment"><div class="line"><span class="lineno"> 1180</span>{</div>
<div class="line"><span class="lineno"> 1181</span>  <span class="keywordflow">if</span> (params.<a class="code hl_variable" href="structnnfw_1_1cker_1_1_binary_arithmetic_op_param.html#aa40d8c9eb2a4cea35596c77395fae910">broadcast_category</a> == BroadcastableOpCategory::kGenericBroadcast)</div>
<div class="line"><span class="lineno"> 1182</span>  {</div>
<div class="line"><span class="lineno"> 1183</span>    <span class="keyword">const</span> std::function&lt;T(<span class="keyword">const</span> <a class="code hl_struct" href="structnnfw_1_1cker_1_1_binary_arithmetic_op_param.html">BinaryArithmeticOpParam</a> &amp;, <span class="keyword">const</span> T &amp;, <span class="keyword">const</span> T &amp;)&gt; fn =</div>
<div class="line"><span class="lineno"> 1184</span>      [](<span class="keyword">const</span> <a class="code hl_struct" href="structnnfw_1_1cker_1_1_binary_arithmetic_op_param.html">BinaryArithmeticOpParam</a> &amp;params, <span class="keyword">const</span> T &amp;a, <span class="keyword">const</span> T &amp;b) {</div>
<div class="line"><span class="lineno"> 1185</span>        <span class="keywordflow">return</span> <span class="keyword">static_cast&lt;</span>T<span class="keyword">&gt;</span>(<a class="code hl_function" href="namespacennfw_1_1cker_1_1optimized.html#a19f1d857236b4b11c737cb4fe7cc7567">quant8_mul</a>(params, a, b));</div>
<div class="line"><span class="lineno"> 1186</span>      };</div>
<div class="line"><span class="lineno"> 1187</span>    reference::BroadcastBinaryArithmeticOpSlow(params, input1_shape, input1_data, input2_shape,</div>
<div class="line"><span class="lineno"> 1188</span>                                               input2_data, output_shape, output_data, fn);</div>
<div class="line"><span class="lineno"> 1189</span>    <span class="keywordflow">return</span>;</div>
<div class="line"><span class="lineno"> 1190</span>  }</div>
<div class="line"><span class="lineno"> 1191</span>  BinaryBroadcastFiveFold(</div>
<div class="line"><span class="lineno"> 1192</span>    params, input1_shape, input1_data, input2_shape, input2_data, output_shape, output_data,</div>
<div class="line"><span class="lineno"> 1193</span>    <span class="keyword">static_cast&lt;</span><span class="keywordtype">void</span> (*)(<span class="keywordtype">int</span>, <span class="keyword">const </span>BinaryArithmeticOpParam &amp;, <span class="keyword">const </span>T *, <span class="keyword">const </span>T *, T *)<span class="keyword">&gt;</span>(</div>
<div class="line"><span class="lineno"> 1194</span>      MulElementwise),</div>
<div class="line"><span class="lineno"> 1195</span>    <span class="keyword">static_cast&lt;</span><span class="keywordtype">void</span> (*)(<span class="keywordtype">int</span>, <span class="keyword">const </span>BinaryArithmeticOpParam &amp;, T, <span class="keyword">const </span>T *, T *)<span class="keyword">&gt;</span>(</div>
<div class="line"><span class="lineno"> 1196</span>      MulSimpleBroadcast));</div>
<div class="line"><span class="lineno"> 1197</span>}</div>
<div class="ttc" id="anamespacennfw_1_1cker_1_1optimized_html_a19f1d857236b4b11c737cb4fe7cc7567"><div class="ttname"><a href="namespacennfw_1_1cker_1_1optimized.html#a19f1d857236b4b11c737cb4fe7cc7567">nnfw::cker::optimized::quant8_mul</a></div><div class="ttdeci">std::enable_if_t&lt; is_quant8&lt; T &gt;::value, int32_t &gt; quant8_mul(const BinaryArithmeticOpParam &amp;params, const T input1_data, const T input2_data)</div><div class="ttdef"><b>Definition:</b> <a href="optimized_2_binary_arithmetic_ops_8h_source.html#l00896">BinaryArithmeticOps.h:896</a></div></div>
</div><!-- fragment -->
<p class="reference">References <a class="el" href="optimized_2_binary_arithmetic_ops_8h_source.html#l00040">BinaryBroadcastFiveFold()</a>, <a class="el" href="compute_2cker_2include_2cker_2_types_8h_source.html#l00175">nnfw::cker::BinaryArithmeticOpParam::broadcast_category</a>, <a class="el" href="reference_2_binary_arithmetic_ops_8h_source.html#l00065">nnfw::cker::reference::BroadcastBinaryArithmeticOpSlow()</a>, <a class="el" href="namespacennfw_1_1cker.html#a098b43a9a087b975a86725a93009c82ea2b25dfc070122a240f4d1155b9cc6d55">nnfw::cker::kGenericBroadcast</a>, <a class="el" href="optimized_2_binary_arithmetic_ops_8h_source.html#l00910">MulElementwise()</a>, <a class="el" href="optimized_2_binary_arithmetic_ops_8h_source.html#l01081">MulSimpleBroadcast()</a>, and <a class="el" href="optimized_2_binary_arithmetic_ops_8h_source.html#l00896">quant8_mul()</a>.</p>

<p class="reference">Referenced by <a class="el" href="_binary_arithmetic_ops_8h_source.html#l00271">nnfw::cker::BroadcastBinaryArithmeticOp()</a>.</p>

</div>
</div>
<a id="ab6b4d976d595839e9c5c829c07f82de2" name="ab6b4d976d595839e9c5c829c07f82de2"></a>
<h2 class="memtitle"><span class="permalink"><a href="#ab6b4d976d595839e9c5c829c07f82de2">&#9670;&#160;</a></span>BroadcastSubDispatch()</h2>

<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">void nnfw::cker::optimized::BroadcastSubDispatch </td>
          <td>(</td>
          <td class="paramtype">const <a class="el" href="structnnfw_1_1cker_1_1_binary_arithmetic_op_param.html">BinaryArithmeticOpParam</a> &amp;&#160;</td>
          <td class="paramname"><em>params</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const <a class="el" href="classnnfw_1_1cker_1_1_shape.html">Shape</a> &amp;&#160;</td>
          <td class="paramname"><em>input1_shape</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const float *&#160;</td>
          <td class="paramname"><em>input1_data</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const <a class="el" href="classnnfw_1_1cker_1_1_shape.html">Shape</a> &amp;&#160;</td>
          <td class="paramname"><em>input2_shape</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const float *&#160;</td>
          <td class="paramname"><em>input2_data</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const <a class="el" href="classnnfw_1_1cker_1_1_shape.html">Shape</a> &amp;&#160;</td>
          <td class="paramname"><em>output_shape</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">float *&#160;</td>
          <td class="paramname"><em>output_data</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">inline</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p class="definition">Definition at line <a class="el" href="optimized_2_binary_arithmetic_ops_8h_source.html#l00867">867</a> of file <a class="el" href="optimized_2_binary_arithmetic_ops_8h_source.html">BinaryArithmeticOps.h</a>.</p>
<div class="fragment"><div class="line"><span class="lineno">  871</span>{</div>
<div class="line"><span class="lineno">  872</span>  <span class="keywordflow">if</span> (params.<a class="code hl_variable" href="structnnfw_1_1cker_1_1_binary_arithmetic_op_param.html#aa40d8c9eb2a4cea35596c77395fae910">broadcast_category</a> == BroadcastableOpCategory::kFirstInputBroadcastsFast)</div>
<div class="line"><span class="lineno">  873</span>  {</div>
<div class="line"><span class="lineno">  874</span>    <span class="keyword">auto</span> implFuncs = getBinaryOpWithActivationImplFloat&lt;BinaryOpFuncSubFloat&gt;(params);</div>
<div class="line"><span class="lineno">  875</span>    <a class="code hl_function" href="namespacennfw_1_1cker_1_1optimized.html#acc41c0b6a568f6a484fbfc9618e0a055">BinaryBroadcastFiveFold</a>(params, <span class="keyword">false</span>, input1_shape, input1_data, input2_shape, input2_data,</div>
<div class="line"><span class="lineno">  876</span>                            output_shape, output_data, implFuncs.first, implFuncs.second);</div>
<div class="line"><span class="lineno">  877</span>  }</div>
<div class="line"><span class="lineno">  878</span>  <span class="keywordflow">else</span> <span class="keywordflow">if</span> (params.<a class="code hl_variable" href="structnnfw_1_1cker_1_1_binary_arithmetic_op_param.html#aa40d8c9eb2a4cea35596c77395fae910">broadcast_category</a> == BroadcastableOpCategory::kSecondInputBroadcastsFast)</div>
<div class="line"><span class="lineno">  879</span>  {</div>
<div class="line"><span class="lineno">  880</span>    <span class="keyword">auto</span> implFuncs =</div>
<div class="line"><span class="lineno">  881</span>      getBinaryOpWithActivationImplFloat&lt;BinaryOpFuncSwapArgs&lt;BinaryOpFuncSubFloat&gt;&gt;(params);</div>
<div class="line"><span class="lineno">  882</span>    BinaryBroadcastFiveFold(params, <span class="keyword">true</span>, input1_shape, input1_data, input2_shape, input2_data,</div>
<div class="line"><span class="lineno">  883</span>                            output_shape, output_data, implFuncs.first, implFuncs.second);</div>
<div class="line"><span class="lineno">  884</span>  }</div>
<div class="line"><span class="lineno">  885</span>  <span class="keywordflow">else</span></div>
<div class="line"><span class="lineno">  886</span>  {</div>
<div class="line"><span class="lineno">  887</span>    <span class="keyword">const</span> std::function&lt;float(<span class="keyword">const</span> <span class="keywordtype">float</span> &amp;, <span class="keyword">const</span> <span class="keywordtype">float</span> &amp;)&gt; fn =</div>
<div class="line"><span class="lineno">  888</span>      [](<span class="keyword">const</span> <span class="keywordtype">float</span> &amp;a, <span class="keyword">const</span> <span class="keywordtype">float</span> &amp;b) -&gt; <span class="keywordtype">float</span> { <span class="keywordflow">return</span> a - b; };</div>
<div class="line"><span class="lineno">  889</span>    reference::BroadcastBinaryArithmeticOpSlow(params, input1_shape, input1_data, input2_shape,</div>
<div class="line"><span class="lineno">  890</span>                                               input2_data, output_shape, output_data, fn);</div>
<div class="line"><span class="lineno">  891</span>  }</div>
<div class="line"><span class="lineno">  892</span>}</div>
</div><!-- fragment -->
<p class="reference">References <a class="el" href="optimized_2_binary_arithmetic_ops_8h_source.html#l00040">BinaryBroadcastFiveFold()</a>, <a class="el" href="compute_2cker_2include_2cker_2_types_8h_source.html#l00175">nnfw::cker::BinaryArithmeticOpParam::broadcast_category</a>, <a class="el" href="reference_2_binary_arithmetic_ops_8h_source.html#l00065">nnfw::cker::reference::BroadcastBinaryArithmeticOpSlow()</a>, <a class="el" href="namespacennfw_1_1cker.html#a098b43a9a087b975a86725a93009c82eacbb6717a21700840a8cc79eb660284d5">nnfw::cker::kFirstInputBroadcastsFast</a>, and <a class="el" href="namespacennfw_1_1cker.html#a098b43a9a087b975a86725a93009c82ea617cfcafa34e51f4c5f637e365f400c3">nnfw::cker::kSecondInputBroadcastsFast</a>.</p>

<p class="reference">Referenced by <a class="el" href="_binary_arithmetic_ops_8h_source.html#l00296">nnfw::cker::BroadcastBinaryArithmeticOp()</a>.</p>

</div>
</div>
<a id="a201402ddf5f904cc04577124681cf526" name="a201402ddf5f904cc04577124681cf526"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a201402ddf5f904cc04577124681cf526">&#9670;&#160;</a></span>Conv()</h2>

<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">void nnfw::cker::optimized::Conv </td>
          <td>(</td>
          <td class="paramtype">const <a class="el" href="structnnfw_1_1cker_1_1_conv_params.html">ConvParams</a> &amp;&#160;</td>
          <td class="paramname"><em>params</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const <a class="el" href="classnnfw_1_1cker_1_1_shape.html">Shape</a> &amp;&#160;</td>
          <td class="paramname"><em>input_shape</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const uint8_t *&#160;</td>
          <td class="paramname"><em>input_data</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const <a class="el" href="classnnfw_1_1cker_1_1_shape.html">Shape</a> &amp;&#160;</td>
          <td class="paramname"><em>filter_shape</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const uint8_t *&#160;</td>
          <td class="paramname"><em>filter_data</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const <a class="el" href="classnnfw_1_1cker_1_1_shape.html">Shape</a> &amp;&#160;</td>
          <td class="paramname"><em>bias_shape</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const int32_t *&#160;</td>
          <td class="paramname"><em>bias_data</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const <a class="el" href="classnnfw_1_1cker_1_1_shape.html">Shape</a> &amp;&#160;</td>
          <td class="paramname"><em>output_shape</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">uint8_t *&#160;</td>
          <td class="paramname"><em>output_data</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const <a class="el" href="classnnfw_1_1cker_1_1_shape.html">Shape</a> &amp;&#160;</td>
          <td class="paramname"><em>im2col_shape</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">uint8_t *&#160;</td>
          <td class="paramname"><em>im2col_data</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">inline</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p class="definition">Definition at line <a class="el" href="compute_2cker_2include_2cker_2operation_2optimized_2_conv_8h_source.html#l00083">83</a> of file <a class="el" href="compute_2cker_2include_2cker_2operation_2optimized_2_conv_8h_source.html">Conv.h</a>.</p>
<div class="fragment"><div class="line"><span class="lineno">   87</span>{</div>
<div class="line"><span class="lineno">   88</span>  gemmlowp::GemmContext *gemm_context = gemm_support::GetGemmLowpContext();</div>
<div class="line"><span class="lineno">   89</span> </div>
<div class="line"><span class="lineno">   90</span>  <span class="keyword">const</span> <span class="keywordtype">int</span> stride_width = params.<a class="code hl_variable" href="structnnfw_1_1cker_1_1_conv_params.html#a273ecef205d663377502d9dcaa77f1b4">stride_width</a>;</div>
<div class="line"><span class="lineno">   91</span>  <span class="keyword">const</span> <span class="keywordtype">int</span> stride_height = params.<a class="code hl_variable" href="structnnfw_1_1cker_1_1_conv_params.html#a0878ee74c474c8a82a574727a2a3048e">stride_height</a>;</div>
<div class="line"><span class="lineno">   92</span>  <span class="keyword">const</span> <span class="keywordtype">int</span> dilation_width_factor = params.<a class="code hl_variable" href="structnnfw_1_1cker_1_1_conv_params.html#a8ff1e952b2e7627e11b47a22da0883cb">dilation_width_factor</a>;</div>
<div class="line"><span class="lineno">   93</span>  <span class="keyword">const</span> <span class="keywordtype">int</span> dilation_height_factor = params.<a class="code hl_variable" href="structnnfw_1_1cker_1_1_conv_params.html#ad47b94004b5b1718979775edaa9eee70">dilation_height_factor</a>;</div>
<div class="line"><span class="lineno">   94</span>  <span class="keyword">const</span> int32_t input_offset = params.<a class="code hl_variable" href="structnnfw_1_1cker_1_1_conv_params.html#a435e0fcf2622a161fe913f6098b2dd1e">input_offset</a>;</div>
<div class="line"><span class="lineno">   95</span>  <span class="keyword">const</span> int32_t filter_offset = params.<a class="code hl_variable" href="structnnfw_1_1cker_1_1_conv_params.html#a7a4237fa84247ed20ecb16119e1ec589">weights_offset</a>;</div>
<div class="line"><span class="lineno">   96</span>  <span class="keyword">const</span> int32_t output_offset = params.<a class="code hl_variable" href="structnnfw_1_1cker_1_1_conv_params.html#a7fef4d3c76b386c35ec04adfd931f991">output_offset</a>;</div>
<div class="line"><span class="lineno">   97</span>  <span class="keyword">const</span> int32_t output_multiplier = params.<a class="code hl_variable" href="structnnfw_1_1cker_1_1_conv_params.html#a636e1132f40731ba32f9060ae7f4dd44">output_multiplier</a>;</div>
<div class="line"><span class="lineno">   98</span>  <span class="keyword">const</span> <span class="keywordtype">int</span> output_shift = params.<a class="code hl_variable" href="structnnfw_1_1cker_1_1_conv_params.html#a1b36fc4cd87255a5e67218c4504db9b9">output_shift</a>;</div>
<div class="line"><span class="lineno">   99</span>  <span class="keyword">const</span> int32_t output_activation_min = params.<a class="code hl_variable" href="structnnfw_1_1cker_1_1_conv_params.html#ad4f74d8bfaa34bd1f44a690e7b77fc5f">quantized_activation_min</a>;</div>
<div class="line"><span class="lineno">  100</span>  <span class="keyword">const</span> int32_t output_activation_max = params.<a class="code hl_variable" href="structnnfw_1_1cker_1_1_conv_params.html#aadb8981e614d4834abc8318f406c5462">quantized_activation_max</a>;</div>
<div class="line"><span class="lineno">  101</span>  assert(input_shape.<a class="code hl_function" href="classnnfw_1_1cker_1_1_shape.html#a01c6b5dc4b24d5c2567c47cb15318839">DimensionsCount</a>() == 4);</div>
<div class="line"><span class="lineno">  102</span>  assert(filter_shape.<a class="code hl_function" href="classnnfw_1_1cker_1_1_shape.html#a01c6b5dc4b24d5c2567c47cb15318839">DimensionsCount</a>() == 4);</div>
<div class="line"><span class="lineno">  103</span>  assert(output_shape.DimensionsCount() == 4);</div>
<div class="line"><span class="lineno">  104</span> </div>
<div class="line"><span class="lineno">  105</span>  <span class="keyword">const</span> uint8_t *gemm_input_data = <span class="keyword">nullptr</span>;</div>
<div class="line"><span class="lineno">  106</span>  <span class="keyword">const</span> <a class="code hl_class" href="classnnfw_1_1cker_1_1_shape.html">Shape</a> *gemm_input_shape = <span class="keyword">nullptr</span>;</div>
<div class="line"><span class="lineno">  107</span>  <span class="keyword">const</span> <span class="keywordtype">int</span> filter_width = filter_shape.<a class="code hl_function" href="classnnfw_1_1cker_1_1_shape.html#a497180ee7844bbef51b36bd58e61fa31">Dims</a>(2);</div>
<div class="line"><span class="lineno">  108</span>  <span class="keyword">const</span> <span class="keywordtype">int</span> filter_height = filter_shape.<a class="code hl_function" href="classnnfw_1_1cker_1_1_shape.html#a497180ee7844bbef51b36bd58e61fa31">Dims</a>(1);</div>
<div class="line"><span class="lineno">  109</span>  <span class="keyword">const</span> <span class="keywordtype">bool</span> need_dilated_im2col = dilation_width_factor != 1 || dilation_height_factor != 1;</div>
<div class="line"><span class="lineno">  110</span>  <span class="keyword">const</span> <span class="keywordtype">bool</span> need_im2col =</div>
<div class="line"><span class="lineno">  111</span>    stride_width != 1 || stride_height != 1 || filter_width != 1 || filter_height != 1;</div>
<div class="line"><span class="lineno">  112</span>  <span class="keywordflow">if</span> (need_dilated_im2col)</div>
<div class="line"><span class="lineno">  113</span>  {</div>
<div class="line"><span class="lineno">  114</span>    assert(im2col_data);</div>
<div class="line"><span class="lineno">  115</span>    <span class="keyword">const</span> <span class="keywordtype">int</span> input_zero_point = -input_offset;</div>
<div class="line"><span class="lineno">  116</span>    assert(input_zero_point &gt;= 0);</div>
<div class="line"><span class="lineno">  117</span>    assert(input_zero_point &lt;= 255);</div>
<div class="line"><span class="lineno">  118</span>    <a class="code hl_function" href="namespacennfw_1_1cker_1_1optimized.html#a403e08d865d6f45ba561998b93304d57">DilatedIm2col</a>(params, input_zero_point, input_shape, input_data, filter_shape, output_shape,</div>
<div class="line"><span class="lineno">  119</span>                  im2col_data);</div>
<div class="line"><span class="lineno">  120</span>    gemm_input_data = im2col_data;</div>
<div class="line"><span class="lineno">  121</span>    gemm_input_shape = &amp;im2col_shape;</div>
<div class="line"><span class="lineno">  122</span>  }</div>
<div class="line"><span class="lineno">  123</span>  <span class="keywordflow">else</span> <span class="keywordflow">if</span> (need_im2col)</div>
<div class="line"><span class="lineno">  124</span>  {</div>
<div class="line"><span class="lineno">  125</span>    assert(im2col_data);</div>
<div class="line"><span class="lineno">  126</span>    <span class="keyword">const</span> <span class="keywordtype">int</span> input_zero_point = -input_offset;</div>
<div class="line"><span class="lineno">  127</span>    assert(input_zero_point &gt;= 0);</div>
<div class="line"><span class="lineno">  128</span>    assert(input_zero_point &lt;= 255);</div>
<div class="line"><span class="lineno">  129</span>    <a class="code hl_function" href="_conv2_d_8float_8cpp.html#a305eb29314f213a066d5a689dfa9124f">Im2col</a>(params, filter_height, filter_width, input_zero_point, input_shape, input_data,</div>
<div class="line"><span class="lineno">  130</span>           im2col_shape, im2col_data);</div>
<div class="line"><span class="lineno">  131</span>    gemm_input_data = im2col_data;</div>
<div class="line"><span class="lineno">  132</span>    gemm_input_shape = &amp;im2col_shape;</div>
<div class="line"><span class="lineno">  133</span>  }</div>
<div class="line"><span class="lineno">  134</span>  <span class="keywordflow">else</span></div>
<div class="line"><span class="lineno">  135</span>  {</div>
<div class="line"><span class="lineno">  136</span>    gemm_input_data = input_data;</div>
<div class="line"><span class="lineno">  137</span>    gemm_input_shape = &amp;input_shape;</div>
<div class="line"><span class="lineno">  138</span>  }</div>
<div class="line"><span class="lineno">  139</span> </div>
<div class="line"><span class="lineno">  140</span>  <span class="keyword">const</span> <span class="keywordtype">int</span> gemm_input_rows = gemm_input_shape-&gt;<a class="code hl_function" href="classnnfw_1_1cker_1_1_shape.html#a497180ee7844bbef51b36bd58e61fa31">Dims</a>(3);</div>
<div class="line"><span class="lineno">  141</span>  <span class="comment">// Using FlatSizeSkipDim causes segfault in some contexts (see b/79927784).</span></div>
<div class="line"><span class="lineno">  142</span>  <span class="comment">// The root cause has not yet been identified though. Same applies below for</span></div>
<div class="line"><span class="lineno">  143</span>  <span class="comment">// the other calls commented out. This is a partial rollback of cl/196819423.</span></div>
<div class="line"><span class="lineno">  144</span>  <span class="comment">// const int gemm_input_cols = FlatSizeSkipDim(*gemm_input_shape, 3);</span></div>
<div class="line"><span class="lineno">  145</span>  <span class="keyword">const</span> <span class="keywordtype">int</span> gemm_input_cols =</div>
<div class="line"><span class="lineno">  146</span>    gemm_input_shape-&gt;<a class="code hl_function" href="classnnfw_1_1cker_1_1_shape.html#a497180ee7844bbef51b36bd58e61fa31">Dims</a>(0) * gemm_input_shape-&gt;<a class="code hl_function" href="classnnfw_1_1cker_1_1_shape.html#a497180ee7844bbef51b36bd58e61fa31">Dims</a>(1) * gemm_input_shape-&gt;<a class="code hl_function" href="classnnfw_1_1cker_1_1_shape.html#a497180ee7844bbef51b36bd58e61fa31">Dims</a>(2);</div>
<div class="line"><span class="lineno">  147</span>  <span class="keyword">const</span> <span class="keywordtype">int</span> filter_rows = filter_shape.<a class="code hl_function" href="classnnfw_1_1cker_1_1_shape.html#a497180ee7844bbef51b36bd58e61fa31">Dims</a>(0);</div>
<div class="line"><span class="lineno">  148</span>  <span class="comment">// See b/79927784.</span></div>
<div class="line"><span class="lineno">  149</span>  <span class="comment">// const int filter_cols = FlatSizeSkipDim(filter_shape, 0);</span></div>
<div class="line"><span class="lineno">  150</span>  <span class="keyword">const</span> <span class="keywordtype">int</span> filter_cols = filter_shape.<a class="code hl_function" href="classnnfw_1_1cker_1_1_shape.html#a497180ee7844bbef51b36bd58e61fa31">Dims</a>(1) * filter_shape.<a class="code hl_function" href="classnnfw_1_1cker_1_1_shape.html#a497180ee7844bbef51b36bd58e61fa31">Dims</a>(2) * filter_shape.<a class="code hl_function" href="classnnfw_1_1cker_1_1_shape.html#a497180ee7844bbef51b36bd58e61fa31">Dims</a>(3);</div>
<div class="line"><span class="lineno">  151</span>  <span class="keyword">const</span> <span class="keywordtype">int</span> output_rows = output_shape.Dims(3);</div>
<div class="line"><span class="lineno">  152</span>  <span class="comment">// See b/79927784.</span></div>
<div class="line"><span class="lineno">  153</span>  <span class="comment">// const int output_cols = FlatSizeSkipDim(output_shape, 3);</span></div>
<div class="line"><span class="lineno">  154</span>  <span class="keyword">const</span> <span class="keywordtype">int</span> output_cols = output_shape.Dims(0) * output_shape.Dims(1) * output_shape.Dims(2);</div>
<div class="line"><span class="lineno">  155</span>  assert(output_rows == filter_rows);</div>
<div class="line"><span class="lineno">  156</span>  assert(output_cols == gemm_input_cols);</div>
<div class="line"><span class="lineno">  157</span>  assert(filter_cols == gemm_input_rows);</div>
<div class="line"><span class="lineno">  158</span>  assert(bias_shape.<a class="code hl_function" href="classnnfw_1_1cker_1_1_shape.html#ac7e29fd510111992fbd44906e2080f12">FlatSize</a>() == output_rows);</div>
<div class="line"><span class="lineno">  159</span>  <a class="code hl_define" href="compute_2cker_2include_2cker_2_shape_8h.html#a044d7f999713173dd3360b0aa40370fd">UNUSED_RELEASE</a>(bias_shape);</div>
<div class="line"><span class="lineno">  160</span>  gemmlowp::MatrixMap&lt;const uint8_t, gemmlowp::MapOrder::RowMajor&gt; filter_matrix(</div>
<div class="line"><span class="lineno">  161</span>    filter_data, filter_rows, filter_cols);</div>
<div class="line"><span class="lineno">  162</span>  gemmlowp::MatrixMap&lt;const uint8_t, gemmlowp::MapOrder::ColMajor&gt; input_matrix(</div>
<div class="line"><span class="lineno">  163</span>    gemm_input_data, gemm_input_rows, gemm_input_cols);</div>
<div class="line"><span class="lineno">  164</span>  gemmlowp::MatrixMap&lt;uint8_t, gemmlowp::MapOrder::ColMajor&gt; output_matrix(output_data, output_rows,</div>
<div class="line"><span class="lineno">  165</span>                                                                           output_cols);</div>
<div class="line"><span class="lineno">  166</span>  <span class="keyword">const</span> <span class="keyword">auto</span> &amp;output_pipeline =</div>
<div class="line"><span class="lineno">  167</span>    GemmlowpOutputPipeline::MakeExp(bias_data, output_rows, output_offset, output_multiplier,</div>
<div class="line"><span class="lineno">  168</span>                                    output_shift, output_activation_min, output_activation_max);</div>
<div class="line"><span class="lineno">  169</span> </div>
<div class="line"><span class="lineno">  170</span>  std::lock_guard&lt;std::mutex&gt; lock_guard(_gemmlowp_mutex);</div>
<div class="line"><span class="lineno">  171</span>  gemmlowp::GemmWithOutputPipeline&lt;uint8_t, uint8_t, gemmlowp::L8R8WithLhsNonzeroBitDepthParams&gt;(</div>
<div class="line"><span class="lineno">  172</span>    gemm_context, filter_matrix, input_matrix, &amp;output_matrix, filter_offset, input_offset,</div>
<div class="line"><span class="lineno">  173</span>    output_pipeline);</div>
<div class="line"><span class="lineno">  174</span>}</div>
<div class="ttc" id="a_conv2_d_8float_8cpp_html_a305eb29314f213a066d5a689dfa9124f"><div class="ttname"><a href="_conv2_d_8float_8cpp.html#a305eb29314f213a066d5a689dfa9124f">Im2col</a></div><div class="ttdeci">void Im2col(const T *input_data, const Dims&lt; 4 &gt; &amp;input_dims, int stride_width, int stride_height, int pad_width, int pad_height, int kheight, int kwidth, uint8 byte_zero, T *output_data, const Dims&lt; 4 &gt; &amp;output_dims)</div><div class="ttdef"><b>Definition:</b> <a href="_conv2_d_8float_8cpp_source.html#l00116">Conv2D.float.cpp:116</a></div></div>
<div class="ttc" id="aclassnnfw_1_1cker_1_1_shape_html"><div class="ttname"><a href="classnnfw_1_1cker_1_1_shape.html">nnfw::cker::Shape</a></div><div class="ttdef"><b>Definition:</b> <a href="compute_2cker_2include_2cker_2_shape_8h_source.html#l00033">Shape.h:34</a></div></div>
<div class="ttc" id="aclassnnfw_1_1cker_1_1_shape_html_a01c6b5dc4b24d5c2567c47cb15318839"><div class="ttname"><a href="classnnfw_1_1cker_1_1_shape.html#a01c6b5dc4b24d5c2567c47cb15318839">nnfw::cker::Shape::DimensionsCount</a></div><div class="ttdeci">int32_t DimensionsCount() const</div><div class="ttdef"><b>Definition:</b> <a href="compute_2cker_2include_2cker_2_shape_8h_source.html#l00093">Shape.h:93</a></div></div>
<div class="ttc" id="aclassnnfw_1_1cker_1_1_shape_html_a497180ee7844bbef51b36bd58e61fa31"><div class="ttname"><a href="classnnfw_1_1cker_1_1_shape.html#a497180ee7844bbef51b36bd58e61fa31">nnfw::cker::Shape::Dims</a></div><div class="ttdeci">int32_t Dims(int i) const</div><div class="ttdef"><b>Definition:</b> <a href="compute_2cker_2include_2cker_2_shape_8h_source.html#l00094">Shape.h:94</a></div></div>
<div class="ttc" id="acompute_2cker_2include_2cker_2_shape_8h_html_a044d7f999713173dd3360b0aa40370fd"><div class="ttname"><a href="compute_2cker_2include_2cker_2_shape_8h.html#a044d7f999713173dd3360b0aa40370fd">UNUSED_RELEASE</a></div><div class="ttdeci">#define UNUSED_RELEASE(a)</div><div class="ttdef"><b>Definition:</b> <a href="compute_2cker_2include_2cker_2_shape_8h_source.html#l00026">Shape.h:26</a></div></div>
<div class="ttc" id="anamespacennfw_1_1cker_1_1optimized_html_a403e08d865d6f45ba561998b93304d57"><div class="ttname"><a href="namespacennfw_1_1cker_1_1optimized.html#a403e08d865d6f45ba561998b93304d57">nnfw::cker::optimized::DilatedIm2col</a></div><div class="ttdeci">void DilatedIm2col(const ConvParams &amp;params, const Shape &amp;input_shape, const T *input_data, const Shape &amp;filter_shape, const Shape &amp;output_shape, T *im2col_data, const int32_t *zero_bytes, const int zero_bytes_len)</div><div class="ttdef"><b>Definition:</b> <a href="_optimized_utils_8h_source.html#l00121">OptimizedUtils.h:121</a></div></div>
<div class="ttc" id="astructnnfw_1_1cker_1_1_conv_params_html_a0878ee74c474c8a82a574727a2a3048e"><div class="ttname"><a href="structnnfw_1_1cker_1_1_conv_params.html#a0878ee74c474c8a82a574727a2a3048e">nnfw::cker::ConvParams::stride_height</a></div><div class="ttdeci">int16_t stride_height</div><div class="ttdef"><b>Definition:</b> <a href="compute_2cker_2include_2cker_2_types_8h_source.html#l00140">Types.h:140</a></div></div>
<div class="ttc" id="astructnnfw_1_1cker_1_1_conv_params_html_a1b36fc4cd87255a5e67218c4504db9b9"><div class="ttname"><a href="structnnfw_1_1cker_1_1_conv_params.html#a1b36fc4cd87255a5e67218c4504db9b9">nnfw::cker::ConvParams::output_shift</a></div><div class="ttdeci">int output_shift</div><div class="ttdef"><b>Definition:</b> <a href="compute_2cker_2include_2cker_2_types_8h_source.html#l00149">Types.h:149</a></div></div>
<div class="ttc" id="astructnnfw_1_1cker_1_1_conv_params_html_a273ecef205d663377502d9dcaa77f1b4"><div class="ttname"><a href="structnnfw_1_1cker_1_1_conv_params.html#a273ecef205d663377502d9dcaa77f1b4">nnfw::cker::ConvParams::stride_width</a></div><div class="ttdeci">int16_t stride_width</div><div class="ttdef"><b>Definition:</b> <a href="compute_2cker_2include_2cker_2_types_8h_source.html#l00139">Types.h:139</a></div></div>
<div class="ttc" id="astructnnfw_1_1cker_1_1_conv_params_html_a435e0fcf2622a161fe913f6098b2dd1e"><div class="ttname"><a href="structnnfw_1_1cker_1_1_conv_params.html#a435e0fcf2622a161fe913f6098b2dd1e">nnfw::cker::ConvParams::input_offset</a></div><div class="ttdeci">int32_t input_offset</div><div class="ttdef"><b>Definition:</b> <a href="compute_2cker_2include_2cker_2_types_8h_source.html#l00145">Types.h:145</a></div></div>
<div class="ttc" id="astructnnfw_1_1cker_1_1_conv_params_html_a636e1132f40731ba32f9060ae7f4dd44"><div class="ttname"><a href="structnnfw_1_1cker_1_1_conv_params.html#a636e1132f40731ba32f9060ae7f4dd44">nnfw::cker::ConvParams::output_multiplier</a></div><div class="ttdeci">int32_t output_multiplier</div><div class="ttdef"><b>Definition:</b> <a href="compute_2cker_2include_2cker_2_types_8h_source.html#l00148">Types.h:148</a></div></div>
<div class="ttc" id="astructnnfw_1_1cker_1_1_conv_params_html_a7a4237fa84247ed20ecb16119e1ec589"><div class="ttname"><a href="structnnfw_1_1cker_1_1_conv_params.html#a7a4237fa84247ed20ecb16119e1ec589">nnfw::cker::ConvParams::weights_offset</a></div><div class="ttdeci">int32_t weights_offset</div><div class="ttdef"><b>Definition:</b> <a href="compute_2cker_2include_2cker_2_types_8h_source.html#l00146">Types.h:146</a></div></div>
<div class="ttc" id="astructnnfw_1_1cker_1_1_conv_params_html_a7fef4d3c76b386c35ec04adfd931f991"><div class="ttname"><a href="structnnfw_1_1cker_1_1_conv_params.html#a7fef4d3c76b386c35ec04adfd931f991">nnfw::cker::ConvParams::output_offset</a></div><div class="ttdeci">int32_t output_offset</div><div class="ttdef"><b>Definition:</b> <a href="compute_2cker_2include_2cker_2_types_8h_source.html#l00147">Types.h:147</a></div></div>
<div class="ttc" id="astructnnfw_1_1cker_1_1_conv_params_html_a8ff1e952b2e7627e11b47a22da0883cb"><div class="ttname"><a href="structnnfw_1_1cker_1_1_conv_params.html#a8ff1e952b2e7627e11b47a22da0883cb">nnfw::cker::ConvParams::dilation_width_factor</a></div><div class="ttdeci">int16_t dilation_width_factor</div><div class="ttdef"><b>Definition:</b> <a href="compute_2cker_2include_2cker_2_types_8h_source.html#l00141">Types.h:141</a></div></div>
<div class="ttc" id="astructnnfw_1_1cker_1_1_conv_params_html_aadb8981e614d4834abc8318f406c5462"><div class="ttname"><a href="structnnfw_1_1cker_1_1_conv_params.html#aadb8981e614d4834abc8318f406c5462">nnfw::cker::ConvParams::quantized_activation_max</a></div><div class="ttdeci">int32_t quantized_activation_max</div><div class="ttdef"><b>Definition:</b> <a href="compute_2cker_2include_2cker_2_types_8h_source.html#l00152">Types.h:152</a></div></div>
<div class="ttc" id="astructnnfw_1_1cker_1_1_conv_params_html_ad47b94004b5b1718979775edaa9eee70"><div class="ttname"><a href="structnnfw_1_1cker_1_1_conv_params.html#ad47b94004b5b1718979775edaa9eee70">nnfw::cker::ConvParams::dilation_height_factor</a></div><div class="ttdeci">int16_t dilation_height_factor</div><div class="ttdef"><b>Definition:</b> <a href="compute_2cker_2include_2cker_2_types_8h_source.html#l00142">Types.h:142</a></div></div>
<div class="ttc" id="astructnnfw_1_1cker_1_1_conv_params_html_ad4f74d8bfaa34bd1f44a690e7b77fc5f"><div class="ttname"><a href="structnnfw_1_1cker_1_1_conv_params.html#ad4f74d8bfaa34bd1f44a690e7b77fc5f">nnfw::cker::ConvParams::quantized_activation_min</a></div><div class="ttdeci">int32_t quantized_activation_min</div><div class="ttdef"><b>Definition:</b> <a href="compute_2cker_2include_2cker_2_types_8h_source.html#l00151">Types.h:151</a></div></div>
</div><!-- fragment -->
<p class="reference">References <a class="el" href="compute_2cker_2include_2cker_2operation_2optimized_2_conv_8h_source.html#l00045">_gemmlowp_mutex</a>, <a class="el" href="_optimized_utils_8h_source.html#l00121">DilatedIm2col()</a>, <a class="el" href="compute_2cker_2include_2cker_2_types_8h_source.html#l00142">nnfw::cker::ConvParams::dilation_height_factor</a>, <a class="el" href="compute_2cker_2include_2cker_2_types_8h_source.html#l00141">nnfw::cker::ConvParams::dilation_width_factor</a>, <a class="el" href="compute_2cker_2include_2cker_2_shape_8h_source.html#l00093">nnfw::cker::Shape::DimensionsCount()</a>, <a class="el" href="compute_2cker_2include_2cker_2_shape_8h_source.html#l00094">nnfw::cker::Shape::Dims()</a>, <a class="el" href="compute_2cker_2include_2cker_2_shape_8h_source.html#l00183">nnfw::cker::Shape::FlatSize()</a>, <a class="el" href="_g_e_m_m_support_8h_source.html#l00057">nnfw::cker::gemm_support::GetGemmLowpContext()</a>, <a class="el" href="_optimized_utils_8h_source.html#l00222">Im2col()</a>, <a class="el" href="compute_2cker_2include_2cker_2_types_8h_source.html#l00145">nnfw::cker::ConvParams::input_offset</a>, <a class="el" href="compute_2cker_2include_2cker_2operation_2optimized_2_conv_8h_source.html#l00054">nnfw::cker::optimized::GemmlowpOutputPipeline::MakeExp()</a>, <a class="el" href="compute_2cker_2include_2cker_2_types_8h_source.html#l00148">nnfw::cker::ConvParams::output_multiplier</a>, <a class="el" href="compute_2cker_2include_2cker_2_types_8h_source.html#l00147">nnfw::cker::ConvParams::output_offset</a>, <a class="el" href="compute_2cker_2include_2cker_2_types_8h_source.html#l00149">nnfw::cker::ConvParams::output_shift</a>, <a class="el" href="compute_2cker_2include_2cker_2_types_8h_source.html#l00152">nnfw::cker::ConvParams::quantized_activation_max</a>, <a class="el" href="compute_2cker_2include_2cker_2_types_8h_source.html#l00151">nnfw::cker::ConvParams::quantized_activation_min</a>, <a class="el" href="compute_2cker_2include_2cker_2_types_8h_source.html#l00140">nnfw::cker::ConvParams::stride_height</a>, <a class="el" href="compute_2cker_2include_2cker_2_types_8h_source.html#l00139">nnfw::cker::ConvParams::stride_width</a>, <a class="el" href="compute_2cker_2include_2cker_2_shape_8h_source.html#l00026">UNUSED_RELEASE</a>, and <a class="el" href="compute_2cker_2include_2cker_2_types_8h_source.html#l00146">nnfw::cker::ConvParams::weights_offset</a>.</p>

<p class="reference">Referenced by <a class="el" href="compute_2cker_2include_2cker_2operation_2_conv_8h_source.html#l00112">nnfw::cker::Conv::operator()()</a>.</p>

</div>
</div>
<a id="ad73d8bcc64e3f5565c0a88e353691ba1" name="ad73d8bcc64e3f5565c0a88e353691ba1"></a>
<h2 class="memtitle"><span class="permalink"><a href="#ad73d8bcc64e3f5565c0a88e353691ba1">&#9670;&#160;</a></span>DepthwiseConvImpl() <span class="overload">[1/2]</span></h2>

<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">void nnfw::cker::optimized::DepthwiseConvImpl </td>
          <td>(</td>
          <td class="paramtype">const <a class="el" href="structnnfw_1_1cker_1_1_depthwise_conv_params.html">DepthwiseConvParams</a> &amp;&#160;</td>
          <td class="paramname"><em>params</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const <a class="el" href="classnnfw_1_1cker_1_1_shape.html">Shape</a> &amp;&#160;</td>
          <td class="paramname"><em>input_shape</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const float *&#160;</td>
          <td class="paramname"><em>input_data</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const <a class="el" href="classnnfw_1_1cker_1_1_shape.html">Shape</a> &amp;&#160;</td>
          <td class="paramname"><em>filter_shape</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const float *&#160;</td>
          <td class="paramname"><em>filter_data</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const <a class="el" href="classnnfw_1_1cker_1_1_shape.html">Shape</a> &amp;&#160;</td>
          <td class="paramname"><em>bias_shape</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const float *&#160;</td>
          <td class="paramname"><em>bias_data</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const <a class="el" href="classnnfw_1_1cker_1_1_shape.html">Shape</a> &amp;&#160;</td>
          <td class="paramname"><em>output_shape</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">float *&#160;</td>
          <td class="paramname"><em>output_data</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">int&#160;</td>
          <td class="paramname"><em>thread_start</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">int&#160;</td>
          <td class="paramname"><em>thread_end</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">int&#160;</td>
          <td class="paramname"><em>thread_dim</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">inline</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p class="definition">Definition at line <a class="el" href="_depthwise_conv_float_8h_source.html#l01033">1033</a> of file <a class="el" href="_depthwise_conv_float_8h_source.html">DepthwiseConvFloat.h</a>.</p>
<div class="fragment"><div class="line"><span class="lineno"> 1038</span>{</div>
<div class="line"><span class="lineno"> 1039</span>  <a class="code hl_define" href="compute_2cker_2include_2cker_2_shape_8h.html#a044d7f999713173dd3360b0aa40370fd">UNUSED_RELEASE</a>(bias_shape);</div>
<div class="line"><span class="lineno"> 1040</span>  <span class="keyword">const</span> <span class="keywordtype">int</span> stride_width = params.<a class="code hl_variable" href="structnnfw_1_1cker_1_1_depthwise_conv_params.html#a65cb7bfeb2b801d8a574a85da10598ff">stride_width</a>;</div>
<div class="line"><span class="lineno"> 1041</span>  <span class="keyword">const</span> <span class="keywordtype">int</span> stride_height = params.<a class="code hl_variable" href="structnnfw_1_1cker_1_1_depthwise_conv_params.html#ad2dbdcb99cb6434e20c073adf4266bef">stride_height</a>;</div>
<div class="line"><span class="lineno"> 1042</span>  <span class="keyword">const</span> <span class="keywordtype">int</span> pad_width = params.<a class="code hl_variable" href="structnnfw_1_1cker_1_1_depthwise_conv_params.html#ab424f92784d7f28eba2b61dcb1be277b">padding_values</a>.<a class="code hl_variable" href="structnnfw_1_1cker_1_1_padding_values.html#a2a3dd79d28c30c5da865290160601fd1">width</a>;</div>
<div class="line"><span class="lineno"> 1043</span>  <span class="keyword">const</span> <span class="keywordtype">int</span> pad_height = params.<a class="code hl_variable" href="structnnfw_1_1cker_1_1_depthwise_conv_params.html#ab424f92784d7f28eba2b61dcb1be277b">padding_values</a>.<a class="code hl_variable" href="structnnfw_1_1cker_1_1_padding_values.html#a1dc41a9ebbd61a7c34ed2b5dd2ec10eb">height</a>;</div>
<div class="line"><span class="lineno"> 1044</span>  <span class="keyword">const</span> <span class="keywordtype">int</span> depth_multiplier = params.<a class="code hl_variable" href="structnnfw_1_1cker_1_1_depthwise_conv_params.html#aaa0e9f0f50f1ed3c8e25886d51a45f75">depth_multiplier</a>;</div>
<div class="line"><span class="lineno"> 1045</span>  <span class="keyword">const</span> <span class="keywordtype">float</span> output_activation_min = params.<a class="code hl_variable" href="structnnfw_1_1cker_1_1_depthwise_conv_params.html#a0591df4ab5a62a8988ced9d209e42fcd">float_activation_min</a>;</div>
<div class="line"><span class="lineno"> 1046</span>  <span class="keyword">const</span> <span class="keywordtype">float</span> output_activation_max = params.<a class="code hl_variable" href="structnnfw_1_1cker_1_1_depthwise_conv_params.html#a5ad1d3c93ba06b5ac78a2ca6a6d1ff4a">float_activation_max</a>;</div>
<div class="line"><span class="lineno"> 1047</span>  <span class="keyword">const</span> <span class="keywordtype">int</span> dilation_width_factor = params.<a class="code hl_variable" href="structnnfw_1_1cker_1_1_depthwise_conv_params.html#a50d46dad3a5d5bec206a3f10f1168768">dilation_width_factor</a>;</div>
<div class="line"><span class="lineno"> 1048</span>  <span class="keyword">const</span> <span class="keywordtype">int</span> dilation_height_factor = params.<a class="code hl_variable" href="structnnfw_1_1cker_1_1_depthwise_conv_params.html#a2a11fa7f567b0faa4551dfe628d6de2b">dilation_height_factor</a>;</div>
<div class="line"><span class="lineno"> 1049</span>  assert(input_shape.<a class="code hl_function" href="classnnfw_1_1cker_1_1_shape.html#a01c6b5dc4b24d5c2567c47cb15318839">DimensionsCount</a>() == 4);</div>
<div class="line"><span class="lineno"> 1050</span>  assert(filter_shape.<a class="code hl_function" href="classnnfw_1_1cker_1_1_shape.html#a01c6b5dc4b24d5c2567c47cb15318839">DimensionsCount</a>() == 4);</div>
<div class="line"><span class="lineno"> 1051</span>  assert(output_shape.DimensionsCount() == 4);</div>
<div class="line"><span class="lineno"> 1052</span>  assert(thread_dim == 0 || thread_dim == 1);</div>
<div class="line"><span class="lineno"> 1053</span> </div>
<div class="line"><span class="lineno"> 1054</span>  <span class="keyword">const</span> <span class="keywordtype">int</span> batches = <a class="code hl_function" href="namespacennfw_1_1cker.html#a0a74e72b1bffcb6519de7fc224416ff9">MatchingDim</a>(input_shape, 0, output_shape, 0);</div>
<div class="line"><span class="lineno"> 1055</span>  <span class="keyword">const</span> <span class="keywordtype">int</span> output_depth = <a class="code hl_function" href="namespacennfw_1_1cker.html#a0a74e72b1bffcb6519de7fc224416ff9">MatchingDim</a>(filter_shape, 3, output_shape, 3);</div>
<div class="line"><span class="lineno"> 1056</span>  <span class="keyword">const</span> <span class="keywordtype">int</span> input_height = input_shape.<a class="code hl_function" href="classnnfw_1_1cker_1_1_shape.html#a497180ee7844bbef51b36bd58e61fa31">Dims</a>(1);</div>
<div class="line"><span class="lineno"> 1057</span>  <span class="keyword">const</span> <span class="keywordtype">int</span> input_width = input_shape.<a class="code hl_function" href="classnnfw_1_1cker_1_1_shape.html#a497180ee7844bbef51b36bd58e61fa31">Dims</a>(2);</div>
<div class="line"><span class="lineno"> 1058</span>  <span class="keyword">const</span> <span class="keywordtype">int</span> input_depth = input_shape.<a class="code hl_function" href="classnnfw_1_1cker_1_1_shape.html#a497180ee7844bbef51b36bd58e61fa31">Dims</a>(3);</div>
<div class="line"><span class="lineno"> 1059</span>  <span class="keyword">const</span> <span class="keywordtype">int</span> filter_height = filter_shape.<a class="code hl_function" href="classnnfw_1_1cker_1_1_shape.html#a497180ee7844bbef51b36bd58e61fa31">Dims</a>(1);</div>
<div class="line"><span class="lineno"> 1060</span>  <span class="keyword">const</span> <span class="keywordtype">int</span> filter_width = filter_shape.<a class="code hl_function" href="classnnfw_1_1cker_1_1_shape.html#a497180ee7844bbef51b36bd58e61fa31">Dims</a>(2);</div>
<div class="line"><span class="lineno"> 1061</span>  <span class="keyword">const</span> <span class="keywordtype">int</span> output_height = output_shape.Dims(1);</div>
<div class="line"><span class="lineno"> 1062</span>  <span class="keyword">const</span> <span class="keywordtype">int</span> output_width = output_shape.Dims(2);</div>
<div class="line"><span class="lineno"> 1063</span>  assert(output_depth == input_depth * depth_multiplier);</div>
<div class="line"><span class="lineno"> 1064</span>  assert(bias_shape.<a class="code hl_function" href="classnnfw_1_1cker_1_1_shape.html#ac7e29fd510111992fbd44906e2080f12">FlatSize</a>() == output_depth);</div>
<div class="line"><span class="lineno"> 1065</span> </div>
<div class="line"><span class="lineno"> 1066</span>  <span class="keyword">static</span> <span class="keyword">const</span> <span class="keywordtype">int</span> kAccBufferMaxSize = 4832;</div>
<div class="line"><span class="lineno"> 1067</span>  <span class="keywordtype">float</span> acc_buffer[kAccBufferMaxSize];</div>
<div class="line"><span class="lineno"> 1068</span>  assert(kAccBufferMaxSize &gt;= output_depth);</div>
<div class="line"><span class="lineno"> 1069</span>  <span class="keyword">const</span> <span class="keywordtype">int</span> kOutputPixelsInAccBuffer = kAccBufferMaxSize / output_depth;</div>
<div class="line"><span class="lineno"> 1070</span>  <span class="keyword">const</span> <span class="keywordtype">int</span> kAccBufferActualSize = kOutputPixelsInAccBuffer * output_depth;</div>
<div class="line"><span class="lineno"> 1071</span>  assert(kOutputPixelsInAccBuffer * output_depth &lt;= kAccBufferActualSize);</div>
<div class="line"><span class="lineno"> 1072</span>  assert(kAccBufferActualSize &lt;= kAccBufferMaxSize);</div>
<div class="line"><span class="lineno"> 1073</span>  assert(kOutputPixelsInAccBuffer &gt;= 1);</div>
<div class="line"><span class="lineno"> 1074</span> </div>
<div class="line"><span class="lineno"> 1075</span>  <a class="code hl_define" href="compute_2cker_2include_2cker_2_shape_8h.html#a044d7f999713173dd3360b0aa40370fd">UNUSED_RELEASE</a>(kAccBufferActualSize);</div>
<div class="line"><span class="lineno"> 1076</span> </div>
<div class="line"><span class="lineno"> 1077</span>  <span class="comment">// row_accum_func will point to the core accumulation function to be used</span></div>
<div class="line"><span class="lineno"> 1078</span>  <span class="comment">// for this DepthwiseConv op.</span></div>
<div class="line"><span class="lineno"> 1079</span>  <span class="keyword">using </span>row_accum_func_t = <span class="keyword">decltype</span>(&amp;<a class="code hl_function" href="namespacennfw_1_1cker_1_1optimized.html#ad4525396811538b4ed0ec83ded38aadf">FloatDepthwiseConvAccumRowGeneric</a>);</div>
<div class="line"><span class="lineno"> 1080</span>  row_accum_func_t row_accum_func = <span class="keyword">nullptr</span>;</div>
<div class="line"><span class="lineno"> 1081</span> </div>
<div class="line"><span class="lineno"> 1082</span><span class="preprocessor">#define TFMINI_USE_DEPTHWISECONV_KERNEL(ALLOW_STRIDED, FIXED_INPUT_DEPTH, FIXED_DEPTH_MULTIPLIER) \</span></div>
<div class="line"><span class="lineno"> 1083</span><span class="preprocessor">  if (!row_accum_func &amp;&amp; (stride_width == 1 || ALLOW_STRIDED) &amp;&amp;                                  \</span></div>
<div class="line"><span class="lineno"> 1084</span><span class="preprocessor">      (input_depth == FIXED_INPUT_DEPTH || FIXED_INPUT_DEPTH == 0) &amp;&amp;                             \</span></div>
<div class="line"><span class="lineno"> 1085</span><span class="preprocessor">      depth_multiplier == FIXED_DEPTH_MULTIPLIER)                                                 \</span></div>
<div class="line"><span class="lineno"> 1086</span><span class="preprocessor">  {                                                                                               \</span></div>
<div class="line"><span class="lineno"> 1087</span><span class="preprocessor">    row_accum_func =                                                                              \</span></div>
<div class="line"><span class="lineno"> 1088</span><span class="preprocessor">      FloatDepthwiseConvAccumRow&lt;ALLOW_STRIDED, FIXED_INPUT_DEPTH, FIXED_DEPTH_MULTIPLIER&gt;;       \</span></div>
<div class="line"><span class="lineno"> 1089</span><span class="preprocessor">  }</span></div>
<div class="line"><span class="lineno"> 1090</span> </div>
<div class="line"><span class="lineno"> 1091</span><span class="preprocessor">#ifdef USE_NEON</span></div>
<div class="line"><span class="lineno"> 1092</span>  <span class="comment">// We go over our list of kernels by decreasing order of preference</span></div>
<div class="line"><span class="lineno"> 1093</span>  <span class="comment">// for the cases where multiple kernels could apply.</span></div>
<div class="line"><span class="lineno"> 1094</span> </div>
<div class="line"><span class="lineno"> 1095</span>  <span class="comment">// Start with the fastest kernels: AllowStrided=false, fixed input depth.</span></div>
<div class="line"><span class="lineno"> 1096</span> </div>
<div class="line"><span class="lineno"> 1097</span>  <a class="code hl_define" href="_depthwise_conv_float_8h.html#a5d6cdde4feb9761abaa001f402b79f27">TFMINI_USE_DEPTHWISECONV_KERNEL</a>(<span class="keyword">false</span>, 8, 1)</div>
<div class="line"><span class="lineno"> 1098</span>  <a class="code hl_define" href="_depthwise_conv_float_8h.html#a5d6cdde4feb9761abaa001f402b79f27">TFMINI_USE_DEPTHWISECONV_KERNEL</a>(false, 2, 1)</div>
<div class="line"><span class="lineno"> 1099</span> </div>
<div class="line"><span class="lineno"> 1100</span>  <span class="comment">// Next come the strided kernels: AllowStrided=true, fixed input depth.</span></div>
<div class="line"><span class="lineno"> 1101</span>  <span class="comment">// They are a bit less efficient, but allow stride!=1.</span></div>
<div class="line"><span class="lineno"> 1102</span> </div>
<div class="line"><span class="lineno"> 1103</span>  <a class="code hl_define" href="_depthwise_conv_float_8h.html#a5d6cdde4feb9761abaa001f402b79f27">TFMINI_USE_DEPTHWISECONV_KERNEL</a>(true, 8, 1)</div>
<div class="line"><span class="lineno"> 1104</span>  <a class="code hl_define" href="_depthwise_conv_float_8h.html#a5d6cdde4feb9761abaa001f402b79f27">TFMINI_USE_DEPTHWISECONV_KERNEL</a>(true, 1, 8)</div>
<div class="line"><span class="lineno"> 1105</span>  <a class="code hl_define" href="_depthwise_conv_float_8h.html#a5d6cdde4feb9761abaa001f402b79f27">TFMINI_USE_DEPTHWISECONV_KERNEL</a>(true, 1, 20)</div>
<div class="line"><span class="lineno"> 1106</span>  <a class="code hl_define" href="_depthwise_conv_float_8h.html#a5d6cdde4feb9761abaa001f402b79f27">TFMINI_USE_DEPTHWISECONV_KERNEL</a>(true, 1, 32)</div>
<div class="line"><span class="lineno"> 1107</span>  <a class="code hl_define" href="_depthwise_conv_float_8h.html#a5d6cdde4feb9761abaa001f402b79f27">TFMINI_USE_DEPTHWISECONV_KERNEL</a>(true, 2, 1)</div>
<div class="line"><span class="lineno"> 1108</span>  <a class="code hl_define" href="_depthwise_conv_float_8h.html#a5d6cdde4feb9761abaa001f402b79f27">TFMINI_USE_DEPTHWISECONV_KERNEL</a>(true, 3, 2)</div>
<div class="line"><span class="lineno"> 1109</span>  <a class="code hl_define" href="_depthwise_conv_float_8h.html#a5d6cdde4feb9761abaa001f402b79f27">TFMINI_USE_DEPTHWISECONV_KERNEL</a>(true, 3, 4)</div>
<div class="line"><span class="lineno"> 1110</span>  <a class="code hl_define" href="_depthwise_conv_float_8h.html#a5d6cdde4feb9761abaa001f402b79f27">TFMINI_USE_DEPTHWISECONV_KERNEL</a>(true, 4, 1)</div>
<div class="line"><span class="lineno"> 1111</span> </div>
<div class="line"><span class="lineno"> 1112</span>  <span class="comment">// Finally, the kernels allowing a variable input depth,</span></div>
<div class="line"><span class="lineno"> 1113</span>  <span class="comment">// these are the least efficient but most general kernels.</span></div>
<div class="line"><span class="lineno"> 1114</span> </div>
<div class="line"><span class="lineno"> 1115</span>  <a class="code hl_define" href="_depthwise_conv_float_8h.html#a5d6cdde4feb9761abaa001f402b79f27">TFMINI_USE_DEPTHWISECONV_KERNEL</a>(true, 0, 1)</div>
<div class="line"><span class="lineno"> 1116</span>  <a class="code hl_define" href="_depthwise_conv_float_8h.html#a5d6cdde4feb9761abaa001f402b79f27">TFMINI_USE_DEPTHWISECONV_KERNEL</a>(true, 0, 2)</div>
<div class="line"><span class="lineno"> 1117</span>  <a class="code hl_define" href="_depthwise_conv_float_8h.html#a5d6cdde4feb9761abaa001f402b79f27">TFMINI_USE_DEPTHWISECONV_KERNEL</a>(true, 0, 8)</div>
<div class="line"><span class="lineno"> 1118</span>  <a class="code hl_define" href="_depthwise_conv_float_8h.html#a5d6cdde4feb9761abaa001f402b79f27">TFMINI_USE_DEPTHWISECONV_KERNEL</a>(true, 0, 16)</div>
<div class="line"><span class="lineno"> 1119</span> </div>
<div class="line"><span class="lineno"> 1120</span><span class="preprocessor">#endif </span><span class="comment">// USE_NEON</span></div>
<div class="line"><span class="lineno"> 1121</span> </div>
<div class="line"><span class="lineno"> 1122</span><span class="preprocessor">#undef TFMINI_USE_DEPTHWISECONV_KERNEL</span></div>
<div class="line"><span class="lineno"> 1123</span> </div>
<div class="line"><span class="lineno"> 1124</span>  <span class="comment">// No matching fast kernel found, use slow fallback.</span></div>
<div class="line"><span class="lineno"> 1125</span>  <span class="keywordflow">if</span> (!row_accum_func)</div>
<div class="line"><span class="lineno"> 1126</span>  {</div>
<div class="line"><span class="lineno"> 1127</span>    row_accum_func = <a class="code hl_function" href="namespacennfw_1_1cker_1_1optimized.html#ad4525396811538b4ed0ec83ded38aadf">FloatDepthwiseConvAccumRowGeneric</a>;</div>
<div class="line"><span class="lineno"> 1128</span>  }</div>
<div class="line"><span class="lineno"> 1129</span> </div>
<div class="line"><span class="lineno"> 1130</span>  <span class="keyword">const</span> <span class="keywordtype">int</span> input_height_stride = input_shape.<a class="code hl_function" href="classnnfw_1_1cker_1_1_shape.html#a497180ee7844bbef51b36bd58e61fa31">Dims</a>(3) * input_shape.<a class="code hl_function" href="classnnfw_1_1cker_1_1_shape.html#a497180ee7844bbef51b36bd58e61fa31">Dims</a>(2);</div>
<div class="line"><span class="lineno"> 1131</span>  <span class="keyword">const</span> <span class="keywordtype">int</span> input_batch_stride = input_height_stride * input_shape.<a class="code hl_function" href="classnnfw_1_1cker_1_1_shape.html#a497180ee7844bbef51b36bd58e61fa31">Dims</a>(1);</div>
<div class="line"><span class="lineno"> 1132</span>  <span class="keyword">const</span> <span class="keywordtype">int</span> filter_height_stride = filter_shape.<a class="code hl_function" href="classnnfw_1_1cker_1_1_shape.html#a497180ee7844bbef51b36bd58e61fa31">Dims</a>(3) * filter_shape.<a class="code hl_function" href="classnnfw_1_1cker_1_1_shape.html#a497180ee7844bbef51b36bd58e61fa31">Dims</a>(2);</div>
<div class="line"><span class="lineno"> 1133</span> </div>
<div class="line"><span class="lineno"> 1134</span>  <span class="comment">// Now that we have determined row_accum_func, we can start work.</span></div>
<div class="line"><span class="lineno"> 1135</span>  <span class="keywordtype">int</span> batch_start = 0;</div>
<div class="line"><span class="lineno"> 1136</span>  <span class="keywordtype">int</span> batch_end = batches;</div>
<div class="line"><span class="lineno"> 1137</span>  <span class="keywordtype">int</span> row_start = 0;</div>
<div class="line"><span class="lineno"> 1138</span>  <span class="keywordtype">int</span> row_end = output_height;</div>
<div class="line"><span class="lineno"> 1139</span>  <span class="keywordtype">int</span> output_ptr_offset = 0;</div>
<div class="line"><span class="lineno"> 1140</span> </div>
<div class="line"><span class="lineno"> 1141</span>  <span class="keywordflow">switch</span> (thread_dim)</div>
<div class="line"><span class="lineno"> 1142</span>  {</div>
<div class="line"><span class="lineno"> 1143</span>    <span class="keywordflow">case</span> 0:</div>
<div class="line"><span class="lineno"> 1144</span>      <span class="comment">// Multithread along with the batch axis</span></div>
<div class="line"><span class="lineno"> 1145</span>      assert(thread_start &gt;= 0);</div>
<div class="line"><span class="lineno"> 1146</span>      assert(thread_end &lt;= batches);</div>
<div class="line"><span class="lineno"> 1147</span>      batch_start = thread_start;</div>
<div class="line"><span class="lineno"> 1148</span>      batch_end = thread_end;</div>
<div class="line"><span class="lineno"> 1149</span>      output_ptr_offset = batch_start * <a class="code hl_function" href="namespacennfw_1_1cker.html#a59fdb0b28360a8fcf55b1080b63eefb3">FlatSizeSkipDim</a>(output_shape, 0);</div>
<div class="line"><span class="lineno"> 1150</span>      <span class="keywordflow">break</span>;</div>
<div class="line"><span class="lineno"> 1151</span>    <span class="keywordflow">case</span> 1:</div>
<div class="line"><span class="lineno"> 1152</span>      <span class="comment">// Multithread along with the row axis</span></div>
<div class="line"><span class="lineno"> 1153</span>      assert(thread_start &gt;= 0);</div>
<div class="line"><span class="lineno"> 1154</span>      assert(thread_end &lt;= output_height);</div>
<div class="line"><span class="lineno"> 1155</span>      row_start = thread_start;</div>
<div class="line"><span class="lineno"> 1156</span>      row_end = thread_end;</div>
<div class="line"><span class="lineno"> 1157</span>      output_ptr_offset = row_start * output_width * output_depth;</div>
<div class="line"><span class="lineno"> 1158</span>      <span class="keywordflow">break</span>;</div>
<div class="line"><span class="lineno"> 1159</span>  }</div>
<div class="line"><span class="lineno"> 1160</span> </div>
<div class="line"><span class="lineno"> 1161</span>  <span class="keywordtype">float</span> *output_ptr = output_data + output_ptr_offset;</div>
<div class="line"><span class="lineno"> 1162</span>  <span class="keywordtype">int</span> batch_step = (output_height + row_start - row_end) * output_width * output_depth;</div>
<div class="line"><span class="lineno"> 1163</span> </div>
<div class="line"><span class="lineno"> 1164</span>  <span class="keywordflow">for</span> (<span class="keywordtype">int</span> b = batch_start; b &lt; batch_end; ++b)</div>
<div class="line"><span class="lineno"> 1165</span>  {</div>
<div class="line"><span class="lineno"> 1166</span>    <span class="keywordflow">for</span> (<span class="keywordtype">int</span> out_y = row_start; out_y &lt; row_end; ++out_y)</div>
<div class="line"><span class="lineno"> 1167</span>    {</div>
<div class="line"><span class="lineno"> 1168</span>      <span class="keyword">const</span> <span class="keywordtype">int</span> in_y_origin = (out_y * stride_height) - pad_height;</div>
<div class="line"><span class="lineno"> 1169</span>      <span class="keyword">const</span> <span class="keywordtype">int</span> filter_y_start =</div>
<div class="line"><span class="lineno"> 1170</span>        std::max(0, (-in_y_origin + dilation_height_factor - 1) / dilation_height_factor);</div>
<div class="line"><span class="lineno"> 1171</span>      <span class="keyword">const</span> <span class="keywordtype">int</span> filter_y_end =</div>
<div class="line"><span class="lineno"> 1172</span>        std::min(filter_height, (input_height - in_y_origin + dilation_height_factor - 1) /</div>
<div class="line"><span class="lineno"> 1173</span>                                  dilation_height_factor);</div>
<div class="line"><span class="lineno"> 1174</span>      <span class="keywordflow">for</span> (<span class="keywordtype">int</span> out_x_buffer_start = 0; out_x_buffer_start &lt; output_width;</div>
<div class="line"><span class="lineno"> 1175</span>           out_x_buffer_start += kOutputPixelsInAccBuffer)</div>
<div class="line"><span class="lineno"> 1176</span>      {</div>
<div class="line"><span class="lineno"> 1177</span>        <span class="keyword">const</span> <span class="keywordtype">int</span> out_x_buffer_end =</div>
<div class="line"><span class="lineno"> 1178</span>          std::min(output_width, out_x_buffer_start + kOutputPixelsInAccBuffer);</div>
<div class="line"><span class="lineno"> 1179</span>        <span class="comment">// We call a &#39;pixel&#39; a group of activation that share all but the</span></div>
<div class="line"><span class="lineno"> 1180</span>        <span class="comment">// &#39;depth&#39;/&#39;channel&#39; coordinate. num_output_pixels is the number of</span></div>
<div class="line"><span class="lineno"> 1181</span>        <span class="comment">// output pixels that we will accumulate in this loop iteration.</span></div>
<div class="line"><span class="lineno"> 1182</span>        <span class="keyword">const</span> <span class="keywordtype">int</span> num_output_pixels = out_x_buffer_end - out_x_buffer_start;</div>
<div class="line"><span class="lineno"> 1183</span>        <span class="comment">// Initialize our local accumulator with the bias values, so we don&#39;t</span></div>
<div class="line"><span class="lineno"> 1184</span>        <span class="comment">// have to add them later.</span></div>
<div class="line"><span class="lineno"> 1185</span>        DepthwiseConvInitAccBuffer(num_output_pixels, output_depth, bias_data, acc_buffer);</div>
<div class="line"><span class="lineno"> 1186</span>        <span class="comment">// Accumulation loop. Most of the time should be spent in here.</span></div>
<div class="line"><span class="lineno"> 1187</span>        <span class="keywordflow">for</span> (<span class="keywordtype">int</span> filter_y = filter_y_start; filter_y &lt; filter_y_end; ++filter_y)</div>
<div class="line"><span class="lineno"> 1188</span>        {</div>
<div class="line"><span class="lineno"> 1189</span>          <span class="keyword">const</span> <span class="keywordtype">int</span> in_y = in_y_origin + dilation_height_factor * filter_y;</div>
<div class="line"><span class="lineno"> 1190</span>          row_accum_func(stride_width, dilation_width_factor, input_depth, input_width,</div>
<div class="line"><span class="lineno"> 1191</span>                         input_data + in_y * input_height_stride + b * input_batch_stride,</div>
<div class="line"><span class="lineno"> 1192</span>                         pad_width, depth_multiplier, filter_width,</div>
<div class="line"><span class="lineno"> 1193</span>                         filter_data + filter_y * filter_height_stride, out_x_buffer_start,</div>
<div class="line"><span class="lineno"> 1194</span>                         out_x_buffer_end, output_depth, acc_buffer);</div>
<div class="line"><span class="lineno"> 1195</span>        }</div>
<div class="line"><span class="lineno"> 1196</span>        <span class="comment">// Finished accumulating. Now store to destination.</span></div>
<div class="line"><span class="lineno"> 1197</span>        <span class="keyword">const</span> <span class="keywordtype">int</span> num_output_values = output_depth * num_output_pixels;</div>
<div class="line"><span class="lineno"> 1198</span>        <span class="keywordtype">int</span> i = 0;</div>
<div class="line"><span class="lineno"> 1199</span><span class="comment">// TODO(benoitjacob) optimized code goes here</span></div>
<div class="line"><span class="lineno"> 1200</span><span class="preprocessor">#ifdef USE_NEON</span></div>
<div class="line"><span class="lineno"> 1201</span>        <span class="comment">// Handle 16 values at a time</span></div>
<div class="line"><span class="lineno"> 1202</span>        <span class="keywordflow">for</span> (; i &lt;= num_output_values - 16; i += 16)</div>
<div class="line"><span class="lineno"> 1203</span>        {</div>
<div class="line"><span class="lineno"> 1204</span>          float32x4_t acc[4];</div>
<div class="line"><span class="lineno"> 1205</span>          <span class="keywordflow">for</span> (<span class="keywordtype">int</span> k = 0; k &lt; 4; k++)</div>
<div class="line"><span class="lineno"> 1206</span>          {</div>
<div class="line"><span class="lineno"> 1207</span>            acc[k] = vld1q_f32(acc_buffer + i + 4 * k);</div>
<div class="line"><span class="lineno"> 1208</span>          }</div>
<div class="line"><span class="lineno"> 1209</span>          <span class="keywordflow">for</span> (<span class="keywordtype">int</span> k = 0; k &lt; 4; k++)</div>
<div class="line"><span class="lineno"> 1210</span>          {</div>
<div class="line"><span class="lineno"> 1211</span>            acc[k] = vmaxq_f32(vdupq_n_f32(output_activation_min),</div>
<div class="line"><span class="lineno"> 1212</span>                               vminq_f32(vdupq_n_f32(output_activation_max), acc[k]));</div>
<div class="line"><span class="lineno"> 1213</span>          }</div>
<div class="line"><span class="lineno"> 1214</span>          <span class="keywordflow">for</span> (<span class="keywordtype">int</span> k = 0; k &lt; 4; k++)</div>
<div class="line"><span class="lineno"> 1215</span>          {</div>
<div class="line"><span class="lineno"> 1216</span>            vst1q_f32(output_ptr + 4 * k, acc[k]);</div>
<div class="line"><span class="lineno"> 1217</span>          }</div>
<div class="line"><span class="lineno"> 1218</span>          output_ptr += 16;</div>
<div class="line"><span class="lineno"> 1219</span>        }</div>
<div class="line"><span class="lineno"> 1220</span>        <span class="comment">// Handle 4 values at a time</span></div>
<div class="line"><span class="lineno"> 1221</span>        <span class="keywordflow">for</span> (; i &lt;= num_output_values - 4; i += 4)</div>
<div class="line"><span class="lineno"> 1222</span>        {</div>
<div class="line"><span class="lineno"> 1223</span>          float32x4_t acc = vld1q_f32(acc_buffer + i);</div>
<div class="line"><span class="lineno"> 1224</span> </div>
<div class="line"><span class="lineno"> 1225</span>          acc = vmaxq_f32(vdupq_n_f32(output_activation_min),</div>
<div class="line"><span class="lineno"> 1226</span>                          vminq_f32(vdupq_n_f32(output_activation_max), acc));</div>
<div class="line"><span class="lineno"> 1227</span> </div>
<div class="line"><span class="lineno"> 1228</span>          vst1q_f32(output_ptr, acc);</div>
<div class="line"><span class="lineno"> 1229</span>          output_ptr += 4;</div>
<div class="line"><span class="lineno"> 1230</span>        }</div>
<div class="line"><span class="lineno"> 1231</span><span class="preprocessor">#endif</span></div>
<div class="line"><span class="lineno"> 1232</span>        <span class="comment">// Handle leftover values, one by one. This is very slow.</span></div>
<div class="line"><span class="lineno"> 1233</span>        <span class="keywordflow">for</span> (; i &lt; num_output_values; i++)</div>
<div class="line"><span class="lineno"> 1234</span>        {</div>
<div class="line"><span class="lineno"> 1235</span>          <span class="keywordtype">float</span> acc = acc_buffer[i];</div>
<div class="line"><span class="lineno"> 1236</span>          acc = std::max(output_activation_min, std::min(output_activation_max, acc));</div>
<div class="line"><span class="lineno"> 1237</span> </div>
<div class="line"><span class="lineno"> 1238</span>          *output_ptr++ = acc;</div>
<div class="line"><span class="lineno"> 1239</span>        }</div>
<div class="line"><span class="lineno"> 1240</span>      }</div>
<div class="line"><span class="lineno"> 1241</span>    }</div>
<div class="line"><span class="lineno"> 1242</span>    output_ptr += batch_step;</div>
<div class="line"><span class="lineno"> 1243</span>  }</div>
<div class="line"><span class="lineno"> 1244</span>}</div>
<div class="ttc" id="a_depthwise_conv_float_8h_html_a5d6cdde4feb9761abaa001f402b79f27"><div class="ttname"><a href="_depthwise_conv_float_8h.html#a5d6cdde4feb9761abaa001f402b79f27">TFMINI_USE_DEPTHWISECONV_KERNEL</a></div><div class="ttdeci">#define TFMINI_USE_DEPTHWISECONV_KERNEL(ALLOW_STRIDED, FIXED_INPUT_DEPTH, FIXED_DEPTH_MULTIPLIER)</div></div>
<div class="ttc" id="anamespacennfw_1_1cker_1_1optimized_html_ad4525396811538b4ed0ec83ded38aadf"><div class="ttname"><a href="namespacennfw_1_1cker_1_1optimized.html#ad4525396811538b4ed0ec83ded38aadf">nnfw::cker::optimized::FloatDepthwiseConvAccumRowGeneric</a></div><div class="ttdeci">void FloatDepthwiseConvAccumRowGeneric(int stride, int dilation_factor, int input_depth, int input_width, const float *input_data, int pad_width, int depth_multiplier, int filter_width, const float *filter_data, int out_x_buffer_start, int out_x_buffer_end, int output_depth, float *acc_buffer)</div><div class="ttdef"><b>Definition:</b> <a href="_depthwise_conv_float_8h_source.html#l00977">DepthwiseConvFloat.h:977</a></div></div>
<div class="ttc" id="anamespacennfw_1_1cker_html_a0a74e72b1bffcb6519de7fc224416ff9"><div class="ttname"><a href="namespacennfw_1_1cker.html#a0a74e72b1bffcb6519de7fc224416ff9">nnfw::cker::MatchingDim</a></div><div class="ttdeci">int MatchingDim(const Shape &amp;shape1, int index1, const Shape &amp;shape2, int index2)</div><div class="ttdef"><b>Definition:</b> <a href="compute_2cker_2include_2cker_2_shape_8h_source.html#l00222">Shape.h:222</a></div></div>
<div class="ttc" id="anamespacennfw_1_1cker_html_a59fdb0b28360a8fcf55b1080b63eefb3"><div class="ttname"><a href="namespacennfw_1_1cker.html#a59fdb0b28360a8fcf55b1080b63eefb3">nnfw::cker::FlatSizeSkipDim</a></div><div class="ttdeci">int FlatSizeSkipDim(const Shape &amp;shape, int skip_dim)</div><div class="ttdef"><b>Definition:</b> <a href="compute_2cker_2include_2cker_2_shape_8h_source.html#l00257">Shape.h:257</a></div></div>
<div class="ttc" id="astructnnfw_1_1cker_1_1_depthwise_conv_params_html_a0591df4ab5a62a8988ced9d209e42fcd"><div class="ttname"><a href="structnnfw_1_1cker_1_1_depthwise_conv_params.html#a0591df4ab5a62a8988ced9d209e42fcd">nnfw::cker::DepthwiseConvParams::float_activation_min</a></div><div class="ttdeci">float float_activation_min</div><div class="ttdef"><b>Definition:</b> <a href="compute_2cker_2include_2cker_2_types_8h_source.html#l00243">Types.h:243</a></div></div>
<div class="ttc" id="astructnnfw_1_1cker_1_1_depthwise_conv_params_html_a2a11fa7f567b0faa4551dfe628d6de2b"><div class="ttname"><a href="structnnfw_1_1cker_1_1_depthwise_conv_params.html#a2a11fa7f567b0faa4551dfe628d6de2b">nnfw::cker::DepthwiseConvParams::dilation_height_factor</a></div><div class="ttdeci">int16_t dilation_height_factor</div><div class="ttdef"><b>Definition:</b> <a href="compute_2cker_2include_2cker_2_types_8h_source.html#l00230">Types.h:230</a></div></div>
<div class="ttc" id="astructnnfw_1_1cker_1_1_depthwise_conv_params_html_a50d46dad3a5d5bec206a3f10f1168768"><div class="ttname"><a href="structnnfw_1_1cker_1_1_depthwise_conv_params.html#a50d46dad3a5d5bec206a3f10f1168768">nnfw::cker::DepthwiseConvParams::dilation_width_factor</a></div><div class="ttdeci">int16_t dilation_width_factor</div><div class="ttdef"><b>Definition:</b> <a href="compute_2cker_2include_2cker_2_types_8h_source.html#l00229">Types.h:229</a></div></div>
<div class="ttc" id="astructnnfw_1_1cker_1_1_depthwise_conv_params_html_a5ad1d3c93ba06b5ac78a2ca6a6d1ff4a"><div class="ttname"><a href="structnnfw_1_1cker_1_1_depthwise_conv_params.html#a5ad1d3c93ba06b5ac78a2ca6a6d1ff4a">nnfw::cker::DepthwiseConvParams::float_activation_max</a></div><div class="ttdeci">float float_activation_max</div><div class="ttdef"><b>Definition:</b> <a href="compute_2cker_2include_2cker_2_types_8h_source.html#l00244">Types.h:244</a></div></div>
<div class="ttc" id="astructnnfw_1_1cker_1_1_depthwise_conv_params_html_a65cb7bfeb2b801d8a574a85da10598ff"><div class="ttname"><a href="structnnfw_1_1cker_1_1_depthwise_conv_params.html#a65cb7bfeb2b801d8a574a85da10598ff">nnfw::cker::DepthwiseConvParams::stride_width</a></div><div class="ttdeci">int16_t stride_width</div><div class="ttdef"><b>Definition:</b> <a href="compute_2cker_2include_2cker_2_types_8h_source.html#l00227">Types.h:227</a></div></div>
<div class="ttc" id="astructnnfw_1_1cker_1_1_depthwise_conv_params_html_aaa0e9f0f50f1ed3c8e25886d51a45f75"><div class="ttname"><a href="structnnfw_1_1cker_1_1_depthwise_conv_params.html#aaa0e9f0f50f1ed3c8e25886d51a45f75">nnfw::cker::DepthwiseConvParams::depth_multiplier</a></div><div class="ttdeci">int16_t depth_multiplier</div><div class="ttdef"><b>Definition:</b> <a href="compute_2cker_2include_2cker_2_types_8h_source.html#l00231">Types.h:231</a></div></div>
<div class="ttc" id="astructnnfw_1_1cker_1_1_depthwise_conv_params_html_ab424f92784d7f28eba2b61dcb1be277b"><div class="ttname"><a href="structnnfw_1_1cker_1_1_depthwise_conv_params.html#ab424f92784d7f28eba2b61dcb1be277b">nnfw::cker::DepthwiseConvParams::padding_values</a></div><div class="ttdeci">PaddingValues padding_values</div><div class="ttdef"><b>Definition:</b> <a href="compute_2cker_2include_2cker_2_types_8h_source.html#l00226">Types.h:226</a></div></div>
<div class="ttc" id="astructnnfw_1_1cker_1_1_depthwise_conv_params_html_ad2dbdcb99cb6434e20c073adf4266bef"><div class="ttname"><a href="structnnfw_1_1cker_1_1_depthwise_conv_params.html#ad2dbdcb99cb6434e20c073adf4266bef">nnfw::cker::DepthwiseConvParams::stride_height</a></div><div class="ttdeci">int16_t stride_height</div><div class="ttdef"><b>Definition:</b> <a href="compute_2cker_2include_2cker_2_types_8h_source.html#l00228">Types.h:228</a></div></div>
<div class="ttc" id="astructnnfw_1_1cker_1_1_padding_values_html_a1dc41a9ebbd61a7c34ed2b5dd2ec10eb"><div class="ttname"><a href="structnnfw_1_1cker_1_1_padding_values.html#a1dc41a9ebbd61a7c34ed2b5dd2ec10eb">nnfw::cker::PaddingValues::height</a></div><div class="ttdeci">int16_t height</div><div class="ttdef"><b>Definition:</b> <a href="compute_2cker_2include_2cker_2_types_8h_source.html#l00069">Types.h:69</a></div></div>
<div class="ttc" id="astructnnfw_1_1cker_1_1_padding_values_html_a2a3dd79d28c30c5da865290160601fd1"><div class="ttname"><a href="structnnfw_1_1cker_1_1_padding_values.html#a2a3dd79d28c30c5da865290160601fd1">nnfw::cker::PaddingValues::width</a></div><div class="ttdeci">int16_t width</div><div class="ttdef"><b>Definition:</b> <a href="compute_2cker_2include_2cker_2_types_8h_source.html#l00068">Types.h:68</a></div></div>
</div><!-- fragment -->
<p class="reference">References <a class="el" href="compute_2cker_2include_2cker_2_types_8h_source.html#l00231">nnfw::cker::DepthwiseConvParams::depth_multiplier</a>, <a class="el" href="_depthwise_conv_float_8h_source.html#l01016">DepthwiseConvInitAccBuffer()</a>, <a class="el" href="compute_2cker_2include_2cker_2_types_8h_source.html#l00230">nnfw::cker::DepthwiseConvParams::dilation_height_factor</a>, <a class="el" href="compute_2cker_2include_2cker_2_types_8h_source.html#l00229">nnfw::cker::DepthwiseConvParams::dilation_width_factor</a>, <a class="el" href="compute_2cker_2include_2cker_2_shape_8h_source.html#l00093">nnfw::cker::Shape::DimensionsCount()</a>, <a class="el" href="compute_2cker_2include_2cker_2_shape_8h_source.html#l00094">nnfw::cker::Shape::Dims()</a>, <a class="el" href="compute_2cker_2include_2cker_2_shape_8h_source.html#l00183">nnfw::cker::Shape::FlatSize()</a>, <a class="el" href="compute_2cker_2include_2cker_2_shape_8h_source.html#l00257">nnfw::cker::FlatSizeSkipDim()</a>, <a class="el" href="compute_2cker_2include_2cker_2_types_8h_source.html#l00244">nnfw::cker::DepthwiseConvParams::float_activation_max</a>, <a class="el" href="compute_2cker_2include_2cker_2_types_8h_source.html#l00243">nnfw::cker::DepthwiseConvParams::float_activation_min</a>, <a class="el" href="_depthwise_conv_float_8h_source.html#l00977">FloatDepthwiseConvAccumRowGeneric()</a>, <a class="el" href="compute_2cker_2include_2cker_2_types_8h_source.html#l00069">nnfw::cker::PaddingValues::height</a>, <a class="el" href="compute_2cker_2include_2cker_2_shape_8h_source.html#l00222">nnfw::cker::MatchingDim()</a>, <a class="el" href="compute_2cker_2include_2cker_2_types_8h_source.html#l00226">nnfw::cker::DepthwiseConvParams::padding_values</a>, <a class="el" href="compute_2cker_2include_2cker_2_types_8h_source.html#l00228">nnfw::cker::DepthwiseConvParams::stride_height</a>, <a class="el" href="compute_2cker_2include_2cker_2_types_8h_source.html#l00227">nnfw::cker::DepthwiseConvParams::stride_width</a>, <a class="el" href="_depthwise_conv_float_8h.html#a5d6cdde4feb9761abaa001f402b79f27">TFMINI_USE_DEPTHWISECONV_KERNEL</a>, <a class="el" href="compute_2cker_2include_2cker_2_shape_8h_source.html#l00026">UNUSED_RELEASE</a>, and <a class="el" href="compute_2cker_2include_2cker_2_types_8h_source.html#l00068">nnfw::cker::PaddingValues::width</a>.</p>

<p class="reference">Referenced by <a class="el" href="_depthwise_conv_8h_source.html#l00121">nnfw::cker::DepthwiseConv()</a>, and <a class="el" href="_depthwise_conv_8h_source.html#l00055">nnfw::cker::DepthwiseConvWorkerTask&lt; T, TS &gt;::Run()</a>.</p>

</div>
</div>
<a id="a7f843a8c10add9f97826e1952b289a07" name="a7f843a8c10add9f97826e1952b289a07"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a7f843a8c10add9f97826e1952b289a07">&#9670;&#160;</a></span>DepthwiseConvImpl() <span class="overload">[2/2]</span></h2>

<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">void nnfw::cker::optimized::DepthwiseConvImpl </td>
          <td>(</td>
          <td class="paramtype">const <a class="el" href="structnnfw_1_1cker_1_1_depthwise_conv_params.html">DepthwiseConvParams</a> &amp;&#160;</td>
          <td class="paramname"><em>params</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const <a class="el" href="classnnfw_1_1cker_1_1_shape.html">Shape</a> &amp;&#160;</td>
          <td class="paramname"><em>input_shape</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const uint8_t *&#160;</td>
          <td class="paramname"><em>input_data</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const <a class="el" href="classnnfw_1_1cker_1_1_shape.html">Shape</a> &amp;&#160;</td>
          <td class="paramname"><em>filter_shape</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const uint8_t *&#160;</td>
          <td class="paramname"><em>filter_data</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const <a class="el" href="classnnfw_1_1cker_1_1_shape.html">Shape</a> &amp;&#160;</td>
          <td class="paramname"><em>bias_shape</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const int32_t *&#160;</td>
          <td class="paramname"><em>bias_data</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const <a class="el" href="classnnfw_1_1cker_1_1_shape.html">Shape</a> &amp;&#160;</td>
          <td class="paramname"><em>output_shape</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">uint8_t *&#160;</td>
          <td class="paramname"><em>output_data</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">int&#160;</td>
          <td class="paramname"><em>thread_start</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">int&#160;</td>
          <td class="paramname"><em>thread_end</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">int&#160;</td>
          <td class="paramname"><em>thread_dim</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">inline</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p class="definition">Definition at line <a class="el" href="optimized_2_depthwise_conv_u_int8_8h_source.html#l02241">2241</a> of file <a class="el" href="optimized_2_depthwise_conv_u_int8_8h_source.html">DepthwiseConvUint8.h</a>.</p>
<div class="fragment"><div class="line"><span class="lineno"> 2247</span>{</div>
<div class="line"><span class="lineno"> 2248</span>  <span class="keywordflow">return</span> <a class="code hl_function" href="namespacennfw_1_1cker_1_1optimized.html#ab9b6acc151e330c9742628a2f37650b6">DepthwiseConvWithRounding</a>(params, input_shape, input_data, filter_shape, filter_data,</div>
<div class="line"><span class="lineno"> 2249</span>                                   bias_shape, bias_data, output_shape, output_data, thread_start,</div>
<div class="line"><span class="lineno"> 2250</span>                                   thread_end, thread_dim);</div>
<div class="line"><span class="lineno"> 2251</span>}</div>
<div class="ttc" id="anamespacennfw_1_1cker_1_1optimized_html_ab9b6acc151e330c9742628a2f37650b6"><div class="ttname"><a href="namespacennfw_1_1cker_1_1optimized.html#ab9b6acc151e330c9742628a2f37650b6">nnfw::cker::optimized::DepthwiseConvWithRounding</a></div><div class="ttdeci">void DepthwiseConvWithRounding(const DepthwiseConvParams &amp;params, const Shape &amp;input_shape, const uint8_t *input_data, const Shape &amp;filter_shape, const uint8_t *filter_data, const Shape &amp;bias_shape, const int32_t *bias_data, const Shape &amp;output_shape, uint8_t *output_data, int thread_start, int thread_end, int thread_dim)</div><div class="ttdef"><b>Definition:</b> <a href="optimized_2_depthwise_conv_u_int8_8h_source.html#l02155">DepthwiseConvUint8.h:2155</a></div></div>
</div><!-- fragment -->
<p class="reference">References <a class="el" href="optimized_2_depthwise_conv_u_int8_8h_source.html#l02155">DepthwiseConvWithRounding()</a>.</p>

</div>
</div>
<a id="a7f4b1eca533bbd4e62a4a44c73f8a803" name="a7f4b1eca533bbd4e62a4a44c73f8a803"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a7f4b1eca533bbd4e62a4a44c73f8a803">&#9670;&#160;</a></span>DepthwiseConvInitAccBuffer()</h2>

<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">void nnfw::cker::optimized::DepthwiseConvInitAccBuffer </td>
          <td>(</td>
          <td class="paramtype">int&#160;</td>
          <td class="paramname"><em>num_output_pixels</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">int&#160;</td>
          <td class="paramname"><em>output_depth</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const float *&#160;</td>
          <td class="paramname"><em>bias_data</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">float *&#160;</td>
          <td class="paramname"><em>acc_buffer</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">inline</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p class="definition">Definition at line <a class="el" href="_depthwise_conv_float_8h_source.html#l01016">1016</a> of file <a class="el" href="_depthwise_conv_float_8h_source.html">DepthwiseConvFloat.h</a>.</p>
<div class="fragment"><div class="line"><span class="lineno"> 1018</span>{</div>
<div class="line"><span class="lineno"> 1019</span>  <span class="comment">// TODO(benoitjacob): This might need optimized specializations</span></div>
<div class="line"><span class="lineno"> 1020</span>  <span class="comment">// for small output_depth values, if that ever becomes an important</span></div>
<div class="line"><span class="lineno"> 1021</span>  <span class="comment">// case (like it was for some quantized DepthwiseConv cases).</span></div>
<div class="line"><span class="lineno"> 1022</span>  <span class="keywordflow">for</span> (<span class="keywordtype">int</span> i = 0; i &lt; num_output_pixels; i++)</div>
<div class="line"><span class="lineno"> 1023</span>  {</div>
<div class="line"><span class="lineno"> 1024</span>    memcpy(acc_buffer + i * output_depth, bias_data, <span class="keyword">sizeof</span>(acc_buffer[0]) * output_depth);</div>
<div class="line"><span class="lineno"> 1025</span>  }</div>
<div class="line"><span class="lineno"> 1026</span>}</div>
</div><!-- fragment -->
<p class="reference">Referenced by <a class="el" href="_depthwise_conv_float_8h_source.html#l01033">DepthwiseConvImpl()</a>.</p>

</div>
</div>
<a id="ab9b6acc151e330c9742628a2f37650b6" name="ab9b6acc151e330c9742628a2f37650b6"></a>
<h2 class="memtitle"><span class="permalink"><a href="#ab9b6acc151e330c9742628a2f37650b6">&#9670;&#160;</a></span>DepthwiseConvWithRounding()</h2>

<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">void nnfw::cker::optimized::DepthwiseConvWithRounding </td>
          <td>(</td>
          <td class="paramtype">const <a class="el" href="structnnfw_1_1cker_1_1_depthwise_conv_params.html">DepthwiseConvParams</a> &amp;&#160;</td>
          <td class="paramname"><em>params</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const <a class="el" href="classnnfw_1_1cker_1_1_shape.html">Shape</a> &amp;&#160;</td>
          <td class="paramname"><em>input_shape</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const uint8_t *&#160;</td>
          <td class="paramname"><em>input_data</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const <a class="el" href="classnnfw_1_1cker_1_1_shape.html">Shape</a> &amp;&#160;</td>
          <td class="paramname"><em>filter_shape</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const uint8_t *&#160;</td>
          <td class="paramname"><em>filter_data</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const <a class="el" href="classnnfw_1_1cker_1_1_shape.html">Shape</a> &amp;&#160;</td>
          <td class="paramname"><em>bias_shape</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const int32_t *&#160;</td>
          <td class="paramname"><em>bias_data</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const <a class="el" href="classnnfw_1_1cker_1_1_shape.html">Shape</a> &amp;&#160;</td>
          <td class="paramname"><em>output_shape</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">uint8_t *&#160;</td>
          <td class="paramname"><em>output_data</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">int&#160;</td>
          <td class="paramname"><em>thread_start</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">int&#160;</td>
          <td class="paramname"><em>thread_end</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">int&#160;</td>
          <td class="paramname"><em>thread_dim</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">inline</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p class="definition">Definition at line <a class="el" href="optimized_2_depthwise_conv_u_int8_8h_source.html#l02155">2155</a> of file <a class="el" href="optimized_2_depthwise_conv_u_int8_8h_source.html">DepthwiseConvUint8.h</a>.</p>
<div class="fragment"><div class="line"><span class="lineno"> 2161</span>{</div>
<div class="line"><span class="lineno"> 2162</span>  <span class="keyword">const</span> <span class="keywordtype">int</span> depth_multiplier = params.<a class="code hl_variable" href="structnnfw_1_1cker_1_1_depthwise_conv_params.html#aaa0e9f0f50f1ed3c8e25886d51a45f75">depth_multiplier</a>;</div>
<div class="line"><span class="lineno"> 2163</span>  <span class="keyword">const</span> int32_t output_activation_min = params.<a class="code hl_variable" href="structnnfw_1_1cker_1_1_depthwise_conv_params.html#a7cc7d4366172494e8a67dc7b69de07b1">quantized_activation_min</a>;</div>
<div class="line"><span class="lineno"> 2164</span>  <span class="keyword">const</span> int32_t output_activation_max = params.<a class="code hl_variable" href="structnnfw_1_1cker_1_1_depthwise_conv_params.html#a54c8882742f94c3f5f601a9d81fce456">quantized_activation_max</a>;</div>
<div class="line"><span class="lineno"> 2165</span>  <span class="keyword">const</span> <span class="keywordtype">int</span> dilation_width_factor = params.<a class="code hl_variable" href="structnnfw_1_1cker_1_1_depthwise_conv_params.html#a50d46dad3a5d5bec206a3f10f1168768">dilation_width_factor</a>;</div>
<div class="line"><span class="lineno"> 2166</span>  <span class="keyword">const</span> <span class="keywordtype">int</span> dilation_height_factor = params.<a class="code hl_variable" href="structnnfw_1_1cker_1_1_depthwise_conv_params.html#a2a11fa7f567b0faa4551dfe628d6de2b">dilation_height_factor</a>;</div>
<div class="line"><span class="lineno"> 2167</span>  assert(dilation_width_factor &gt;= 1);</div>
<div class="line"><span class="lineno"> 2168</span>  assert(dilation_height_factor &gt;= 1);</div>
<div class="line"><span class="lineno"> 2169</span>  assert(input_shape.<a class="code hl_function" href="classnnfw_1_1cker_1_1_shape.html#a01c6b5dc4b24d5c2567c47cb15318839">DimensionsCount</a>() == 4);</div>
<div class="line"><span class="lineno"> 2170</span>  assert(filter_shape.<a class="code hl_function" href="classnnfw_1_1cker_1_1_shape.html#a01c6b5dc4b24d5c2567c47cb15318839">DimensionsCount</a>() == 4);</div>
<div class="line"><span class="lineno"> 2171</span>  assert(output_shape.DimensionsCount() == 4);</div>
<div class="line"><span class="lineno"> 2172</span>  assert(output_activation_min &lt;= output_activation_max);</div>
<div class="line"><span class="lineno"> 2173</span>  <span class="keyword">const</span> <span class="keywordtype">int</span> output_depth = <a class="code hl_function" href="namespacennfw_1_1cker.html#a0a74e72b1bffcb6519de7fc224416ff9">MatchingDim</a>(filter_shape, 3, output_shape, 3);</div>
<div class="line"><span class="lineno"> 2174</span>  <span class="keyword">const</span> <span class="keywordtype">int</span> input_depth = input_shape.<a class="code hl_function" href="classnnfw_1_1cker_1_1_shape.html#a497180ee7844bbef51b36bd58e61fa31">Dims</a>(3);</div>
<div class="line"><span class="lineno"> 2175</span>  assert(output_depth == input_depth * depth_multiplier);</div>
<div class="line"><span class="lineno"> 2176</span>  assert(bias_shape.<a class="code hl_function" href="classnnfw_1_1cker_1_1_shape.html#ac7e29fd510111992fbd44906e2080f12">FlatSize</a>() == output_depth);</div>
<div class="line"><span class="lineno"> 2177</span> </div>
<div class="line"><span class="lineno"> 2178</span>  <a class="code hl_define" href="compute_2cker_2include_2cker_2_shape_8h.html#a044d7f999713173dd3360b0aa40370fd">UNUSED_RELEASE</a>(depth_multiplier);</div>
<div class="line"><span class="lineno"> 2179</span>  <a class="code hl_define" href="compute_2cker_2include_2cker_2_shape_8h.html#a044d7f999713173dd3360b0aa40370fd">UNUSED_RELEASE</a>(output_activation_min);</div>
<div class="line"><span class="lineno"> 2180</span>  <a class="code hl_define" href="compute_2cker_2include_2cker_2_shape_8h.html#a044d7f999713173dd3360b0aa40370fd">UNUSED_RELEASE</a>(output_activation_max);</div>
<div class="line"><span class="lineno"> 2181</span>  <a class="code hl_define" href="compute_2cker_2include_2cker_2_shape_8h.html#a044d7f999713173dd3360b0aa40370fd">UNUSED_RELEASE</a>(dilation_width_factor);</div>
<div class="line"><span class="lineno"> 2182</span>  <a class="code hl_define" href="compute_2cker_2include_2cker_2_shape_8h.html#a044d7f999713173dd3360b0aa40370fd">UNUSED_RELEASE</a>(dilation_height_factor);</div>
<div class="line"><span class="lineno"> 2183</span>  <a class="code hl_define" href="compute_2cker_2include_2cker_2_shape_8h.html#a044d7f999713173dd3360b0aa40370fd">UNUSED_RELEASE</a>(output_depth);</div>
<div class="line"><span class="lineno"> 2184</span>  <a class="code hl_define" href="compute_2cker_2include_2cker_2_shape_8h.html#a044d7f999713173dd3360b0aa40370fd">UNUSED_RELEASE</a>(input_depth);</div>
<div class="line"><span class="lineno"> 2185</span> </div>
<div class="line"><span class="lineno"> 2186</span><span class="comment">// Enable for arm64 except for the Nvidia Linux 4 Tegra (L4T) running on</span></div>
<div class="line"><span class="lineno"> 2187</span><span class="comment">// Jetson TX-2. This compiler does not support the offsetof() macro.</span></div>
<div class="line"><span class="lineno"> 2188</span><span class="preprocessor">#if defined(__aarch64__) &amp;&amp; !defined(GOOGLE_L4T)</span></div>
<div class="line"><span class="lineno"> 2189</span><span class="comment">//  TODO Use below codes</span></div>
<div class="line"><span class="lineno"> 2190</span><span class="comment">//  // Dispatch to dot-product 3x3 kernels when supported.</span></div>
<div class="line"><span class="lineno"> 2191</span><span class="comment">//</span></div>
<div class="line"><span class="lineno"> 2192</span><span class="comment">//  ruy::Context *ruy_context = cpu_backend_context-&gt;ruy_context();</span></div>
<div class="line"><span class="lineno"> 2193</span><span class="comment">//  const bool has_dot_product_instructions =</span></div>
<div class="line"><span class="lineno"> 2194</span><span class="comment">//      ruy_context != nullptr &amp;&amp;</span></div>
<div class="line"><span class="lineno"> 2195</span><span class="comment">//      (ruy_context-&gt;GetRuntimeEnabledPaths() &amp; ruy::Path::kNeonDotprod) != ruy::Path::kNone;</span></div>
<div class="line"><span class="lineno"> 2196</span><span class="comment">//  if (has_dot_product_instructions)</span></div>
<div class="line"><span class="lineno"> 2197</span><span class="comment">//  {</span></div>
<div class="line"><span class="lineno"> 2198</span><span class="comment">//    using optimized_ops::depthwise_conv::DotProduct3x3KernelType;</span></div>
<div class="line"><span class="lineno"> 2199</span><span class="comment">//    DotProduct3x3KernelType kernel_type =</span></div>
<div class="line"><span class="lineno"> 2200</span><span class="comment">//    optimized_ops::depthwise_conv::CategorizeDotProductKernel(</span></div>
<div class="line"><span class="lineno"> 2201</span><span class="comment">//        input_shape, filter_shape, params);</span></div>
<div class="line"><span class="lineno"> 2202</span><span class="comment">//    if (kernel_type != DotProduct3x3KernelType::kNone)</span></div>
<div class="line"><span class="lineno"> 2203</span><span class="comment">//    {</span></div>
<div class="line"><span class="lineno"> 2204</span><span class="comment">//      optimized_ops::depthwise_conv::DepthwiseConvDotProduct3x3&lt;</span></div>
<div class="line"><span class="lineno"> 2205</span><span class="comment">//          DepthwiseConvImplementation::kUseNeon3x3DotProduct&gt;(params, input_shape, input_data,</span></div>
<div class="line"><span class="lineno"> 2206</span><span class="comment">//                                                              filter_shape, filter_data,</span></div>
<div class="line"><span class="lineno"> 2207</span><span class="comment">//                                                              bias_shape,</span></div>
<div class="line"><span class="lineno"> 2208</span><span class="comment">//                                                              bias_data, output_shape,</span></div>
<div class="line"><span class="lineno"> 2209</span><span class="comment">//                                                              output_data);</span></div>
<div class="line"><span class="lineno"> 2210</span><span class="comment">//      return;</span></div>
<div class="line"><span class="lineno"> 2211</span><span class="comment">//    }</span></div>
<div class="line"><span class="lineno"> 2212</span><span class="comment">//  }</span></div>
<div class="line"><span class="lineno"> 2213</span><span class="comment">//</span></div>
<div class="line"><span class="lineno"> 2214</span><span class="comment">//  // Dispatch to non-dot-product 3x3 kernels when supported.</span></div>
<div class="line"><span class="lineno"> 2215</span><span class="comment">//</span></div>
<div class="line"><span class="lineno"> 2216</span><span class="comment">//  const int stride_width = params.stride_width;</span></div>
<div class="line"><span class="lineno"> 2217</span><span class="comment">//  const int stride_height = params.stride_height;</span></div>
<div class="line"><span class="lineno"> 2218</span><span class="comment">//  const int pad_width = params.padding_values.width;</span></div>
<div class="line"><span class="lineno"> 2219</span><span class="comment">//  const int pad_height = params.padding_values.height;</span></div>
<div class="line"><span class="lineno"> 2220</span><span class="comment">//  const int output_shift = params.output_shift;</span></div>
<div class="line"><span class="lineno"> 2221</span><span class="comment">//</span></div>
<div class="line"><span class="lineno"> 2222</span><span class="comment">//  // Call kernel optimized for depthwise convolutions using 3x3 filters if</span></div>
<div class="line"><span class="lineno"> 2223</span><span class="comment">//  // parameters are supported.</span></div>
<div class="line"><span class="lineno"> 2224</span><span class="comment">//  if (depthwise_conv::Fast3x3FilterKernelSupported(input_shape, filter_shape, stride_width,</span></div>
<div class="line"><span class="lineno"> 2225</span><span class="comment">//                                                   stride_height, dilation_width_factor,</span></div>
<div class="line"><span class="lineno"> 2226</span><span class="comment">//                                                   dilation_height_factor, pad_width, pad_height,</span></div>
<div class="line"><span class="lineno"> 2227</span><span class="comment">//                                                   depth_multiplier, output_shape, output_shift))</span></div>
<div class="line"><span class="lineno"> 2228</span><span class="comment">//  {</span></div>
<div class="line"><span class="lineno"> 2229</span><span class="comment">//    depthwise_conv::DepthwiseConv3x3Filter&lt;kOutputRounding&gt;(</span></div>
<div class="line"><span class="lineno"> 2230</span><span class="comment">//        params, input_shape, input_data, filter_shape, filter_data, bias_shape, bias_data,</span></div>
<div class="line"><span class="lineno"> 2231</span><span class="comment">//        output_shape, output_data, thread_start, thread_end, thread_dim);</span></div>
<div class="line"><span class="lineno"> 2232</span><span class="comment">//    return;</span></div>
<div class="line"><span class="lineno"> 2233</span><span class="comment">//  }</span></div>
<div class="line"><span class="lineno"> 2234</span><span class="preprocessor">#endif</span></div>
<div class="line"><span class="lineno"> 2235</span> </div>
<div class="line"><span class="lineno"> 2236</span>  depthwise_conv::DepthwiseConvGeneral(params, input_shape, input_data, filter_shape, filter_data,</div>
<div class="line"><span class="lineno"> 2237</span>                                       bias_shape, bias_data, output_shape, output_data,</div>
<div class="line"><span class="lineno"> 2238</span>                                       thread_start, thread_end, thread_dim);</div>
<div class="line"><span class="lineno"> 2239</span>}</div>
<div class="ttc" id="astructnnfw_1_1cker_1_1_depthwise_conv_params_html_a54c8882742f94c3f5f601a9d81fce456"><div class="ttname"><a href="structnnfw_1_1cker_1_1_depthwise_conv_params.html#a54c8882742f94c3f5f601a9d81fce456">nnfw::cker::DepthwiseConvParams::quantized_activation_max</a></div><div class="ttdeci">int32_t quantized_activation_max</div><div class="ttdef"><b>Definition:</b> <a href="compute_2cker_2include_2cker_2_types_8h_source.html#l00241">Types.h:241</a></div></div>
<div class="ttc" id="astructnnfw_1_1cker_1_1_depthwise_conv_params_html_a7cc7d4366172494e8a67dc7b69de07b1"><div class="ttname"><a href="structnnfw_1_1cker_1_1_depthwise_conv_params.html#a7cc7d4366172494e8a67dc7b69de07b1">nnfw::cker::DepthwiseConvParams::quantized_activation_min</a></div><div class="ttdeci">int32_t quantized_activation_min</div><div class="ttdef"><b>Definition:</b> <a href="compute_2cker_2include_2cker_2_types_8h_source.html#l00240">Types.h:240</a></div></div>
</div><!-- fragment -->
<p class="reference">References <a class="el" href="compute_2cker_2include_2cker_2_types_8h_source.html#l00231">nnfw::cker::DepthwiseConvParams::depth_multiplier</a>, <a class="el" href="optimized_2_depthwise_conv_u_int8_8h_source.html#l01814">nnfw::cker::optimized::depthwise_conv::DepthwiseConvGeneral()</a>, <a class="el" href="compute_2cker_2include_2cker_2_types_8h_source.html#l00230">nnfw::cker::DepthwiseConvParams::dilation_height_factor</a>, <a class="el" href="compute_2cker_2include_2cker_2_types_8h_source.html#l00229">nnfw::cker::DepthwiseConvParams::dilation_width_factor</a>, <a class="el" href="compute_2cker_2include_2cker_2_shape_8h_source.html#l00093">nnfw::cker::Shape::DimensionsCount()</a>, <a class="el" href="compute_2cker_2include_2cker_2_shape_8h_source.html#l00094">nnfw::cker::Shape::Dims()</a>, <a class="el" href="compute_2cker_2include_2cker_2_shape_8h_source.html#l00183">nnfw::cker::Shape::FlatSize()</a>, <a class="el" href="compute_2cker_2include_2cker_2_shape_8h_source.html#l00222">nnfw::cker::MatchingDim()</a>, <a class="el" href="compute_2cker_2include_2cker_2_types_8h_source.html#l00241">nnfw::cker::DepthwiseConvParams::quantized_activation_max</a>, <a class="el" href="compute_2cker_2include_2cker_2_types_8h_source.html#l00240">nnfw::cker::DepthwiseConvParams::quantized_activation_min</a>, and <a class="el" href="compute_2cker_2include_2cker_2_shape_8h_source.html#l00026">UNUSED_RELEASE</a>.</p>

<p class="reference">Referenced by <a class="el" href="optimized_2_depthwise_conv_u_int8_8h_source.html#l02241">DepthwiseConvImpl()</a>.</p>

</div>
</div>
<a id="a403e08d865d6f45ba561998b93304d57" name="a403e08d865d6f45ba561998b93304d57"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a403e08d865d6f45ba561998b93304d57">&#9670;&#160;</a></span>DilatedIm2col() <span class="overload">[1/2]</span></h2>

<div class="memitem">
<div class="memproto">
<div class="memtemplate">
template&lt;typename T &gt; </div>
      <table class="memname">
        <tr>
          <td class="memname">void nnfw::cker::optimized::DilatedIm2col </td>
          <td>(</td>
          <td class="paramtype">const <a class="el" href="structnnfw_1_1cker_1_1_conv_params.html">ConvParams</a> &amp;&#160;</td>
          <td class="paramname"><em>params</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const <a class="el" href="classnnfw_1_1cker_1_1_shape.html">Shape</a> &amp;&#160;</td>
          <td class="paramname"><em>input_shape</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const T *&#160;</td>
          <td class="paramname"><em>input_data</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const <a class="el" href="classnnfw_1_1cker_1_1_shape.html">Shape</a> &amp;&#160;</td>
          <td class="paramname"><em>filter_shape</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const <a class="el" href="classnnfw_1_1cker_1_1_shape.html">Shape</a> &amp;&#160;</td>
          <td class="paramname"><em>output_shape</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">T *&#160;</td>
          <td class="paramname"><em>im2col_data</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const int32_t *&#160;</td>
          <td class="paramname"><em>zero_bytes</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const int&#160;</td>
          <td class="paramname"><em>zero_bytes_len</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">

<p class="definition">Definition at line <a class="el" href="_optimized_utils_8h_source.html#l00121">121</a> of file <a class="el" href="_optimized_utils_8h_source.html">OptimizedUtils.h</a>.</p>
<div class="fragment"><div class="line"><span class="lineno">  124</span>{</div>
<div class="line"><span class="lineno">  125</span>  <span class="keyword">const</span> <span class="keywordtype">int</span> stride_width = params.<a class="code hl_variable" href="structnnfw_1_1cker_1_1_conv_params.html#a273ecef205d663377502d9dcaa77f1b4">stride_width</a>;</div>
<div class="line"><span class="lineno">  126</span>  <span class="keyword">const</span> <span class="keywordtype">int</span> stride_height = params.<a class="code hl_variable" href="structnnfw_1_1cker_1_1_conv_params.html#a0878ee74c474c8a82a574727a2a3048e">stride_height</a>;</div>
<div class="line"><span class="lineno">  127</span>  <span class="keyword">const</span> <span class="keywordtype">int</span> dilation_width_factor = params.<a class="code hl_variable" href="structnnfw_1_1cker_1_1_conv_params.html#a8ff1e952b2e7627e11b47a22da0883cb">dilation_width_factor</a>;</div>
<div class="line"><span class="lineno">  128</span>  <span class="keyword">const</span> <span class="keywordtype">int</span> dilation_height_factor = params.<a class="code hl_variable" href="structnnfw_1_1cker_1_1_conv_params.html#ad47b94004b5b1718979775edaa9eee70">dilation_height_factor</a>;</div>
<div class="line"><span class="lineno">  129</span>  <span class="keyword">const</span> <span class="keywordtype">int</span> pad_width = params.<a class="code hl_variable" href="structnnfw_1_1cker_1_1_conv_params.html#a0a3450e62d20d27c2638d197f1fd2bb2">padding_values</a>.<a class="code hl_variable" href="structnnfw_1_1cker_1_1_padding_values.html#a2a3dd79d28c30c5da865290160601fd1">width</a>;</div>
<div class="line"><span class="lineno">  130</span>  <span class="keyword">const</span> <span class="keywordtype">int</span> pad_height = params.<a class="code hl_variable" href="structnnfw_1_1cker_1_1_conv_params.html#a0a3450e62d20d27c2638d197f1fd2bb2">padding_values</a>.<a class="code hl_variable" href="structnnfw_1_1cker_1_1_padding_values.html#a1dc41a9ebbd61a7c34ed2b5dd2ec10eb">height</a>;</div>
<div class="line"><span class="lineno">  131</span>  assert(input_shape.<a class="code hl_function" href="classnnfw_1_1cker_1_1_shape.html#a01c6b5dc4b24d5c2567c47cb15318839">DimensionsCount</a>() == 4);</div>
<div class="line"><span class="lineno">  132</span>  assert(filter_shape.<a class="code hl_function" href="classnnfw_1_1cker_1_1_shape.html#a01c6b5dc4b24d5c2567c47cb15318839">DimensionsCount</a>() == 4);</div>
<div class="line"><span class="lineno">  133</span>  assert(output_shape.DimensionsCount() == 4);</div>
<div class="line"><span class="lineno">  134</span> </div>
<div class="line"><span class="lineno">  135</span>  <span class="comment">// For dilated convolution, the input pixels are not contiguous therefore we</span></div>
<div class="line"><span class="lineno">  136</span>  <span class="comment">// can&#39;t use the same optimizations as Im2Col(). Though note this code would</span></div>
<div class="line"><span class="lineno">  137</span>  <span class="comment">// work fine for the non-dilated case too (though likely a bit slower).</span></div>
<div class="line"><span class="lineno">  138</span>  assert(dilation_width_factor != 1 || dilation_height_factor != 1);</div>
<div class="line"><span class="lineno">  139</span>  assert(im2col_data);</div>
<div class="line"><span class="lineno">  140</span>  <span class="keyword">const</span> <span class="keywordtype">int</span> batches = <a class="code hl_function" href="namespacennfw_1_1cker.html#a0a74e72b1bffcb6519de7fc224416ff9">MatchingDim</a>(input_shape, 0, output_shape, 0);</div>
<div class="line"><span class="lineno">  141</span>  <span class="keyword">const</span> <span class="keywordtype">int</span> input_height = input_shape.<a class="code hl_function" href="classnnfw_1_1cker_1_1_shape.html#a497180ee7844bbef51b36bd58e61fa31">Dims</a>(1);</div>
<div class="line"><span class="lineno">  142</span>  <span class="keyword">const</span> <span class="keywordtype">int</span> input_width = input_shape.<a class="code hl_function" href="classnnfw_1_1cker_1_1_shape.html#a497180ee7844bbef51b36bd58e61fa31">Dims</a>(2);</div>
<div class="line"><span class="lineno">  143</span>  <span class="keyword">const</span> <span class="keywordtype">int</span> input_depth = <a class="code hl_function" href="namespacennfw_1_1cker.html#a0a74e72b1bffcb6519de7fc224416ff9">MatchingDim</a>(input_shape, 3, filter_shape, 3);</div>
<div class="line"><span class="lineno">  144</span>  <span class="keyword">const</span> <span class="keywordtype">int</span> filter_height = filter_shape.<a class="code hl_function" href="classnnfw_1_1cker_1_1_shape.html#a497180ee7844bbef51b36bd58e61fa31">Dims</a>(1);</div>
<div class="line"><span class="lineno">  145</span>  <span class="keyword">const</span> <span class="keywordtype">int</span> filter_width = filter_shape.<a class="code hl_function" href="classnnfw_1_1cker_1_1_shape.html#a497180ee7844bbef51b36bd58e61fa31">Dims</a>(2);</div>
<div class="line"><span class="lineno">  146</span>  <span class="keyword">const</span> <span class="keywordtype">int</span> output_height = output_shape.Dims(1);</div>
<div class="line"><span class="lineno">  147</span>  <span class="keyword">const</span> <span class="keywordtype">int</span> output_width = output_shape.Dims(2);</div>
<div class="line"><span class="lineno">  148</span>  <a class="code hl_function" href="namespacennfw_1_1cker.html#a0a74e72b1bffcb6519de7fc224416ff9">MatchingDim</a>(output_shape, 3, filter_shape, 0);</div>
<div class="line"><span class="lineno">  149</span> </div>
<div class="line"><span class="lineno">  150</span>  <span class="comment">// Construct the MxN sized im2col matrix.</span></div>
<div class="line"><span class="lineno">  151</span>  <span class="comment">// The rows M, are sub-ordered B x H x W</span></div>
<div class="line"><span class="lineno">  152</span>  <span class="keyword">const</span> <a class="code hl_class" href="classnnfw_1_1cker_1_1_shape.html">Shape</a> row_shape({1, batches, output_height, output_width});</div>
<div class="line"><span class="lineno">  153</span>  <span class="comment">// The columns, N, are sub-ordered Kh x Kw x Din</span></div>
<div class="line"><span class="lineno">  154</span>  <span class="keyword">const</span> <a class="code hl_class" href="classnnfw_1_1cker_1_1_shape.html">Shape</a> col_shape({1, filter_height, filter_width, input_depth});</div>
<div class="line"><span class="lineno">  155</span>  <span class="comment">// Use dimensions M and N to construct dims for indexing directly into im2col</span></div>
<div class="line"><span class="lineno">  156</span>  <span class="keyword">const</span> <a class="code hl_struct" href="struct_shape.html">Shape</a> im2col_shape({1, 1, row_shape.FlatSize(), col_shape.FlatSize()});</div>
<div class="line"><span class="lineno">  157</span> </div>
<div class="line"><span class="lineno">  158</span>  <span class="comment">// Loop through the output rows (B x H x W)</span></div>
<div class="line"><span class="lineno">  159</span>  <span class="keywordflow">for</span> (<span class="keywordtype">int</span> batch = 0; batch &lt; batches; ++batch)</div>
<div class="line"><span class="lineno">  160</span>  {</div>
<div class="line"><span class="lineno">  161</span>    <span class="keyword">const</span> T zero_byte =</div>
<div class="line"><span class="lineno">  162</span>      zero_bytes_len &gt; 1 ? <span class="keyword">static_cast&lt;</span>T<span class="keyword">&gt;</span>(zero_bytes[batch]) : <span class="keyword">static_cast&lt;</span>T<span class="keyword">&gt;</span>(zero_bytes[0]);</div>
<div class="line"><span class="lineno">  163</span>    <span class="keywordflow">for</span> (<span class="keywordtype">int</span> out_y = 0; out_y &lt; output_height; ++out_y)</div>
<div class="line"><span class="lineno">  164</span>    {</div>
<div class="line"><span class="lineno">  165</span>      <span class="keywordflow">for</span> (<span class="keywordtype">int</span> out_x = 0; out_x &lt; output_width; ++out_x)</div>
<div class="line"><span class="lineno">  166</span>      {</div>
<div class="line"><span class="lineno">  167</span>        <span class="comment">// Each im2col row is an output pixel. Arrange the input data in this</span></div>
<div class="line"><span class="lineno">  168</span>        <span class="comment">// row in an order we can conveniently multiply with the filter data.</span></div>
<div class="line"><span class="lineno">  169</span>        <span class="keywordtype">int</span> row_offset = <a class="code hl_function" href="ann-ref_2src_2ops_2internal_2dims_8h.html#a1c037ff89c0da8b5cd52088b914b9c1d">Offset</a>(row_shape, 0, batch, out_y, out_x);</div>
<div class="line"><span class="lineno">  170</span>        <span class="keyword">const</span> <span class="keywordtype">int</span> in_x_origin = (out_x * stride_width) - pad_width;</div>
<div class="line"><span class="lineno">  171</span>        <span class="keyword">const</span> <span class="keywordtype">int</span> in_y_origin = (out_y * stride_height) - pad_height;</div>
<div class="line"><span class="lineno">  172</span>        <span class="comment">// Loop through all the pixels of the filter (Kh x Kw)</span></div>
<div class="line"><span class="lineno">  173</span>        <span class="keywordflow">for</span> (<span class="keywordtype">int</span> filter_y = 0; filter_y &lt; filter_height; ++filter_y)</div>
<div class="line"><span class="lineno">  174</span>        {</div>
<div class="line"><span class="lineno">  175</span>          <span class="keyword">const</span> <span class="keywordtype">int</span> in_y = in_y_origin + dilation_height_factor * filter_y;</div>
<div class="line"><span class="lineno">  176</span>          <span class="keywordflow">if</span> ((in_y &gt;= 0) &amp;&amp; (in_y &lt; input_height))</div>
<div class="line"><span class="lineno">  177</span>          {</div>
<div class="line"><span class="lineno">  178</span>            <span class="comment">// Filter row is within the input data.</span></div>
<div class="line"><span class="lineno">  179</span>            <span class="comment">// Loop through all the filter pixels in this row.</span></div>
<div class="line"><span class="lineno">  180</span>            <span class="keywordflow">for</span> (<span class="keywordtype">int</span> filter_x = 0; filter_x &lt; filter_width; ++filter_x)</div>
<div class="line"><span class="lineno">  181</span>            {</div>
<div class="line"><span class="lineno">  182</span>              <span class="keyword">const</span> <span class="keywordtype">int</span> in_x = in_x_origin + dilation_width_factor * filter_x;</div>
<div class="line"><span class="lineno">  183</span>              <span class="keywordtype">int</span> col_offset = <a class="code hl_function" href="ann-ref_2src_2ops_2internal_2dims_8h.html#a1c037ff89c0da8b5cd52088b914b9c1d">Offset</a>(col_shape, 0, filter_y, filter_x, 0);</div>
<div class="line"><span class="lineno">  184</span>              T *dst = im2col_data + <a class="code hl_function" href="ann-ref_2src_2ops_2internal_2dims_8h.html#a1c037ff89c0da8b5cd52088b914b9c1d">Offset</a>(im2col_shape, 0, 0, row_offset, col_offset);</div>
<div class="line"><span class="lineno">  185</span>              <span class="keywordflow">if</span> ((in_x &gt;= 0) &amp;&amp; (in_x &lt; input_width))</div>
<div class="line"><span class="lineno">  186</span>              {</div>
<div class="line"><span class="lineno">  187</span>                <span class="comment">// Filter pixel is within the input, copy the input data.</span></div>
<div class="line"><span class="lineno">  188</span>                T <span class="keyword">const</span> *src = input_data + <a class="code hl_function" href="ann-ref_2src_2ops_2internal_2dims_8h.html#a1c037ff89c0da8b5cd52088b914b9c1d">Offset</a>(input_shape, batch, in_y, in_x, 0);</div>
<div class="line"><span class="lineno">  189</span>                memcpy(dst, src, input_depth * <span class="keyword">sizeof</span>(T));</div>
<div class="line"><span class="lineno">  190</span>              }</div>
<div class="line"><span class="lineno">  191</span>              <span class="keywordflow">else</span></div>
<div class="line"><span class="lineno">  192</span>              {</div>
<div class="line"><span class="lineno">  193</span>                <span class="comment">// Filter pixel is outside the input, zero it out.</span></div>
<div class="line"><span class="lineno">  194</span>                memset(dst, zero_byte, input_depth * <span class="keyword">sizeof</span>(T));</div>
<div class="line"><span class="lineno">  195</span>              }</div>
<div class="line"><span class="lineno">  196</span>            }</div>
<div class="line"><span class="lineno">  197</span>          }</div>
<div class="line"><span class="lineno">  198</span>          <span class="keywordflow">else</span></div>
<div class="line"><span class="lineno">  199</span>          {</div>
<div class="line"><span class="lineno">  200</span>            <span class="comment">// Filter row is outside the input, zero out the entire filter row.</span></div>
<div class="line"><span class="lineno">  201</span>            <span class="keywordtype">int</span> col_offset = <a class="code hl_function" href="ann-ref_2src_2ops_2internal_2dims_8h.html#a1c037ff89c0da8b5cd52088b914b9c1d">Offset</a>(col_shape, 0, filter_y, 0, 0);</div>
<div class="line"><span class="lineno">  202</span>            T *dst = im2col_data + <a class="code hl_function" href="ann-ref_2src_2ops_2internal_2dims_8h.html#a1c037ff89c0da8b5cd52088b914b9c1d">Offset</a>(im2col_shape, 0, 0, row_offset, col_offset);</div>
<div class="line"><span class="lineno">  203</span>            memset(dst, zero_byte, filter_width * input_depth * <span class="keyword">sizeof</span>(T));</div>
<div class="line"><span class="lineno">  204</span>          }</div>
<div class="line"><span class="lineno">  205</span>        }</div>
<div class="line"><span class="lineno">  206</span>      }</div>
<div class="line"><span class="lineno">  207</span>    }</div>
<div class="line"><span class="lineno">  208</span>  }</div>
<div class="line"><span class="lineno">  209</span>}</div>
<div class="ttc" id="aann-ref_2src_2ops_2internal_2dims_8h_html_a1c037ff89c0da8b5cd52088b914b9c1d"><div class="ttname"><a href="ann-ref_2src_2ops_2internal_2dims_8h.html#a1c037ff89c0da8b5cd52088b914b9c1d">Offset</a></div><div class="ttdeci">int Offset(const Dims&lt; 4 &gt; &amp;dims, int i0, int i1, int i2, int i3)</div><div class="ttdef"><b>Definition:</b> <a href="ann-ref_2src_2ops_2internal_2dims_8h_source.html#l00064">Dims.h:64</a></div></div>
<div class="ttc" id="astruct_shape_html"><div class="ttname"><a href="struct_shape.html">Shape</a></div><div class="ttdef"><b>Definition:</b> <a href="compiler_2ann-ref_2src_2_shape_8h_source.html#l00027">Shape.h:28</a></div></div>
<div class="ttc" id="astructnnfw_1_1cker_1_1_conv_params_html_a0a3450e62d20d27c2638d197f1fd2bb2"><div class="ttname"><a href="structnnfw_1_1cker_1_1_conv_params.html#a0a3450e62d20d27c2638d197f1fd2bb2">nnfw::cker::ConvParams::padding_values</a></div><div class="ttdeci">PaddingValues padding_values</div><div class="ttdef"><b>Definition:</b> <a href="compute_2cker_2include_2cker_2_types_8h_source.html#l00137">Types.h:137</a></div></div>
</div><!-- fragment -->
<p class="reference">References <a class="el" href="compute_2cker_2include_2cker_2_types_8h_source.html#l00142">nnfw::cker::ConvParams::dilation_height_factor</a>, <a class="el" href="compute_2cker_2include_2cker_2_types_8h_source.html#l00141">nnfw::cker::ConvParams::dilation_width_factor</a>, <a class="el" href="compute_2cker_2include_2cker_2_shape_8h_source.html#l00093">nnfw::cker::Shape::DimensionsCount()</a>, <a class="el" href="compute_2cker_2include_2cker_2_shape_8h_source.html#l00094">nnfw::cker::Shape::Dims()</a>, <a class="el" href="compute_2cker_2include_2cker_2_types_8h_source.html#l00069">nnfw::cker::PaddingValues::height</a>, <a class="el" href="compute_2cker_2include_2cker_2_shape_8h_source.html#l00222">nnfw::cker::MatchingDim()</a>, <a class="el" href="compute_2cker_2include_2cker_2_shape_8h_source.html#l00241">nnfw::cker::Offset()</a>, <a class="el" href="compute_2cker_2include_2cker_2_types_8h_source.html#l00137">nnfw::cker::ConvParams::padding_values</a>, <a class="el" href="compute_2cker_2include_2cker_2_types_8h_source.html#l00140">nnfw::cker::ConvParams::stride_height</a>, <a class="el" href="compute_2cker_2include_2cker_2_types_8h_source.html#l00139">nnfw::cker::ConvParams::stride_width</a>, and <a class="el" href="compute_2cker_2include_2cker_2_types_8h_source.html#l00068">nnfw::cker::PaddingValues::width</a>.</p>

<p class="reference">Referenced by <a class="el" href="compute_2cker_2include_2cker_2operation_2optimized_2_conv_8h_source.html#l00083">Conv()</a>.</p>

</div>
</div>
<a id="ac7c3ecad381acad6f04dbc55ed5f750e" name="ac7c3ecad381acad6f04dbc55ed5f750e"></a>
<h2 class="memtitle"><span class="permalink"><a href="#ac7c3ecad381acad6f04dbc55ed5f750e">&#9670;&#160;</a></span>DilatedIm2col() <span class="overload">[2/2]</span></h2>

<div class="memitem">
<div class="memproto">
<div class="memtemplate">
template&lt;typename T &gt; </div>
      <table class="memname">
        <tr>
          <td class="memname">void nnfw::cker::optimized::DilatedIm2col </td>
          <td>(</td>
          <td class="paramtype">const <a class="el" href="structnnfw_1_1cker_1_1_conv_params.html">ConvParams</a> &amp;&#160;</td>
          <td class="paramname"><em>params</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">uint8_t&#160;</td>
          <td class="paramname"><em>zero_byte</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const <a class="el" href="classnnfw_1_1cker_1_1_shape.html">Shape</a> &amp;&#160;</td>
          <td class="paramname"><em>input_shape</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const T *&#160;</td>
          <td class="paramname"><em>input_data</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const <a class="el" href="classnnfw_1_1cker_1_1_shape.html">Shape</a> &amp;&#160;</td>
          <td class="paramname"><em>filter_shape</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const <a class="el" href="classnnfw_1_1cker_1_1_shape.html">Shape</a> &amp;&#160;</td>
          <td class="paramname"><em>output_shape</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">T *&#160;</td>
          <td class="paramname"><em>im2col_data</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">

<p class="definition">Definition at line <a class="el" href="_optimized_utils_8h_source.html#l00212">212</a> of file <a class="el" href="_optimized_utils_8h_source.html">OptimizedUtils.h</a>.</p>
<div class="fragment"><div class="line"><span class="lineno">  215</span>{</div>
<div class="line"><span class="lineno">  216</span>  <span class="keyword">const</span> int32_t zero_point = <span class="keyword">static_cast&lt;</span>int32_t<span class="keyword">&gt;</span>(zero_byte);</div>
<div class="line"><span class="lineno">  217</span>  DilatedIm2col&lt;T&gt;(params, input_shape, input_data, filter_shape, output_shape, im2col_data,</div>
<div class="line"><span class="lineno">  218</span>                   &amp;zero_point, 1);</div>
<div class="line"><span class="lineno">  219</span>}</div>
</div><!-- fragment -->
</div>
</div>
<a id="ad0750df2998efe611386c45ffd03b581" name="ad0750df2998efe611386c45ffd03b581"></a>
<h2 class="memtitle"><span class="permalink"><a href="#ad0750df2998efe611386c45ffd03b581">&#9670;&#160;</a></span>Div()</h2>

<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">void nnfw::cker::optimized::Div </td>
          <td>(</td>
          <td class="paramtype">const <a class="el" href="structnnfw_1_1cker_1_1_binary_arithmetic_op_param.html">BinaryArithmeticOpParam</a> &amp;&#160;</td>
          <td class="paramname"><em>params</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const <a class="el" href="classnnfw_1_1cker_1_1_shape.html">Shape</a> &amp;&#160;</td>
          <td class="paramname"><em>input1_shape</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const float *&#160;</td>
          <td class="paramname"><em>input1_data</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const <a class="el" href="classnnfw_1_1cker_1_1_shape.html">Shape</a> &amp;&#160;</td>
          <td class="paramname"><em>input2_shape</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const float *&#160;</td>
          <td class="paramname"><em>input2_data</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const <a class="el" href="classnnfw_1_1cker_1_1_shape.html">Shape</a> &amp;&#160;</td>
          <td class="paramname"><em>output_shape</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">float *&#160;</td>
          <td class="paramname"><em>output_data</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">inline</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p class="definition">Definition at line <a class="el" href="optimized_2_binary_arithmetic_ops_8h_source.html#l01218">1218</a> of file <a class="el" href="optimized_2_binary_arithmetic_ops_8h_source.html">BinaryArithmeticOps.h</a>.</p>
<div class="fragment"><div class="line"><span class="lineno"> 1221</span>{</div>
<div class="line"><span class="lineno"> 1222</span><span class="preprocessor">#ifdef __aarch64__</span></div>
<div class="line"><span class="lineno"> 1223</span>  <span class="keyword">const</span> <span class="keywordtype">int</span> flat_size = <a class="code hl_function" href="namespacennfw_1_1cker.html#ab86126de4e835f9499051e43b841abca">MatchingElementsSize</a>(input1_shape, input2_shape, output_shape);</div>
<div class="line"><span class="lineno"> 1224</span>  <span class="keyword">auto</span> implFuncs = getBinaryOpWithActivationImplFloat&lt;BinaryOpFuncDivFloat&gt;(params);</div>
<div class="line"><span class="lineno"> 1225</span>  (*implFuncs.first)(flat_size, params, input1_data, input2_data, output_data);</div>
<div class="line"><span class="lineno"> 1226</span><span class="preprocessor">#else</span></div>
<div class="line"><span class="lineno"> 1227</span>  <span class="keyword">const</span> std::function&lt;float(<span class="keyword">const</span> <span class="keywordtype">float</span> &amp;, <span class="keyword">const</span> <span class="keywordtype">float</span> &amp;)&gt; fn =</div>
<div class="line"><span class="lineno"> 1228</span>    [](<span class="keyword">const</span> <span class="keywordtype">float</span> &amp;a, <span class="keyword">const</span> <span class="keywordtype">float</span> &amp;b) -&gt; <span class="keywordtype">float</span> { <span class="keywordflow">return</span> a / b; };</div>
<div class="line"><span class="lineno"> 1229</span>  reference::BinaryArithmeticOp(params, input1_shape, input1_data, input2_shape, input2_data,</div>
<div class="line"><span class="lineno"> 1230</span>                                output_shape, output_data, fn);</div>
<div class="line"><span class="lineno"> 1231</span><span class="preprocessor">#endif </span><span class="comment">// __aarch64__</span></div>
<div class="line"><span class="lineno"> 1232</span>}</div>
</div><!-- fragment -->
<p class="reference">References <a class="el" href="reference_2_binary_arithmetic_ops_8h_source.html#l00035">nnfw::cker::reference::BinaryArithmeticOp()</a>, and <a class="el" href="compute_2cker_2include_2cker_2_shape_8h_source.html#l00337">nnfw::cker::MatchingElementsSize()</a>.</p>

<p class="reference">Referenced by <a class="el" href="_binary_arithmetic_ops_8h_source.html#l00228">nnfw::cker::BinaryArithmeticOp()</a>.</p>

</div>
</div>
<a id="a3faac8ad96bfaec0b409322c20a42230" name="a3faac8ad96bfaec0b409322c20a42230"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a3faac8ad96bfaec0b409322c20a42230">&#9670;&#160;</a></span>ExtractPatchIntoBufferColumn()</h2>

<div class="memitem">
<div class="memproto">
<div class="memtemplate">
template&lt;typename T &gt; </div>
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">void nnfw::cker::optimized::ExtractPatchIntoBufferColumn </td>
          <td>(</td>
          <td class="paramtype">const <a class="el" href="classnnfw_1_1cker_1_1_shape.html">Shape</a> &amp;&#160;</td>
          <td class="paramname"><em>input_shape</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">int&#160;</td>
          <td class="paramname"><em>w</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">int&#160;</td>
          <td class="paramname"><em>h</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">int&#160;</td>
          <td class="paramname"><em>b</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">int&#160;</td>
          <td class="paramname"><em>kheight</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">int&#160;</td>
          <td class="paramname"><em>kwidth</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">int&#160;</td>
          <td class="paramname"><em>stride_width</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">int&#160;</td>
          <td class="paramname"><em>stride_height</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">int&#160;</td>
          <td class="paramname"><em>pad_width</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">int&#160;</td>
          <td class="paramname"><em>pad_height</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">int&#160;</td>
          <td class="paramname"><em>in_width</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">int&#160;</td>
          <td class="paramname"><em>in_height</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">int&#160;</td>
          <td class="paramname"><em>in_depth</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">int&#160;</td>
          <td class="paramname"><em>single_buffer_length</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">int&#160;</td>
          <td class="paramname"><em>buffer_id</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const T *&#160;</td>
          <td class="paramname"><em>in_data</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">T *&#160;</td>
          <td class="paramname"><em>conv_buffer_data</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">uint8_t&#160;</td>
          <td class="paramname"><em>zero_byte</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">inline</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p class="definition">Definition at line <a class="el" href="_optimized_utils_8h_source.html#l00034">34</a> of file <a class="el" href="_optimized_utils_8h_source.html">OptimizedUtils.h</a>.</p>
<div class="fragment"><div class="line"><span class="lineno">   39</span>{</div>
<div class="line"><span class="lineno">   40</span>  assert(input_shape.<a class="code hl_function" href="classnnfw_1_1cker_1_1_shape.html#a01c6b5dc4b24d5c2567c47cb15318839">DimensionsCount</a>() == 4);</div>
<div class="line"><span class="lineno">   41</span>  <span class="comment">// This chunk of code reshapes all the inputs corresponding to</span></div>
<div class="line"><span class="lineno">   42</span>  <span class="comment">// output (b, h, w) to a column vector in conv_buffer(:, buffer_id).</span></div>
<div class="line"><span class="lineno">   43</span>  <span class="keyword">const</span> <span class="keywordtype">int</span> kwidth_times_indepth = kwidth * in_depth;</div>
<div class="line"><span class="lineno">   44</span>  <span class="keyword">const</span> <span class="keywordtype">int</span> inwidth_times_indepth = in_width * in_depth;</div>
<div class="line"><span class="lineno">   45</span>  <span class="keyword">const</span> <span class="keywordtype">int</span> ih_ungated_start = h * stride_height - pad_height;</div>
<div class="line"><span class="lineno">   46</span>  <span class="keyword">const</span> <span class="keywordtype">int</span> ih_ungated_end = (ih_ungated_start + kheight);</div>
<div class="line"><span class="lineno">   47</span>  <span class="keyword">const</span> <span class="keywordtype">int</span> ih_end = std::min(ih_ungated_end, in_height);</div>
<div class="line"><span class="lineno">   48</span>  <span class="keyword">const</span> <span class="keywordtype">int</span> iw_ungated_start = w * stride_width - pad_width;</div>
<div class="line"><span class="lineno">   49</span>  <span class="keyword">const</span> <span class="keywordtype">int</span> iw_ungated_end = (iw_ungated_start + kwidth);</div>
<div class="line"><span class="lineno">   50</span>  <span class="keyword">const</span> <span class="keywordtype">int</span> iw_end = std::min(iw_ungated_end, in_width);</div>
<div class="line"><span class="lineno">   51</span>  <span class="comment">// If the patch is off the edge of the input image, skip writing those rows</span></div>
<div class="line"><span class="lineno">   52</span>  <span class="comment">// and columns from the patch into the output array.</span></div>
<div class="line"><span class="lineno">   53</span>  <span class="keyword">const</span> <span class="keywordtype">int</span> h_offset = std::max(0, -ih_ungated_start);</div>
<div class="line"><span class="lineno">   54</span>  <span class="keyword">const</span> <span class="keywordtype">int</span> w_offset = std::max(0, -iw_ungated_start);</div>
<div class="line"><span class="lineno">   55</span>  <span class="keyword">const</span> <span class="keywordtype">int</span> ih_start = std::max(0, ih_ungated_start);</div>
<div class="line"><span class="lineno">   56</span>  <span class="keyword">const</span> <span class="keywordtype">int</span> iw_start = std::max(0, iw_ungated_start);</div>
<div class="line"><span class="lineno">   57</span>  <span class="keyword">const</span> <span class="keywordtype">int</span> single_row_num = std::min(kwidth - w_offset, in_width - iw_start) * in_depth;</div>
<div class="line"><span class="lineno">   58</span>  <span class="keyword">const</span> <span class="keywordtype">int</span> output_row_offset = (buffer_id * single_buffer_length);</div>
<div class="line"><span class="lineno">   59</span>  <span class="keywordtype">int</span> out_offset = output_row_offset + (h_offset * kwidth + w_offset) * in_depth;</div>
<div class="line"><span class="lineno">   60</span>  <span class="keywordtype">int</span> in_offset = <a class="code hl_function" href="ann-ref_2src_2ops_2internal_2dims_8h.html#a1c037ff89c0da8b5cd52088b914b9c1d">Offset</a>(input_shape, b, ih_start, iw_start, 0);</div>
<div class="line"><span class="lineno">   61</span> </div>
<div class="line"><span class="lineno">   62</span>  <span class="comment">// Express all of the calculations as padding around the input patch.</span></div>
<div class="line"><span class="lineno">   63</span>  <span class="keyword">const</span> <span class="keywordtype">int</span> top_padding = h_offset;</div>
<div class="line"><span class="lineno">   64</span>  <span class="keyword">const</span> <span class="keywordtype">int</span> bottom_padding = (ih_ungated_end - ih_end);</div>
<div class="line"><span class="lineno">   65</span>  <span class="keyword">const</span> <span class="keywordtype">int</span> left_padding = w_offset;</div>
<div class="line"><span class="lineno">   66</span>  <span class="keyword">const</span> <span class="keywordtype">int</span> right_padding = (iw_ungated_end - iw_end);</div>
<div class="line"><span class="lineno">   67</span>  assert(single_row_num == ((kwidth - (left_padding + right_padding)) * in_depth));</div>
<div class="line"><span class="lineno">   68</span> </div>
<div class="line"><span class="lineno">   69</span>  <span class="comment">// Write out zeroes to the elements representing the top rows of the input</span></div>
<div class="line"><span class="lineno">   70</span>  <span class="comment">// patch that are off the edge of the input image.</span></div>
<div class="line"><span class="lineno">   71</span>  <span class="keywordflow">if</span> (top_padding &gt; 0)</div>
<div class="line"><span class="lineno">   72</span>  {</div>
<div class="line"><span class="lineno">   73</span>    <span class="keyword">const</span> <span class="keywordtype">int</span> top_row_elements = (top_padding * kwidth * in_depth);</div>
<div class="line"><span class="lineno">   74</span>    memset(conv_buffer_data + output_row_offset, zero_byte, (top_row_elements * <span class="keyword">sizeof</span>(T)));</div>
<div class="line"><span class="lineno">   75</span>  }</div>
<div class="line"><span class="lineno">   76</span> </div>
<div class="line"><span class="lineno">   77</span>  <span class="comment">// If the patch is on the interior of the input image horizontally, just copy</span></div>
<div class="line"><span class="lineno">   78</span>  <span class="comment">// over the rows sequentially, otherwise add zero padding at the start or end.</span></div>
<div class="line"><span class="lineno">   79</span>  <span class="keywordflow">if</span> ((left_padding == 0) &amp;&amp; (right_padding == 0))</div>
<div class="line"><span class="lineno">   80</span>  {</div>
<div class="line"><span class="lineno">   81</span>    <span class="keywordflow">for</span> (<span class="keywordtype">int</span> ih = ih_start; ih &lt; ih_end; ++ih)</div>
<div class="line"><span class="lineno">   82</span>    {</div>
<div class="line"><span class="lineno">   83</span>      memcpy(conv_buffer_data + out_offset, in_data + in_offset, single_row_num * <span class="keyword">sizeof</span>(T));</div>
<div class="line"><span class="lineno">   84</span>      out_offset += kwidth_times_indepth;</div>
<div class="line"><span class="lineno">   85</span>      in_offset += inwidth_times_indepth;</div>
<div class="line"><span class="lineno">   86</span>    }</div>
<div class="line"><span class="lineno">   87</span>  }</div>
<div class="line"><span class="lineno">   88</span>  <span class="keywordflow">else</span></div>
<div class="line"><span class="lineno">   89</span>  {</div>
<div class="line"><span class="lineno">   90</span>    <span class="keywordflow">for</span> (<span class="keywordtype">int</span> ih = ih_start; ih &lt; ih_end; ++ih)</div>
<div class="line"><span class="lineno">   91</span>    {</div>
<div class="line"><span class="lineno">   92</span>      <span class="keywordflow">if</span> (left_padding &gt; 0)</div>
<div class="line"><span class="lineno">   93</span>      {</div>
<div class="line"><span class="lineno">   94</span>        <span class="keyword">const</span> <span class="keywordtype">int</span> left_start = (out_offset - (left_padding * in_depth));</div>
<div class="line"><span class="lineno">   95</span>        memset(conv_buffer_data + left_start, zero_byte, (left_padding * in_depth * <span class="keyword">sizeof</span>(T)));</div>
<div class="line"><span class="lineno">   96</span>      }</div>
<div class="line"><span class="lineno">   97</span>      memcpy(conv_buffer_data + out_offset, in_data + in_offset, single_row_num * <span class="keyword">sizeof</span>(T));</div>
<div class="line"><span class="lineno">   98</span>      <span class="keywordflow">if</span> (right_padding &gt; 0)</div>
<div class="line"><span class="lineno">   99</span>      {</div>
<div class="line"><span class="lineno">  100</span>        <span class="keyword">const</span> <span class="keywordtype">int</span> right_start = (out_offset + single_row_num);</div>
<div class="line"><span class="lineno">  101</span>        memset(conv_buffer_data + right_start, zero_byte, (right_padding * in_depth * <span class="keyword">sizeof</span>(T)));</div>
<div class="line"><span class="lineno">  102</span>      }</div>
<div class="line"><span class="lineno">  103</span>      out_offset += kwidth_times_indepth;</div>
<div class="line"><span class="lineno">  104</span>      in_offset += inwidth_times_indepth;</div>
<div class="line"><span class="lineno">  105</span>    }</div>
<div class="line"><span class="lineno">  106</span>  }</div>
<div class="line"><span class="lineno">  107</span> </div>
<div class="line"><span class="lineno">  108</span>  <span class="comment">// If the bottom of the patch falls off the input image, pad the values</span></div>
<div class="line"><span class="lineno">  109</span>  <span class="comment">// representing those input rows with zeroes.</span></div>
<div class="line"><span class="lineno">  110</span>  <span class="keywordflow">if</span> (bottom_padding &gt; 0)</div>
<div class="line"><span class="lineno">  111</span>  {</div>
<div class="line"><span class="lineno">  112</span>    <span class="keyword">const</span> <span class="keywordtype">int</span> bottom_row_elements = (bottom_padding * kwidth * in_depth);</div>
<div class="line"><span class="lineno">  113</span>    <span class="keyword">const</span> <span class="keywordtype">int</span> bottom_start =</div>
<div class="line"><span class="lineno">  114</span>      output_row_offset + ((top_padding + (ih_end - ih_start)) * kwidth * in_depth);</div>
<div class="line"><span class="lineno">  115</span>    memset(conv_buffer_data + bottom_start, zero_byte, (bottom_row_elements * <span class="keyword">sizeof</span>(T)));</div>
<div class="line"><span class="lineno">  116</span>  }</div>
<div class="line"><span class="lineno">  117</span>}</div>
</div><!-- fragment -->
<p class="reference">References <a class="el" href="compute_2cker_2include_2cker_2_shape_8h_source.html#l00093">nnfw::cker::Shape::DimensionsCount()</a>, and <a class="el" href="compute_2cker_2include_2cker_2_shape_8h_source.html#l00241">nnfw::cker::Offset()</a>.</p>

<p class="reference">Referenced by <a class="el" href="_optimized_utils_8h_source.html#l00222">Im2col()</a>.</p>

</div>
</div>
<a id="ac8ea6e7a1b57d1cd9f70bc0035be203c" name="ac8ea6e7a1b57d1cd9f70bc0035be203c"></a>
<h2 class="memtitle"><span class="permalink"><a href="#ac8ea6e7a1b57d1cd9f70bc0035be203c">&#9670;&#160;</a></span>FloatDepthwiseConvAccumRow()</h2>

<div class="memitem">
<div class="memproto">
<div class="memtemplate">
template&lt;bool kAllowStrided, int kFixedInputDepth, int kFixedDepthMultiplier&gt; </div>
      <table class="memname">
        <tr>
          <td class="memname">void nnfw::cker::optimized::FloatDepthwiseConvAccumRow </td>
          <td>(</td>
          <td class="paramtype">int&#160;</td>
          <td class="paramname"><em>stride</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">int&#160;</td>
          <td class="paramname"><em>dilation_factor</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">int&#160;</td>
          <td class="paramname"><em>input_depth</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">int&#160;</td>
          <td class="paramname"><em>input_width</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const float *&#160;</td>
          <td class="paramname"><em>input_data</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">int&#160;</td>
          <td class="paramname"><em>pad_width</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">int&#160;</td>
          <td class="paramname"><em>depth_multiplier</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">int&#160;</td>
          <td class="paramname"><em>filter_width</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const float *&#160;</td>
          <td class="paramname"><em>filter_data</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">int&#160;</td>
          <td class="paramname"><em>out_x_buffer_start</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">int&#160;</td>
          <td class="paramname"><em>out_x_buffer_end</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">int&#160;</td>
          <td class="paramname"><em>output_depth</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">float *&#160;</td>
          <td class="paramname"><em>acc_buffer</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">

<p class="definition">Definition at line <a class="el" href="_depthwise_conv_float_8h_source.html#l00908">908</a> of file <a class="el" href="_depthwise_conv_float_8h_source.html">DepthwiseConvFloat.h</a>.</p>
<div class="fragment"><div class="line"><span class="lineno">  912</span>{</div>
<div class="line"><span class="lineno">  913</span>  <span class="comment">// Sanity check parameters. This is important in particular to ensure</span></div>
<div class="line"><span class="lineno">  914</span>  <span class="comment">// that we keep the number of template instantiations minimal, so we don&#39;t</span></div>
<div class="line"><span class="lineno">  915</span>  <span class="comment">// increase binary size unnecessarily.</span></div>
<div class="line"><span class="lineno">  916</span>  <span class="keyword">static_assert</span>(kFixedDepthMultiplier || !kFixedInputDepth, <span class="stringliteral">&quot;&quot;</span>);</div>
<div class="line"><span class="lineno">  917</span>  <span class="keyword">static_assert</span>(kFixedInputDepth || kAllowStrided, <span class="stringliteral">&quot;&quot;</span>);</div>
<div class="line"><span class="lineno">  918</span>  assert(stride == 1 || kAllowStrided);</div>
<div class="line"><span class="lineno">  919</span>  <span class="keywordflow">if</span> (kFixedInputDepth)</div>
<div class="line"><span class="lineno">  920</span>  {</div>
<div class="line"><span class="lineno">  921</span>    assert(input_depth == kFixedInputDepth);</div>
<div class="line"><span class="lineno">  922</span>  }</div>
<div class="line"><span class="lineno">  923</span>  <span class="keywordflow">if</span> (kFixedDepthMultiplier)</div>
<div class="line"><span class="lineno">  924</span>  {</div>
<div class="line"><span class="lineno">  925</span>    assert(depth_multiplier == kFixedDepthMultiplier);</div>
<div class="line"><span class="lineno">  926</span>  }</div>
<div class="line"><span class="lineno">  927</span>  assert(output_depth == input_depth * depth_multiplier);</div>
<div class="line"><span class="lineno">  928</span>  <span class="keyword">const</span> <span class="keywordtype">int</span> input_ptr_increment = stride * input_depth;</div>
<div class="line"><span class="lineno">  929</span>  <span class="keyword">const</span> <span class="keywordtype">float</span> *filter_base_ptr = filter_data;</div>
<div class="line"><span class="lineno">  930</span>  <span class="keywordflow">for</span> (<span class="keywordtype">int</span> filter_x = 0; filter_x &lt; filter_width; ++filter_x)</div>
<div class="line"><span class="lineno">  931</span>  {</div>
<div class="line"><span class="lineno">  932</span>    <span class="comment">// For the current (filter_x, filter_y) point in the filter,</span></div>
<div class="line"><span class="lineno">  933</span>    <span class="comment">// compute the boundaries of the corresponding output row segment.</span></div>
<div class="line"><span class="lineno">  934</span>    <span class="keywordtype">int</span> out_x_loop_start_unclamped = 0;</div>
<div class="line"><span class="lineno">  935</span>    <span class="keywordtype">int</span> out_x_loop_end_unclamped = 0;</div>
<div class="line"><span class="lineno">  936</span>    <span class="keywordflow">if</span> (kAllowStrided)</div>
<div class="line"><span class="lineno">  937</span>    {</div>
<div class="line"><span class="lineno">  938</span>      <span class="keywordflow">if</span> (stride == 2)</div>
<div class="line"><span class="lineno">  939</span>      {</div>
<div class="line"><span class="lineno">  940</span>        out_x_loop_start_unclamped = (pad_width - dilation_factor * filter_x + 1) / 2;</div>
<div class="line"><span class="lineno">  941</span>        out_x_loop_end_unclamped = (pad_width + input_width - dilation_factor * filter_x + 1) / 2;</div>
<div class="line"><span class="lineno">  942</span>      }</div>
<div class="line"><span class="lineno">  943</span>      <span class="keywordflow">else</span> <span class="keywordflow">if</span> (stride == 4)</div>
<div class="line"><span class="lineno">  944</span>      {</div>
<div class="line"><span class="lineno">  945</span>        out_x_loop_start_unclamped = (pad_width - dilation_factor * filter_x + 3) / 4;</div>
<div class="line"><span class="lineno">  946</span>        out_x_loop_end_unclamped = (pad_width + input_width - dilation_factor * filter_x + 3) / 4;</div>
<div class="line"><span class="lineno">  947</span>      }</div>
<div class="line"><span class="lineno">  948</span>      <span class="keywordflow">else</span></div>
<div class="line"><span class="lineno">  949</span>      {</div>
<div class="line"><span class="lineno">  950</span>        out_x_loop_start_unclamped = (pad_width - dilation_factor * filter_x + stride - 1) / stride;</div>
<div class="line"><span class="lineno">  951</span>        out_x_loop_end_unclamped =</div>
<div class="line"><span class="lineno">  952</span>          (pad_width + input_width - dilation_factor * filter_x + stride - 1) / stride;</div>
<div class="line"><span class="lineno">  953</span>      }</div>
<div class="line"><span class="lineno">  954</span>    }</div>
<div class="line"><span class="lineno">  955</span>    <span class="keywordflow">else</span></div>
<div class="line"><span class="lineno">  956</span>    {</div>
<div class="line"><span class="lineno">  957</span>      out_x_loop_start_unclamped = pad_width - dilation_factor * filter_x;</div>
<div class="line"><span class="lineno">  958</span>      out_x_loop_end_unclamped = pad_width + input_width - dilation_factor * filter_x;</div>
<div class="line"><span class="lineno">  959</span>    }</div>
<div class="line"><span class="lineno">  960</span>    <span class="comment">// The kernel will have to iterate on the segment of the</span></div>
<div class="line"><span class="lineno">  961</span>    <span class="comment">// output row that starts at out_x_loop_start and out_x_loop_end.</span></div>
<div class="line"><span class="lineno">  962</span>    <span class="keyword">const</span> <span class="keywordtype">int</span> out_x_loop_start = std::max(out_x_buffer_start, out_x_loop_start_unclamped);</div>
<div class="line"><span class="lineno">  963</span>    <span class="keyword">const</span> <span class="keywordtype">int</span> out_x_loop_end = std::min(out_x_buffer_end, out_x_loop_end_unclamped);</div>
<div class="line"><span class="lineno">  964</span> </div>
<div class="line"><span class="lineno">  965</span>    <span class="keywordtype">float</span> *acc_buffer_ptr = acc_buffer + (out_x_loop_start - out_x_buffer_start) * output_depth;</div>
<div class="line"><span class="lineno">  966</span>    <span class="keyword">const</span> <span class="keywordtype">int</span> in_x_origin = (out_x_loop_start * stride) - pad_width + dilation_factor * filter_x;</div>
<div class="line"><span class="lineno">  967</span>    <span class="keyword">const</span> <span class="keywordtype">float</span> *input_ptr = <a class="code hl_variable" href="namespacepart__eval__one.html#a818987310f1a181c06a880dab57fcf39">input_data</a> + in_x_origin * input_depth;</div>
<div class="line"><span class="lineno">  968</span>    <span class="keyword">const</span> <span class="keywordtype">int</span> num_output_pixels = out_x_loop_end - out_x_loop_start;</div>
<div class="line"><span class="lineno">  969</span>    FloatDepthwiseConvKernel&lt;kAllowStrided, kFixedInputDepth, kFixedDepthMultiplier&gt;::Run(</div>
<div class="line"><span class="lineno">  970</span>      num_output_pixels, input_depth, depth_multiplier, input_ptr, input_ptr_increment,</div>
<div class="line"><span class="lineno">  971</span>      filter_base_ptr, acc_buffer_ptr);</div>
<div class="line"><span class="lineno">  972</span>    filter_base_ptr += output_depth;</div>
<div class="line"><span class="lineno">  973</span>  }</div>
<div class="line"><span class="lineno">  974</span>}</div>
<div class="ttc" id="anamespacepart__eval__one_html_a818987310f1a181c06a880dab57fcf39"><div class="ttname"><a href="namespacepart__eval__one.html#a818987310f1a181c06a880dab57fcf39">part_eval_one.input_data</a></div><div class="ttdeci">input_data</div><div class="ttdef"><b>Definition:</b> <a href="part__eval__one_8py_source.html#l00075">part_eval_one.py:75</a></div></div>
</div><!-- fragment -->
</div>
</div>
<a id="ad4525396811538b4ed0ec83ded38aadf" name="ad4525396811538b4ed0ec83ded38aadf"></a>
<h2 class="memtitle"><span class="permalink"><a href="#ad4525396811538b4ed0ec83ded38aadf">&#9670;&#160;</a></span>FloatDepthwiseConvAccumRowGeneric()</h2>

<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">void nnfw::cker::optimized::FloatDepthwiseConvAccumRowGeneric </td>
          <td>(</td>
          <td class="paramtype">int&#160;</td>
          <td class="paramname"><em>stride</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">int&#160;</td>
          <td class="paramname"><em>dilation_factor</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">int&#160;</td>
          <td class="paramname"><em>input_depth</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">int&#160;</td>
          <td class="paramname"><em>input_width</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const float *&#160;</td>
          <td class="paramname"><em>input_data</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">int&#160;</td>
          <td class="paramname"><em>pad_width</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">int&#160;</td>
          <td class="paramname"><em>depth_multiplier</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">int&#160;</td>
          <td class="paramname"><em>filter_width</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const float *&#160;</td>
          <td class="paramname"><em>filter_data</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">int&#160;</td>
          <td class="paramname"><em>out_x_buffer_start</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">int&#160;</td>
          <td class="paramname"><em>out_x_buffer_end</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">int&#160;</td>
          <td class="paramname"><em>output_depth</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">float *&#160;</td>
          <td class="paramname"><em>acc_buffer</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">inline</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p class="definition">Definition at line <a class="el" href="_depthwise_conv_float_8h_source.html#l00977">977</a> of file <a class="el" href="_depthwise_conv_float_8h_source.html">DepthwiseConvFloat.h</a>.</p>
<div class="fragment"><div class="line"><span class="lineno">  983</span>{</div>
<div class="line"><span class="lineno">  984</span>  <span class="keyword">const</span> <span class="keywordtype">float</span> *filter_base_ptr = filter_data;</div>
<div class="line"><span class="lineno">  985</span>  <span class="keywordflow">for</span> (<span class="keywordtype">int</span> filter_x = 0; filter_x &lt; filter_width; ++filter_x)</div>
<div class="line"><span class="lineno">  986</span>  {</div>
<div class="line"><span class="lineno">  987</span>    <span class="keyword">const</span> <span class="keywordtype">int</span> out_x_loop_start =</div>
<div class="line"><span class="lineno">  988</span>      std::max(out_x_buffer_start, (pad_width - dilation_factor * filter_x + stride - 1) / stride);</div>
<div class="line"><span class="lineno">  989</span>    <span class="keyword">const</span> <span class="keywordtype">int</span> out_x_loop_end =</div>
<div class="line"><span class="lineno">  990</span>      std::min(out_x_buffer_end,</div>
<div class="line"><span class="lineno">  991</span>               (pad_width + input_width - dilation_factor * filter_x + stride - 1) / stride);</div>
<div class="line"><span class="lineno">  992</span> </div>
<div class="line"><span class="lineno">  993</span>    <span class="keywordtype">float</span> *acc_buffer_ptr = acc_buffer + (out_x_loop_start - out_x_buffer_start) * output_depth;</div>
<div class="line"><span class="lineno">  994</span>    <span class="keyword">const</span> <span class="keywordtype">int</span> in_x_origin = (out_x_loop_start * stride) - pad_width + dilation_factor * filter_x;</div>
<div class="line"><span class="lineno">  995</span>    <span class="keyword">const</span> <span class="keywordtype">float</span> *input_ptr = input_data + in_x_origin * input_depth;</div>
<div class="line"><span class="lineno">  996</span>    <span class="keyword">const</span> <span class="keywordtype">int</span> input_ptr_increment = (stride - 1) * input_depth;</div>
<div class="line"><span class="lineno">  997</span>    <span class="keywordflow">for</span> (<span class="keywordtype">int</span> out_x = out_x_loop_start; out_x &lt; out_x_loop_end; out_x++)</div>
<div class="line"><span class="lineno">  998</span>    {</div>
<div class="line"><span class="lineno">  999</span>      <span class="keyword">const</span> <span class="keywordtype">float</span> *filter_ptr = filter_base_ptr;</div>
<div class="line"><span class="lineno"> 1000</span>      <span class="keywordflow">for</span> (<span class="keywordtype">int</span> ic = 0; ic &lt; input_depth; ++ic)</div>
<div class="line"><span class="lineno"> 1001</span>      {</div>
<div class="line"><span class="lineno"> 1002</span>        <span class="keyword">const</span> <span class="keywordtype">float</span> input_val = *input_ptr++;</div>
<div class="line"><span class="lineno"> 1003</span>        <span class="keywordflow">for</span> (<span class="keywordtype">int</span> <a class="code hl_variable" href="_n_e_g_e_m_m_matrix_accumulate_biases_kernel_8cpp.html#aa73e018b04368e7afaded77ef0ac69db">m</a> = 0; <a class="code hl_variable" href="_n_e_g_e_m_m_matrix_accumulate_biases_kernel_8cpp.html#aa73e018b04368e7afaded77ef0ac69db">m</a> &lt; depth_multiplier; <a class="code hl_variable" href="_n_e_g_e_m_m_matrix_accumulate_biases_kernel_8cpp.html#aa73e018b04368e7afaded77ef0ac69db">m</a>++)</div>
<div class="line"><span class="lineno"> 1004</span>        {</div>
<div class="line"><span class="lineno"> 1005</span>          <span class="keyword">const</span> <span class="keywordtype">float</span> filter_val = *filter_ptr++;</div>
<div class="line"><span class="lineno"> 1006</span>          *acc_buffer_ptr++ += filter_val * input_val;</div>
<div class="line"><span class="lineno"> 1007</span>        }</div>
<div class="line"><span class="lineno"> 1008</span>      }</div>
<div class="line"><span class="lineno"> 1009</span>      input_ptr += input_ptr_increment;</div>
<div class="line"><span class="lineno"> 1010</span>    }</div>
<div class="line"><span class="lineno"> 1011</span>    filter_base_ptr += output_depth;</div>
<div class="line"><span class="lineno"> 1012</span>  }</div>
<div class="line"><span class="lineno"> 1013</span>}</div>
<div class="ttc" id="a_n_e_g_e_m_m_matrix_accumulate_biases_kernel_8cpp_html_aa73e018b04368e7afaded77ef0ac69db"><div class="ttname"><a href="_n_e_g_e_m_m_matrix_accumulate_biases_kernel_8cpp.html#aa73e018b04368e7afaded77ef0ac69db">m</a></div><div class="ttdeci">std::mutex m</div><div class="ttdef"><b>Definition:</b> <a href="_n_e_g_e_m_m_matrix_accumulate_biases_kernel_8cpp_source.html#l00135">NEGEMMMatrixAccumulateBiasesKernel.cpp:135</a></div></div>
</div><!-- fragment -->
<p class="reference">References <a class="el" href="_n_e_g_e_m_m_matrix_accumulate_biases_kernel_8cpp_source.html#l00135">m</a>.</p>

<p class="reference">Referenced by <a class="el" href="_depthwise_conv_float_8h_source.html#l01033">DepthwiseConvImpl()</a>.</p>

</div>
</div>
<a id="ac1cccd42931df44b8f4e71ce728429eb" name="ac1cccd42931df44b8f4e71ce728429eb"></a>
<h2 class="memtitle"><span class="permalink"><a href="#ac1cccd42931df44b8f4e71ce728429eb">&#9670;&#160;</a></span>getBinaryOpWithActivationImplFloat()</h2>

<div class="memitem">
<div class="memproto">
<div class="memtemplate">
template&lt;class FUNC &gt; </div>
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname"><a class="el" href="namespacennfw_1_1cker_1_1optimized.html#ad3a3cb812bffb092ce5a4791b841b168">BinaryOpImplFloatFuncs</a> nnfw::cker::optimized::getBinaryOpWithActivationImplFloat </td>
          <td>(</td>
          <td class="paramtype">const <a class="el" href="structnnfw_1_1cker_1_1_binary_arithmetic_op_param.html">BinaryArithmeticOpParam</a> &amp;&#160;</td>
          <td class="paramname"><em>params</em></td><td>)</td>
          <td></td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">inline</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p class="definition">Definition at line <a class="el" href="optimized_2_binary_arithmetic_ops_8h_source.html#l00676">676</a> of file <a class="el" href="optimized_2_binary_arithmetic_ops_8h_source.html">BinaryArithmeticOps.h</a>.</p>
<div class="fragment"><div class="line"><span class="lineno">  677</span>{</div>
<div class="line"><span class="lineno">  678</span>  <span class="keywordflow">if</span> (params.<a class="code hl_variable" href="structnnfw_1_1cker_1_1_binary_arithmetic_op_param.html#ac8fdfaa8e3b43c341a0128857d3bcf20">float_activation_max</a> == std::numeric_limits&lt;float&gt;::max())</div>
<div class="line"><span class="lineno">  679</span>    <span class="keywordflow">if</span> (params.<a class="code hl_variable" href="structnnfw_1_1cker_1_1_binary_arithmetic_op_param.html#af87d13f6f63a9d5eeb175c2db94bae92">float_activation_min</a> == std::numeric_limits&lt;float&gt;::lowest())</div>
<div class="line"><span class="lineno">  680</span>      <span class="keywordflow">return</span> <a class="code hl_typedef" href="namespacennfw_1_1cker_1_1optimized.html#ad3a3cb812bffb092ce5a4791b841b168">BinaryOpImplFloatFuncs</a>(BinaryOpElementwise&lt;FUNC, BinaryOpActivationFloatNone&gt;,</div>
<div class="line"><span class="lineno">  681</span>                                    BinaryOpScalarBroadcast&lt;FUNC, BinaryOpActivationFloatNone&gt;);</div>
<div class="line"><span class="lineno">  682</span>    <span class="keywordflow">else</span></div>
<div class="line"><span class="lineno">  683</span>      <span class="keywordflow">return</span> <a class="code hl_typedef" href="namespacennfw_1_1cker_1_1optimized.html#ad3a3cb812bffb092ce5a4791b841b168">BinaryOpImplFloatFuncs</a>(BinaryOpElementwise&lt;FUNC, BinaryOpActivationFloatMax&gt;,</div>
<div class="line"><span class="lineno">  684</span>                                    BinaryOpScalarBroadcast&lt;FUNC, BinaryOpActivationFloatMax&gt;);</div>
<div class="line"><span class="lineno">  685</span>  <span class="keywordflow">else</span></div>
<div class="line"><span class="lineno">  686</span>    <span class="keywordflow">return</span> <a class="code hl_typedef" href="namespacennfw_1_1cker_1_1optimized.html#ad3a3cb812bffb092ce5a4791b841b168">BinaryOpImplFloatFuncs</a>(BinaryOpElementwise&lt;FUNC, BinaryOpActivationFloatMinMax&gt;,</div>
<div class="line"><span class="lineno">  687</span>                                  BinaryOpScalarBroadcast&lt;FUNC, BinaryOpActivationFloatMinMax&gt;);</div>
<div class="line"><span class="lineno">  688</span>}</div>
<div class="ttc" id="anamespacennfw_1_1cker_1_1optimized_html_ad3a3cb812bffb092ce5a4791b841b168"><div class="ttname"><a href="namespacennfw_1_1cker_1_1optimized.html#ad3a3cb812bffb092ce5a4791b841b168">nnfw::cker::optimized::BinaryOpImplFloatFuncs</a></div><div class="ttdeci">std::pair&lt; void(*)(int, const BinaryArithmeticOpParam &amp;, const float *, const float *, float *), void(*)(int, const BinaryArithmeticOpParam &amp;, const float, const float *, float *)&gt; BinaryOpImplFloatFuncs</div><div class="ttdef"><b>Definition:</b> <a href="optimized_2_binary_arithmetic_ops_8h_source.html#l00670">BinaryArithmeticOps.h:672</a></div></div>
</div><!-- fragment -->
<p class="reference">References <a class="el" href="compute_2cker_2include_2cker_2_types_8h_source.html#l00193">nnfw::cker::BinaryArithmeticOpParam::float_activation_max</a>, and <a class="el" href="compute_2cker_2include_2cker_2_types_8h_source.html#l00192">nnfw::cker::BinaryArithmeticOpParam::float_activation_min</a>.</p>

</div>
</div>
<a id="a94dc0ed7df7b941c9a6f0b67a1155e51" name="a94dc0ed7df7b941c9a6f0b67a1155e51"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a94dc0ed7df7b941c9a6f0b67a1155e51">&#9670;&#160;</a></span>Im2col()</h2>

<div class="memitem">
<div class="memproto">
<div class="memtemplate">
template&lt;typename T &gt; </div>
      <table class="memname">
        <tr>
          <td class="memname">void nnfw::cker::optimized::Im2col </td>
          <td>(</td>
          <td class="paramtype">const <a class="el" href="structnnfw_1_1cker_1_1_conv_params.html">ConvParams</a> &amp;&#160;</td>
          <td class="paramname"><em>params</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">int&#160;</td>
          <td class="paramname"><em>kheight</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">int&#160;</td>
          <td class="paramname"><em>kwidth</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">uint8_t&#160;</td>
          <td class="paramname"><em>zero_byte</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const <a class="el" href="classnnfw_1_1cker_1_1_shape.html">Shape</a> &amp;&#160;</td>
          <td class="paramname"><em>input_shape</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const T *&#160;</td>
          <td class="paramname"><em>input_data</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const <a class="el" href="classnnfw_1_1cker_1_1_shape.html">Shape</a> &amp;&#160;</td>
          <td class="paramname"><em>output_shape</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">T *&#160;</td>
          <td class="paramname"><em>output_data</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">

<p class="definition">Definition at line <a class="el" href="_optimized_utils_8h_source.html#l00222">222</a> of file <a class="el" href="_optimized_utils_8h_source.html">OptimizedUtils.h</a>.</p>
<div class="fragment"><div class="line"><span class="lineno">  225</span>{</div>
<div class="line"><span class="lineno">  226</span>  <span class="keyword">const</span> <span class="keywordtype">int</span> stride_width = params.<a class="code hl_variable" href="structnnfw_1_1cker_1_1_conv_params.html#a273ecef205d663377502d9dcaa77f1b4">stride_width</a>;</div>
<div class="line"><span class="lineno">  227</span>  <span class="keyword">const</span> <span class="keywordtype">int</span> stride_height = params.<a class="code hl_variable" href="structnnfw_1_1cker_1_1_conv_params.html#a0878ee74c474c8a82a574727a2a3048e">stride_height</a>;</div>
<div class="line"><span class="lineno">  228</span>  <span class="keyword">const</span> <span class="keywordtype">int</span> pad_width = params.<a class="code hl_variable" href="structnnfw_1_1cker_1_1_conv_params.html#a0a3450e62d20d27c2638d197f1fd2bb2">padding_values</a>.<a class="code hl_variable" href="structnnfw_1_1cker_1_1_padding_values.html#a2a3dd79d28c30c5da865290160601fd1">width</a>;</div>
<div class="line"><span class="lineno">  229</span>  <span class="keyword">const</span> <span class="keywordtype">int</span> pad_height = params.<a class="code hl_variable" href="structnnfw_1_1cker_1_1_conv_params.html#a0a3450e62d20d27c2638d197f1fd2bb2">padding_values</a>.<a class="code hl_variable" href="structnnfw_1_1cker_1_1_padding_values.html#a1dc41a9ebbd61a7c34ed2b5dd2ec10eb">height</a>;</div>
<div class="line"><span class="lineno">  230</span>  assert(input_shape.<a class="code hl_function" href="classnnfw_1_1cker_1_1_shape.html#a01c6b5dc4b24d5c2567c47cb15318839">DimensionsCount</a>() == 4);</div>
<div class="line"><span class="lineno">  231</span>  assert(output_shape.DimensionsCount() == 4);</div>
<div class="line"><span class="lineno">  232</span> </div>
<div class="line"><span class="lineno">  233</span>  <span class="keyword">const</span> <span class="keywordtype">int</span> batches = <a class="code hl_function" href="namespacennfw_1_1cker.html#a0a74e72b1bffcb6519de7fc224416ff9">MatchingDim</a>(input_shape, 0, output_shape, 0);</div>
<div class="line"><span class="lineno">  234</span>  <span class="keyword">const</span> <span class="keywordtype">int</span> input_depth = input_shape.<a class="code hl_function" href="classnnfw_1_1cker_1_1_shape.html#a497180ee7844bbef51b36bd58e61fa31">Dims</a>(3);</div>
<div class="line"><span class="lineno">  235</span>  <span class="keyword">const</span> <span class="keywordtype">int</span> input_width = input_shape.<a class="code hl_function" href="classnnfw_1_1cker_1_1_shape.html#a497180ee7844bbef51b36bd58e61fa31">Dims</a>(2);</div>
<div class="line"><span class="lineno">  236</span>  <span class="keyword">const</span> <span class="keywordtype">int</span> input_height = input_shape.<a class="code hl_function" href="classnnfw_1_1cker_1_1_shape.html#a497180ee7844bbef51b36bd58e61fa31">Dims</a>(1);</div>
<div class="line"><span class="lineno">  237</span>  <span class="keyword">const</span> <span class="keywordtype">int</span> output_depth = output_shape.Dims(3);</div>
<div class="line"><span class="lineno">  238</span>  <span class="keyword">const</span> <span class="keywordtype">int</span> output_width = output_shape.Dims(2);</div>
<div class="line"><span class="lineno">  239</span>  <span class="keyword">const</span> <span class="keywordtype">int</span> output_height = output_shape.Dims(1);</div>
<div class="line"><span class="lineno">  240</span> </div>
<div class="line"><span class="lineno">  241</span>  <span class="keywordtype">int</span> buffer_id = 0;</div>
<div class="line"><span class="lineno">  242</span>  <span class="comment">// Loop over the output nodes.</span></div>
<div class="line"><span class="lineno">  243</span>  <span class="keywordflow">for</span> (<span class="keywordtype">int</span> b = 0; b &lt; batches; ++b)</div>
<div class="line"><span class="lineno">  244</span>  {</div>
<div class="line"><span class="lineno">  245</span>    <span class="keywordflow">for</span> (<span class="keywordtype">int</span> h = 0; h &lt; output_height; ++h)</div>
<div class="line"><span class="lineno">  246</span>    {</div>
<div class="line"><span class="lineno">  247</span>      <span class="keywordflow">for</span> (<span class="keywordtype">int</span> w = 0; w &lt; output_width; ++w)</div>
<div class="line"><span class="lineno">  248</span>      {</div>
<div class="line"><span class="lineno">  249</span>        <a class="code hl_function" href="_conv2_d_8float_8cpp.html#ae4aac893b639643d7687d01f8c4b28a9">ExtractPatchIntoBufferColumn</a>(input_shape, w, h, b, kheight, kwidth, stride_width,</div>
<div class="line"><span class="lineno">  250</span>                                     stride_height, pad_width, pad_height, input_width,</div>
<div class="line"><span class="lineno">  251</span>                                     input_height, input_depth, output_depth, buffer_id, input_data,</div>
<div class="line"><span class="lineno">  252</span>                                     output_data, zero_byte);</div>
<div class="line"><span class="lineno">  253</span>        ++buffer_id;</div>
<div class="line"><span class="lineno">  254</span>      }</div>
<div class="line"><span class="lineno">  255</span>    }</div>
<div class="line"><span class="lineno">  256</span>  }</div>
<div class="line"><span class="lineno">  257</span>}</div>
<div class="ttc" id="a_conv2_d_8float_8cpp_html_ae4aac893b639643d7687d01f8c4b28a9"><div class="ttname"><a href="_conv2_d_8float_8cpp.html#ae4aac893b639643d7687d01f8c4b28a9">ExtractPatchIntoBufferColumn</a></div><div class="ttdeci">void ExtractPatchIntoBufferColumn(const Dims&lt; 4 &gt; &amp;input_dims, int w, int h, int b, int kheight, int kwidth, int stride_width, int stride_height, int pad_width, int pad_height, int in_width, int in_height, int in_depth, int single_buffer_length, int buffer_id, const T *in_data, T *conv_buffer_data, uint8 byte_zero)</div><div class="ttdef"><b>Definition:</b> <a href="_conv2_d_8float_8cpp_source.html#l00030">Conv2D.float.cpp:30</a></div></div>
</div><!-- fragment -->
<p class="reference">References <a class="el" href="compute_2cker_2include_2cker_2_shape_8h_source.html#l00093">nnfw::cker::Shape::DimensionsCount()</a>, <a class="el" href="compute_2cker_2include_2cker_2_shape_8h_source.html#l00094">nnfw::cker::Shape::Dims()</a>, <a class="el" href="_optimized_utils_8h_source.html#l00034">ExtractPatchIntoBufferColumn()</a>, <a class="el" href="compute_2cker_2include_2cker_2_types_8h_source.html#l00069">nnfw::cker::PaddingValues::height</a>, <a class="el" href="compute_2cker_2include_2cker_2_shape_8h_source.html#l00222">nnfw::cker::MatchingDim()</a>, <a class="el" href="compute_2cker_2include_2cker_2_types_8h_source.html#l00137">nnfw::cker::ConvParams::padding_values</a>, <a class="el" href="compute_2cker_2include_2cker_2_types_8h_source.html#l00140">nnfw::cker::ConvParams::stride_height</a>, <a class="el" href="compute_2cker_2include_2cker_2_types_8h_source.html#l00139">nnfw::cker::ConvParams::stride_width</a>, and <a class="el" href="compute_2cker_2include_2cker_2_types_8h_source.html#l00068">nnfw::cker::PaddingValues::width</a>.</p>

<p class="reference">Referenced by <a class="el" href="compute_2cker_2include_2cker_2operation_2optimized_2_conv_8h_source.html#l00083">Conv()</a>.</p>

</div>
</div>
<a id="a4aa4bafb600c8c06ce9dc5217ee5cf22" name="a4aa4bafb600c8c06ce9dc5217ee5cf22"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a4aa4bafb600c8c06ce9dc5217ee5cf22">&#9670;&#160;</a></span>Mul() <span class="overload">[1/2]</span></h2>

<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">void nnfw::cker::optimized::Mul </td>
          <td>(</td>
          <td class="paramtype">const <a class="el" href="structnnfw_1_1cker_1_1_binary_arithmetic_op_param.html">BinaryArithmeticOpParam</a> &amp;&#160;</td>
          <td class="paramname"><em>params</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const <a class="el" href="classnnfw_1_1cker_1_1_shape.html">Shape</a> &amp;&#160;</td>
          <td class="paramname"><em>input1_shape</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const float *&#160;</td>
          <td class="paramname"><em>input1_data</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const <a class="el" href="classnnfw_1_1cker_1_1_shape.html">Shape</a> &amp;&#160;</td>
          <td class="paramname"><em>input2_shape</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const float *&#160;</td>
          <td class="paramname"><em>input2_data</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const <a class="el" href="classnnfw_1_1cker_1_1_shape.html">Shape</a> &amp;&#160;</td>
          <td class="paramname"><em>output_shape</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">float *&#160;</td>
          <td class="paramname"><em>output_data</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">inline</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p class="definition">Definition at line <a class="el" href="optimized_2_binary_arithmetic_ops_8h_source.html#l01072">1072</a> of file <a class="el" href="optimized_2_binary_arithmetic_ops_8h_source.html">BinaryArithmeticOps.h</a>.</p>
<div class="fragment"><div class="line"><span class="lineno"> 1075</span>{</div>
<div class="line"><span class="lineno"> 1076</span>  <span class="keyword">const</span> <span class="keywordtype">int</span> flat_size = <a class="code hl_function" href="namespacennfw_1_1cker.html#ab86126de4e835f9499051e43b841abca">MatchingElementsSize</a>(input1_shape, input2_shape, output_shape);</div>
<div class="line"><span class="lineno"> 1077</span>  <span class="keyword">auto</span> implFuncs = getBinaryOpWithActivationImplFloat&lt;BinaryOpFuncMulFloat&gt;(params);</div>
<div class="line"><span class="lineno"> 1078</span>  (*implFuncs.first)(flat_size, params, input1_data, input2_data, output_data);</div>
<div class="line"><span class="lineno"> 1079</span>}</div>
</div><!-- fragment -->
<p class="reference">References <a class="el" href="compute_2cker_2include_2cker_2_shape_8h_source.html#l00337">nnfw::cker::MatchingElementsSize()</a>.</p>

</div>
</div>
<a id="a09dafd64d21fcf9e4c9a1b4c735d1b22" name="a09dafd64d21fcf9e4c9a1b4c735d1b22"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a09dafd64d21fcf9e4c9a1b4c735d1b22">&#9670;&#160;</a></span>Mul() <span class="overload">[2/2]</span></h2>

<div class="memitem">
<div class="memproto">
<div class="memtemplate">
template&lt;typename T &gt; </div>
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">std::enable_if_t&lt; <a class="el" href="structnnfw_1_1cker_1_1is__quant8.html">is_quant8</a>&lt; T &gt;::value &gt; nnfw::cker::optimized::Mul </td>
          <td>(</td>
          <td class="paramtype">const <a class="el" href="structnnfw_1_1cker_1_1_binary_arithmetic_op_param.html">BinaryArithmeticOpParam</a> &amp;&#160;</td>
          <td class="paramname"><em>params</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const <a class="el" href="classnnfw_1_1cker_1_1_shape.html">Shape</a> &amp;&#160;</td>
          <td class="paramname"><em>input1_shape</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const T *&#160;</td>
          <td class="paramname"><em>input1_data</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const <a class="el" href="classnnfw_1_1cker_1_1_shape.html">Shape</a> &amp;&#160;</td>
          <td class="paramname"><em>input2_shape</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const T *&#160;</td>
          <td class="paramname"><em>input2_data</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const <a class="el" href="classnnfw_1_1cker_1_1_shape.html">Shape</a> &amp;&#160;</td>
          <td class="paramname"><em>output_shape</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">T *&#160;</td>
          <td class="paramname"><em>output_data</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">inline</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p class="definition">Definition at line <a class="el" href="optimized_2_binary_arithmetic_ops_8h_source.html#l01065">1065</a> of file <a class="el" href="optimized_2_binary_arithmetic_ops_8h_source.html">BinaryArithmeticOps.h</a>.</p>
<div class="fragment"><div class="line"><span class="lineno"> 1067</span>{</div>
<div class="line"><span class="lineno"> 1068</span>  <span class="keyword">const</span> <span class="keywordtype">int</span> flat_size = <a class="code hl_function" href="namespacennfw_1_1cker.html#ab86126de4e835f9499051e43b841abca">MatchingElementsSize</a>(input1_shape, input2_shape, output_shape);</div>
<div class="line"><span class="lineno"> 1069</span>  <a class="code hl_function" href="namespacennfw_1_1cker_1_1optimized.html#a38f818fd8d5df7e473fa22d6491d0ad1">MulElementwise</a>(flat_size, params, input1_data, input2_data, output_data);</div>
<div class="line"><span class="lineno"> 1070</span>}</div>
<div class="ttc" id="anamespacennfw_1_1cker_1_1optimized_html_a38f818fd8d5df7e473fa22d6491d0ad1"><div class="ttname"><a href="namespacennfw_1_1cker_1_1optimized.html#a38f818fd8d5df7e473fa22d6491d0ad1">nnfw::cker::optimized::MulElementwise</a></div><div class="ttdeci">void MulElementwise(int size, const BinaryArithmeticOpParam &amp;params, const uint8_t *input1_data, const uint8_t *input2_data, uint8_t *output_data)</div><div class="ttdef"><b>Definition:</b> <a href="optimized_2_binary_arithmetic_ops_8h_source.html#l00910">BinaryArithmeticOps.h:910</a></div></div>
</div><!-- fragment -->
<p class="reference">References <a class="el" href="compute_2cker_2include_2cker_2_shape_8h_source.html#l00337">nnfw::cker::MatchingElementsSize()</a>, and <a class="el" href="optimized_2_binary_arithmetic_ops_8h_source.html#l00910">MulElementwise()</a>.</p>

<p class="reference">Referenced by <a class="el" href="_binary_arithmetic_ops_8h_source.html#l00204">nnfw::cker::BinaryArithmeticOp()</a>.</p>

</div>
</div>
<a id="a002839028eca24a863415f9a73e13958" name="a002839028eca24a863415f9a73e13958"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a002839028eca24a863415f9a73e13958">&#9670;&#160;</a></span>MulElementwise() <span class="overload">[1/2]</span></h2>

<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">void nnfw::cker::optimized::MulElementwise </td>
          <td>(</td>
          <td class="paramtype">int&#160;</td>
          <td class="paramname"><em>size</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const <a class="el" href="structnnfw_1_1cker_1_1_binary_arithmetic_op_param.html">BinaryArithmeticOpParam</a> &amp;&#160;</td>
          <td class="paramname"><em>params</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const int8_t *&#160;</td>
          <td class="paramname"><em>input1_data</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const int8_t *&#160;</td>
          <td class="paramname"><em>input2_data</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">int8_t *&#160;</td>
          <td class="paramname"><em>output_data</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">inline</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p class="definition">Definition at line <a class="el" href="optimized_2_binary_arithmetic_ops_8h_source.html#l00974">974</a> of file <a class="el" href="optimized_2_binary_arithmetic_ops_8h_source.html">BinaryArithmeticOps.h</a>.</p>
<div class="fragment"><div class="line"><span class="lineno">  977</span>{</div>
<div class="line"><span class="lineno">  978</span>  <span class="keywordtype">int</span> i = 0;</div>
<div class="line"><span class="lineno">  979</span><span class="preprocessor">#ifdef USE_NEON</span></div>
<div class="line"><span class="lineno">  980</span>  <span class="keyword">const</span> int16x8_t input1_offset_vector = vdupq_n_s16(params.<a class="code hl_variable" href="structnnfw_1_1cker_1_1_binary_arithmetic_op_param.html#a553e59d259a380f2da048553a23af0b0">input1_offset</a>);</div>
<div class="line"><span class="lineno">  981</span>  <span class="keyword">const</span> int16x8_t input2_offset_vector = vdupq_n_s16(params.<a class="code hl_variable" href="structnnfw_1_1cker_1_1_binary_arithmetic_op_param.html#ac4d8a22f96208f1efd75047318475f03">input2_offset</a>);</div>
<div class="line"><span class="lineno">  982</span>  <span class="keyword">const</span> int16x8_t output_offset_vector = vdupq_n_s16(params.<a class="code hl_variable" href="structnnfw_1_1cker_1_1_binary_arithmetic_op_param.html#aa5bf6b7c6e3772bd04264f6729915729">output_offset</a>);</div>
<div class="line"><span class="lineno">  983</span>  <span class="keyword">const</span> <span class="keyword">auto</span> output_activation_min_vector = vdupq_n_s8(params.<a class="code hl_variable" href="structnnfw_1_1cker_1_1_binary_arithmetic_op_param.html#abb8ef3401fcae98626f1da50cda22b2f">quantized_activation_min</a>);</div>
<div class="line"><span class="lineno">  984</span>  <span class="keyword">const</span> <span class="keyword">auto</span> output_activation_max_vector = vdupq_n_s8(params.<a class="code hl_variable" href="structnnfw_1_1cker_1_1_binary_arithmetic_op_param.html#a20d919b6418bf302a97df426260bdec0">quantized_activation_max</a>);</div>
<div class="line"><span class="lineno">  985</span>  <span class="keyword">const</span> <span class="keywordtype">int</span> left_shift = std::max(0, params.<a class="code hl_variable" href="structnnfw_1_1cker_1_1_binary_arithmetic_op_param.html#a1c0adc1575a15790a35479c5fa5093a4">output_shift</a>);</div>
<div class="line"><span class="lineno">  986</span>  <span class="keyword">const</span> <span class="keywordtype">int</span> right_shift = std::max(0, -params.<a class="code hl_variable" href="structnnfw_1_1cker_1_1_binary_arithmetic_op_param.html#a1c0adc1575a15790a35479c5fa5093a4">output_shift</a>);</div>
<div class="line"><span class="lineno">  987</span>  <span class="keyword">const</span> int32x4_t left_shift_vec = vdupq_n_s32(left_shift);</div>
<div class="line"><span class="lineno">  988</span>  <span class="keywordflow">for</span> (; i &lt;= size - 16; i += 16)</div>
<div class="line"><span class="lineno">  989</span>  {</div>
<div class="line"><span class="lineno">  990</span>    <span class="comment">// We load / store 16 at a time, multiplying as four sets of 4 int32s.</span></div>
<div class="line"><span class="lineno">  991</span>    <span class="keyword">const</span> int8x16_t input1_val_original = vld1q_s8(input1_data + i);</div>
<div class="line"><span class="lineno">  992</span>    <span class="keyword">const</span> int8x16_t input2_val_original = vld1q_s8(input2_data + i);</div>
<div class="line"><span class="lineno">  993</span> </div>
<div class="line"><span class="lineno">  994</span>    <span class="keyword">const</span> int16x8_t input1_val_s16_high = vmovl_s8(vget_high_s8(input1_val_original));</div>
<div class="line"><span class="lineno">  995</span>    <span class="keyword">const</span> int16x8_t input1_val_s16_low = vmovl_s8(vget_low_s8(input1_val_original));</div>
<div class="line"><span class="lineno">  996</span> </div>
<div class="line"><span class="lineno">  997</span>    <span class="keyword">const</span> int16x8_t input2_val_s16_high = vmovl_s8(vget_high_s8(input2_val_original));</div>
<div class="line"><span class="lineno">  998</span>    <span class="keyword">const</span> int16x8_t input2_val_s16_low = vmovl_s8(vget_low_s8(input2_val_original));</div>
<div class="line"><span class="lineno">  999</span>    <span class="keyword">const</span> int16x8_t input1_val_high = vaddq_s16(input1_val_s16_high, input1_offset_vector);</div>
<div class="line"><span class="lineno"> 1000</span>    <span class="keyword">const</span> int16x8_t input2_val_high = vaddq_s16(input2_val_s16_high, input2_offset_vector);</div>
<div class="line"><span class="lineno"> 1001</span>    <span class="keyword">const</span> int16x8_t input1_val_low = vaddq_s16(input1_val_s16_low, input1_offset_vector);</div>
<div class="line"><span class="lineno"> 1002</span>    <span class="keyword">const</span> int16x8_t input2_val_low = vaddq_s16(input2_val_s16_low, input2_offset_vector);</div>
<div class="line"><span class="lineno"> 1003</span>    <span class="keyword">const</span> int16x4_t input1_val_high_high = vget_high_s16(input1_val_high);</div>
<div class="line"><span class="lineno"> 1004</span>    <span class="keyword">const</span> int16x4_t input1_val_high_low = vget_low_s16(input1_val_high);</div>
<div class="line"><span class="lineno"> 1005</span>    <span class="keyword">const</span> int16x4_t input1_val_low_high = vget_high_s16(input1_val_low);</div>
<div class="line"><span class="lineno"> 1006</span>    <span class="keyword">const</span> int16x4_t input1_val_low_low = vget_low_s16(input1_val_low);</div>
<div class="line"><span class="lineno"> 1007</span>    <span class="keyword">const</span> int16x4_t input2_val_high_high = vget_high_s16(input2_val_high);</div>
<div class="line"><span class="lineno"> 1008</span>    <span class="keyword">const</span> int16x4_t input2_val_high_low = vget_low_s16(input2_val_high);</div>
<div class="line"><span class="lineno"> 1009</span>    <span class="keyword">const</span> int16x4_t input2_val_low_high = vget_high_s16(input2_val_low);</div>
<div class="line"><span class="lineno"> 1010</span>    <span class="keyword">const</span> int16x4_t input2_val_low_low = vget_low_s16(input2_val_low);</div>
<div class="line"><span class="lineno"> 1011</span> </div>
<div class="line"><span class="lineno"> 1012</span>    <span class="keyword">auto</span> p1 = vmull_s16(input2_val_high_high, input1_val_high_high);</div>
<div class="line"><span class="lineno"> 1013</span>    <span class="keyword">auto</span> p2 = vmull_s16(input2_val_high_low, input1_val_high_low);</div>
<div class="line"><span class="lineno"> 1014</span>    <span class="keyword">auto</span> p3 = vmull_s16(input2_val_low_high, input1_val_low_high);</div>
<div class="line"><span class="lineno"> 1015</span>    <span class="keyword">auto</span> p4 = vmull_s16(input2_val_low_low, input1_val_low_low);</div>
<div class="line"><span class="lineno"> 1016</span> </div>
<div class="line"><span class="lineno"> 1017</span>    p1 = vshlq_s32(p1, left_shift_vec);</div>
<div class="line"><span class="lineno"> 1018</span>    p2 = vshlq_s32(p2, left_shift_vec);</div>
<div class="line"><span class="lineno"> 1019</span>    p3 = vshlq_s32(p3, left_shift_vec);</div>
<div class="line"><span class="lineno"> 1020</span>    p4 = vshlq_s32(p4, left_shift_vec);</div>
<div class="line"><span class="lineno"> 1021</span> </div>
<div class="line"><span class="lineno"> 1022</span>    p1 = vqrdmulhq_n_s32(p1, params.<a class="code hl_variable" href="structnnfw_1_1cker_1_1_binary_arithmetic_op_param.html#adae6f3cd4f5808bde90f4dfd3d915b4c">output_multiplier</a>);</div>
<div class="line"><span class="lineno"> 1023</span>    p2 = vqrdmulhq_n_s32(p2, params.<a class="code hl_variable" href="structnnfw_1_1cker_1_1_binary_arithmetic_op_param.html#adae6f3cd4f5808bde90f4dfd3d915b4c">output_multiplier</a>);</div>
<div class="line"><span class="lineno"> 1024</span>    p3 = vqrdmulhq_n_s32(p3, params.<a class="code hl_variable" href="structnnfw_1_1cker_1_1_binary_arithmetic_op_param.html#adae6f3cd4f5808bde90f4dfd3d915b4c">output_multiplier</a>);</div>
<div class="line"><span class="lineno"> 1025</span>    p4 = vqrdmulhq_n_s32(p4, params.<a class="code hl_variable" href="structnnfw_1_1cker_1_1_binary_arithmetic_op_param.html#adae6f3cd4f5808bde90f4dfd3d915b4c">output_multiplier</a>);</div>
<div class="line"><span class="lineno"> 1026</span>    <span class="keyword">using </span>gemmlowp::RoundingDivideByPOT;</div>
<div class="line"><span class="lineno"> 1027</span>    p1 = RoundingDivideByPOT(p1, right_shift);</div>
<div class="line"><span class="lineno"> 1028</span>    p2 = RoundingDivideByPOT(p2, right_shift);</div>
<div class="line"><span class="lineno"> 1029</span>    p3 = RoundingDivideByPOT(p3, right_shift);</div>
<div class="line"><span class="lineno"> 1030</span>    p4 = RoundingDivideByPOT(p4, right_shift);</div>
<div class="line"><span class="lineno"> 1031</span> </div>
<div class="line"><span class="lineno"> 1032</span>    <span class="keyword">const</span> <span class="keyword">auto</span> p1_narrowed = vqmovn_s32(p1);</div>
<div class="line"><span class="lineno"> 1033</span>    <span class="keyword">const</span> <span class="keyword">auto</span> p2_narrowed = vqmovn_s32(p2);</div>
<div class="line"><span class="lineno"> 1034</span>    <span class="keyword">const</span> <span class="keyword">auto</span> p3_narrowed = vqmovn_s32(p3);</div>
<div class="line"><span class="lineno"> 1035</span>    <span class="keyword">const</span> <span class="keyword">auto</span> p4_narrowed = vqmovn_s32(p4);</div>
<div class="line"><span class="lineno"> 1036</span> </div>
<div class="line"><span class="lineno"> 1037</span>    <span class="keyword">const</span> int16x8_t p_part1 =</div>
<div class="line"><span class="lineno"> 1038</span>      vaddq_s16(vcombine_s16(p2_narrowed, p1_narrowed), output_offset_vector);</div>
<div class="line"><span class="lineno"> 1039</span>    <span class="keyword">const</span> int16x8_t p_part2 =</div>
<div class="line"><span class="lineno"> 1040</span>      vaddq_s16(vcombine_s16(p4_narrowed, p3_narrowed), output_offset_vector);</div>
<div class="line"><span class="lineno"> 1041</span>    <span class="keyword">const</span> int8x16_t p = vcombine_s8(vqmovn_s16(p_part2), vqmovn_s16(p_part1));</div>
<div class="line"><span class="lineno"> 1042</span> </div>
<div class="line"><span class="lineno"> 1043</span>    <span class="keyword">const</span> <span class="keyword">auto</span> clamped =</div>
<div class="line"><span class="lineno"> 1044</span>      vmaxq_s8(output_activation_min_vector, vminq_s8(output_activation_max_vector, p));</div>
<div class="line"><span class="lineno"> 1045</span>    vst1q_s8(output_data + i, clamped);</div>
<div class="line"><span class="lineno"> 1046</span>  }</div>
<div class="line"><span class="lineno"> 1047</span><span class="preprocessor">#endif </span><span class="comment">// NEON</span></div>
<div class="line"><span class="lineno"> 1048</span> </div>
<div class="line"><span class="lineno"> 1049</span>  <span class="keywordflow">for</span> (; i &lt; size; ++i)</div>
<div class="line"><span class="lineno"> 1050</span>  {</div>
<div class="line"><span class="lineno"> 1051</span>    <span class="keyword">const</span> int32_t input1_val = params.<a class="code hl_variable" href="structnnfw_1_1cker_1_1_binary_arithmetic_op_param.html#a553e59d259a380f2da048553a23af0b0">input1_offset</a> + input1_data[i];</div>
<div class="line"><span class="lineno"> 1052</span>    <span class="keyword">const</span> int32_t input2_val = params.<a class="code hl_variable" href="structnnfw_1_1cker_1_1_binary_arithmetic_op_param.html#ac4d8a22f96208f1efd75047318475f03">input2_offset</a> + input2_data[i];</div>
<div class="line"><span class="lineno"> 1053</span>    <span class="keyword">const</span> int32_t unclamped_result =</div>
<div class="line"><span class="lineno"> 1054</span>      params.<a class="code hl_variable" href="structnnfw_1_1cker_1_1_binary_arithmetic_op_param.html#aa5bf6b7c6e3772bd04264f6729915729">output_offset</a> + <a class="code hl_function" href="namespacennfw_1_1cker.html#af6b33a8f0c1d60e578337bdb8dcb3673">MultiplyByQuantizedMultiplier</a>(input1_val * input2_val,</div>
<div class="line"><span class="lineno"> 1055</span>                                                           params.<a class="code hl_variable" href="structnnfw_1_1cker_1_1_binary_arithmetic_op_param.html#adae6f3cd4f5808bde90f4dfd3d915b4c">output_multiplier</a>,</div>
<div class="line"><span class="lineno"> 1056</span>                                                           params.<a class="code hl_variable" href="structnnfw_1_1cker_1_1_binary_arithmetic_op_param.html#a1c0adc1575a15790a35479c5fa5093a4">output_shift</a>);</div>
<div class="line"><span class="lineno"> 1057</span>    <span class="keyword">const</span> int32_t clamped_output = std::min(</div>
<div class="line"><span class="lineno"> 1058</span>      params.<a class="code hl_variable" href="structnnfw_1_1cker_1_1_binary_arithmetic_op_param.html#a20d919b6418bf302a97df426260bdec0">quantized_activation_max</a>, std::max(params.<a class="code hl_variable" href="structnnfw_1_1cker_1_1_binary_arithmetic_op_param.html#abb8ef3401fcae98626f1da50cda22b2f">quantized_activation_min</a>, unclamped_result));</div>
<div class="line"><span class="lineno"> 1059</span>    output_data[i] = <span class="keyword">static_cast&lt;</span>int8_t<span class="keyword">&gt;</span>(clamped_output);</div>
<div class="line"><span class="lineno"> 1060</span>  }</div>
<div class="line"><span class="lineno"> 1061</span>}</div>
<div class="ttc" id="anamespacennfw_1_1cker_html_af6b33a8f0c1d60e578337bdb8dcb3673"><div class="ttname"><a href="namespacennfw_1_1cker.html#af6b33a8f0c1d60e578337bdb8dcb3673">nnfw::cker::MultiplyByQuantizedMultiplier</a></div><div class="ttdeci">int32_t MultiplyByQuantizedMultiplier(int32_t x, int32_t quantized_multiplier, int shift)</div><div class="ttdef"><b>Definition:</b> <a href="compute_2cker_2include_2cker_2_utils_8h_source.html#l00093">Utils.h:93</a></div></div>
</div><!-- fragment -->
<p class="reference">References <a class="el" href="compute_2cker_2include_2cker_2_types_8h_source.html#l00177">nnfw::cker::BinaryArithmeticOpParam::input1_offset</a>, <a class="el" href="compute_2cker_2include_2cker_2_types_8h_source.html#l00178">nnfw::cker::BinaryArithmeticOpParam::input2_offset</a>, <a class="el" href="compute_2cker_2include_2cker_2_utils_8h_source.html#l00093">nnfw::cker::MultiplyByQuantizedMultiplier()</a>, <a class="el" href="compute_2cker_2include_2cker_2_types_8h_source.html#l00180">nnfw::cker::BinaryArithmeticOpParam::output_multiplier</a>, <a class="el" href="compute_2cker_2include_2cker_2_types_8h_source.html#l00179">nnfw::cker::BinaryArithmeticOpParam::output_offset</a>, <a class="el" href="compute_2cker_2include_2cker_2_types_8h_source.html#l00181">nnfw::cker::BinaryArithmeticOpParam::output_shift</a>, <a class="el" href="compute_2cker_2include_2cker_2_types_8h_source.html#l00190">nnfw::cker::BinaryArithmeticOpParam::quantized_activation_max</a>, and <a class="el" href="compute_2cker_2include_2cker_2_types_8h_source.html#l00189">nnfw::cker::BinaryArithmeticOpParam::quantized_activation_min</a>.</p>

</div>
</div>
<a id="a38f818fd8d5df7e473fa22d6491d0ad1" name="a38f818fd8d5df7e473fa22d6491d0ad1"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a38f818fd8d5df7e473fa22d6491d0ad1">&#9670;&#160;</a></span>MulElementwise() <span class="overload">[2/2]</span></h2>

<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">void nnfw::cker::optimized::MulElementwise </td>
          <td>(</td>
          <td class="paramtype">int&#160;</td>
          <td class="paramname"><em>size</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const <a class="el" href="structnnfw_1_1cker_1_1_binary_arithmetic_op_param.html">BinaryArithmeticOpParam</a> &amp;&#160;</td>
          <td class="paramname"><em>params</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const uint8_t *&#160;</td>
          <td class="paramname"><em>input1_data</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const uint8_t *&#160;</td>
          <td class="paramname"><em>input2_data</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">uint8_t *&#160;</td>
          <td class="paramname"><em>output_data</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">inline</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p class="definition">Definition at line <a class="el" href="optimized_2_binary_arithmetic_ops_8h_source.html#l00910">910</a> of file <a class="el" href="optimized_2_binary_arithmetic_ops_8h_source.html">BinaryArithmeticOps.h</a>.</p>
<div class="fragment"><div class="line"><span class="lineno">  913</span>{</div>
<div class="line"><span class="lineno">  914</span>  <span class="keywordtype">int</span> i = 0;</div>
<div class="line"><span class="lineno">  915</span> </div>
<div class="line"><span class="lineno">  916</span><span class="preprocessor">#ifdef USE_NEON</span></div>
<div class="line"><span class="lineno">  917</span>  <span class="keyword">const</span> <span class="keyword">auto</span> input1_offset_vector = vdupq_n_s16(params.<a class="code hl_variable" href="structnnfw_1_1cker_1_1_binary_arithmetic_op_param.html#a553e59d259a380f2da048553a23af0b0">input1_offset</a>);</div>
<div class="line"><span class="lineno">  918</span>  <span class="keyword">const</span> <span class="keyword">auto</span> input2_offset_vector = vdupq_n_s16(params.<a class="code hl_variable" href="structnnfw_1_1cker_1_1_binary_arithmetic_op_param.html#ac4d8a22f96208f1efd75047318475f03">input2_offset</a>);</div>
<div class="line"><span class="lineno">  919</span>  <span class="keyword">const</span> <span class="keyword">auto</span> output_offset_vector = vdupq_n_s16(params.<a class="code hl_variable" href="structnnfw_1_1cker_1_1_binary_arithmetic_op_param.html#aa5bf6b7c6e3772bd04264f6729915729">output_offset</a>);</div>
<div class="line"><span class="lineno">  920</span>  <span class="keyword">const</span> <span class="keyword">auto</span> output_activation_min_vector = vdup_n_u8(params.<a class="code hl_variable" href="structnnfw_1_1cker_1_1_binary_arithmetic_op_param.html#abb8ef3401fcae98626f1da50cda22b2f">quantized_activation_min</a>);</div>
<div class="line"><span class="lineno">  921</span>  <span class="keyword">const</span> <span class="keyword">auto</span> output_activation_max_vector = vdup_n_u8(params.<a class="code hl_variable" href="structnnfw_1_1cker_1_1_binary_arithmetic_op_param.html#a20d919b6418bf302a97df426260bdec0">quantized_activation_max</a>);</div>
<div class="line"><span class="lineno">  922</span>  <span class="keyword">const</span> <span class="keywordtype">int</span> left_shift = std::max(0, params.<a class="code hl_variable" href="structnnfw_1_1cker_1_1_binary_arithmetic_op_param.html#a1c0adc1575a15790a35479c5fa5093a4">output_shift</a>);</div>
<div class="line"><span class="lineno">  923</span>  <span class="keyword">const</span> <span class="keywordtype">int</span> right_shift = std::max(0, -params.<a class="code hl_variable" href="structnnfw_1_1cker_1_1_binary_arithmetic_op_param.html#a1c0adc1575a15790a35479c5fa5093a4">output_shift</a>);</div>
<div class="line"><span class="lineno">  924</span>  <span class="keyword">const</span> int32x4_t left_shift_vec = vdupq_n_s32(left_shift);</div>
<div class="line"><span class="lineno">  925</span>  <span class="keywordflow">for</span> (; i &lt;= size - 8; i += 8)</div>
<div class="line"><span class="lineno">  926</span>  {</div>
<div class="line"><span class="lineno">  927</span>    <span class="comment">// We load / store 8 at a time, multiplying as two sets of 4 int32s.</span></div>
<div class="line"><span class="lineno">  928</span>    <span class="keyword">const</span> <span class="keyword">auto</span> input1_val_original = vld1_u8(input1_data + i);</div>
<div class="line"><span class="lineno">  929</span>    <span class="keyword">const</span> <span class="keyword">auto</span> input2_val_original = vld1_u8(input2_data + i);</div>
<div class="line"><span class="lineno">  930</span>    <span class="keyword">const</span> <span class="keyword">auto</span> input1_val_s16 = vreinterpretq_s16_u16(vmovl_u8(input1_val_original));</div>
<div class="line"><span class="lineno">  931</span>    <span class="keyword">const</span> <span class="keyword">auto</span> input2_val_s16 = vreinterpretq_s16_u16(vmovl_u8(input2_val_original));</div>
<div class="line"><span class="lineno">  932</span>    <span class="keyword">const</span> <span class="keyword">auto</span> input1_val = vaddq_s16(input1_val_s16, input1_offset_vector);</div>
<div class="line"><span class="lineno">  933</span>    <span class="keyword">const</span> <span class="keyword">auto</span> input2_val = vaddq_s16(input2_val_s16, input2_offset_vector);</div>
<div class="line"><span class="lineno">  934</span> </div>
<div class="line"><span class="lineno">  935</span>    <span class="keyword">const</span> <span class="keyword">auto</span> input1_val_low = vget_low_s16(input1_val);</div>
<div class="line"><span class="lineno">  936</span>    <span class="keyword">const</span> <span class="keyword">auto</span> input1_val_high = vget_high_s16(input1_val);</div>
<div class="line"><span class="lineno">  937</span>    <span class="keyword">const</span> <span class="keyword">auto</span> input2_val_low = vget_low_s16(input2_val);</div>
<div class="line"><span class="lineno">  938</span>    <span class="keyword">const</span> <span class="keyword">auto</span> input2_val_high = vget_high_s16(input2_val);</div>
<div class="line"><span class="lineno">  939</span> </div>
<div class="line"><span class="lineno">  940</span>    <span class="keyword">auto</span> p1 = vmull_s16(input2_val_low, input1_val_low);</div>
<div class="line"><span class="lineno">  941</span>    <span class="keyword">auto</span> p2 = vmull_s16(input2_val_high, input1_val_high);</div>
<div class="line"><span class="lineno">  942</span> </div>
<div class="line"><span class="lineno">  943</span>    p1 = vshlq_s32(p1, left_shift_vec);</div>
<div class="line"><span class="lineno">  944</span>    p2 = vshlq_s32(p2, left_shift_vec);</div>
<div class="line"><span class="lineno">  945</span>    p1 = vqrdmulhq_n_s32(p1, params.<a class="code hl_variable" href="structnnfw_1_1cker_1_1_binary_arithmetic_op_param.html#adae6f3cd4f5808bde90f4dfd3d915b4c">output_multiplier</a>);</div>
<div class="line"><span class="lineno">  946</span>    p2 = vqrdmulhq_n_s32(p2, params.<a class="code hl_variable" href="structnnfw_1_1cker_1_1_binary_arithmetic_op_param.html#adae6f3cd4f5808bde90f4dfd3d915b4c">output_multiplier</a>);</div>
<div class="line"><span class="lineno">  947</span>    <span class="keyword">using </span>gemmlowp::RoundingDivideByPOT;</div>
<div class="line"><span class="lineno">  948</span>    p1 = RoundingDivideByPOT(p1, right_shift);</div>
<div class="line"><span class="lineno">  949</span>    p2 = RoundingDivideByPOT(p2, right_shift);</div>
<div class="line"><span class="lineno">  950</span> </div>
<div class="line"><span class="lineno">  951</span>    <span class="keyword">const</span> <span class="keyword">auto</span> p1_narrowed = vqmovn_s32(p1);</div>
<div class="line"><span class="lineno">  952</span>    <span class="keyword">const</span> <span class="keyword">auto</span> p2_narrowed = vqmovn_s32(p2);</div>
<div class="line"><span class="lineno">  953</span>    <span class="keyword">const</span> <span class="keyword">auto</span> p = vaddq_s16(vcombine_s16(p1_narrowed, p2_narrowed), output_offset_vector);</div>
<div class="line"><span class="lineno">  954</span>    <span class="keyword">const</span> <span class="keyword">auto</span> clamped =</div>
<div class="line"><span class="lineno">  955</span>      vmax_u8(output_activation_min_vector, vmin_u8(output_activation_max_vector, vqmovun_s16(p)));</div>
<div class="line"><span class="lineno">  956</span>    vst1_u8(output_data + i, clamped);</div>
<div class="line"><span class="lineno">  957</span>  }</div>
<div class="line"><span class="lineno">  958</span><span class="preprocessor">#endif </span><span class="comment">// NEON</span></div>
<div class="line"><span class="lineno">  959</span> </div>
<div class="line"><span class="lineno">  960</span>  <span class="keywordflow">for</span> (; i &lt; size; ++i)</div>
<div class="line"><span class="lineno">  961</span>  {</div>
<div class="line"><span class="lineno">  962</span>    <span class="keyword">const</span> int32_t input1_val = params.<a class="code hl_variable" href="structnnfw_1_1cker_1_1_binary_arithmetic_op_param.html#a553e59d259a380f2da048553a23af0b0">input1_offset</a> + input1_data[i];</div>
<div class="line"><span class="lineno">  963</span>    <span class="keyword">const</span> int32_t input2_val = params.<a class="code hl_variable" href="structnnfw_1_1cker_1_1_binary_arithmetic_op_param.html#ac4d8a22f96208f1efd75047318475f03">input2_offset</a> + input2_data[i];</div>
<div class="line"><span class="lineno">  964</span>    <span class="keyword">const</span> int32_t unclamped_result =</div>
<div class="line"><span class="lineno">  965</span>      params.<a class="code hl_variable" href="structnnfw_1_1cker_1_1_binary_arithmetic_op_param.html#aa5bf6b7c6e3772bd04264f6729915729">output_offset</a> + <a class="code hl_function" href="namespacennfw_1_1cker.html#af6b33a8f0c1d60e578337bdb8dcb3673">MultiplyByQuantizedMultiplier</a>(input1_val * input2_val,</div>
<div class="line"><span class="lineno">  966</span>                                                           params.<a class="code hl_variable" href="structnnfw_1_1cker_1_1_binary_arithmetic_op_param.html#adae6f3cd4f5808bde90f4dfd3d915b4c">output_multiplier</a>,</div>
<div class="line"><span class="lineno">  967</span>                                                           params.<a class="code hl_variable" href="structnnfw_1_1cker_1_1_binary_arithmetic_op_param.html#a1c0adc1575a15790a35479c5fa5093a4">output_shift</a>);</div>
<div class="line"><span class="lineno">  968</span>    <span class="keyword">const</span> int32_t clamped_output = std::min(</div>
<div class="line"><span class="lineno">  969</span>      params.<a class="code hl_variable" href="structnnfw_1_1cker_1_1_binary_arithmetic_op_param.html#a20d919b6418bf302a97df426260bdec0">quantized_activation_max</a>, std::max(params.<a class="code hl_variable" href="structnnfw_1_1cker_1_1_binary_arithmetic_op_param.html#abb8ef3401fcae98626f1da50cda22b2f">quantized_activation_min</a>, unclamped_result));</div>
<div class="line"><span class="lineno">  970</span>    output_data[i] = <span class="keyword">static_cast&lt;</span>uint8_t<span class="keyword">&gt;</span>(clamped_output);</div>
<div class="line"><span class="lineno">  971</span>  }</div>
<div class="line"><span class="lineno">  972</span>}</div>
</div><!-- fragment -->
<p class="reference">References <a class="el" href="compute_2cker_2include_2cker_2_types_8h_source.html#l00177">nnfw::cker::BinaryArithmeticOpParam::input1_offset</a>, <a class="el" href="compute_2cker_2include_2cker_2_types_8h_source.html#l00178">nnfw::cker::BinaryArithmeticOpParam::input2_offset</a>, <a class="el" href="compute_2cker_2include_2cker_2_utils_8h_source.html#l00093">nnfw::cker::MultiplyByQuantizedMultiplier()</a>, <a class="el" href="compute_2cker_2include_2cker_2_types_8h_source.html#l00180">nnfw::cker::BinaryArithmeticOpParam::output_multiplier</a>, <a class="el" href="compute_2cker_2include_2cker_2_types_8h_source.html#l00179">nnfw::cker::BinaryArithmeticOpParam::output_offset</a>, <a class="el" href="compute_2cker_2include_2cker_2_types_8h_source.html#l00181">nnfw::cker::BinaryArithmeticOpParam::output_shift</a>, <a class="el" href="compute_2cker_2include_2cker_2_types_8h_source.html#l00190">nnfw::cker::BinaryArithmeticOpParam::quantized_activation_max</a>, and <a class="el" href="compute_2cker_2include_2cker_2_types_8h_source.html#l00189">nnfw::cker::BinaryArithmeticOpParam::quantized_activation_min</a>.</p>

<p class="reference">Referenced by <a class="el" href="optimized_2_binary_arithmetic_ops_8h_source.html#l01177">BroadcastMulDispatch()</a>, and <a class="el" href="optimized_2_binary_arithmetic_ops_8h_source.html#l01065">Mul()</a>.</p>

</div>
</div>
<a id="af1a0a78101d8a1152b8c8dafe5834292" name="af1a0a78101d8a1152b8c8dafe5834292"></a>
<h2 class="memtitle"><span class="permalink"><a href="#af1a0a78101d8a1152b8c8dafe5834292">&#9670;&#160;</a></span>MulSimpleBroadcast() <span class="overload">[1/2]</span></h2>

<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">void nnfw::cker::optimized::MulSimpleBroadcast </td>
          <td>(</td>
          <td class="paramtype">int&#160;</td>
          <td class="paramname"><em>size</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const <a class="el" href="structnnfw_1_1cker_1_1_binary_arithmetic_op_param.html">BinaryArithmeticOpParam</a> &amp;&#160;</td>
          <td class="paramname"><em>params</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const int8_t&#160;</td>
          <td class="paramname"><em>broadcast_value</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const int8_t *&#160;</td>
          <td class="paramname"><em>input2_data</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">int8_t *&#160;</td>
          <td class="paramname"><em>output_data</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">inline</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p class="definition">Definition at line <a class="el" href="optimized_2_binary_arithmetic_ops_8h_source.html#l01095">1095</a> of file <a class="el" href="optimized_2_binary_arithmetic_ops_8h_source.html">BinaryArithmeticOps.h</a>.</p>
<div class="fragment"><div class="line"><span class="lineno"> 1098</span>{</div>
<div class="line"><span class="lineno"> 1099</span>  <span class="keyword">const</span> int16_t input1_val = params.<a class="code hl_variable" href="structnnfw_1_1cker_1_1_binary_arithmetic_op_param.html#a553e59d259a380f2da048553a23af0b0">input1_offset</a> + broadcast_value;</div>
<div class="line"><span class="lineno"> 1100</span> </div>
<div class="line"><span class="lineno"> 1101</span>  <span class="keywordtype">int</span> i = 0;</div>
<div class="line"><span class="lineno"> 1102</span><span class="preprocessor">#ifdef USE_NEON</span></div>
<div class="line"><span class="lineno"> 1103</span>  <span class="keyword">const</span> <span class="keyword">auto</span> input2_offset_vector = vdupq_n_s16(params.<a class="code hl_variable" href="structnnfw_1_1cker_1_1_binary_arithmetic_op_param.html#ac4d8a22f96208f1efd75047318475f03">input2_offset</a>);</div>
<div class="line"><span class="lineno"> 1104</span>  <span class="keyword">const</span> <span class="keyword">auto</span> output_offset_vector = vdupq_n_s16(params.<a class="code hl_variable" href="structnnfw_1_1cker_1_1_binary_arithmetic_op_param.html#aa5bf6b7c6e3772bd04264f6729915729">output_offset</a>);</div>
<div class="line"><span class="lineno"> 1105</span>  <span class="keyword">const</span> <span class="keyword">auto</span> output_activation_min_vector = vdupq_n_s8(params.<a class="code hl_variable" href="structnnfw_1_1cker_1_1_binary_arithmetic_op_param.html#abb8ef3401fcae98626f1da50cda22b2f">quantized_activation_min</a>);</div>
<div class="line"><span class="lineno"> 1106</span>  <span class="keyword">const</span> <span class="keyword">auto</span> output_activation_max_vector = vdupq_n_s8(params.<a class="code hl_variable" href="structnnfw_1_1cker_1_1_binary_arithmetic_op_param.html#a20d919b6418bf302a97df426260bdec0">quantized_activation_max</a>);</div>
<div class="line"><span class="lineno"> 1107</span>  <span class="keyword">const</span> <span class="keywordtype">int</span> left_shift = std::max(0, params.<a class="code hl_variable" href="structnnfw_1_1cker_1_1_binary_arithmetic_op_param.html#a1c0adc1575a15790a35479c5fa5093a4">output_shift</a>);</div>
<div class="line"><span class="lineno"> 1108</span>  <span class="keyword">const</span> <span class="keywordtype">int</span> right_shift = std::max(0, -params.<a class="code hl_variable" href="structnnfw_1_1cker_1_1_binary_arithmetic_op_param.html#a1c0adc1575a15790a35479c5fa5093a4">output_shift</a>);</div>
<div class="line"><span class="lineno"> 1109</span>  <span class="keyword">const</span> int32x4_t left_shift_vec = vdupq_n_s32(left_shift);</div>
<div class="line"><span class="lineno"> 1110</span>  <span class="keywordflow">for</span> (; i &lt;= size - 16; i += 16)</div>
<div class="line"><span class="lineno"> 1111</span>  {</div>
<div class="line"><span class="lineno"> 1112</span>    <span class="comment">// We load / store 16 at a time, multiplying as four sets of 4 int32s.</span></div>
<div class="line"><span class="lineno"> 1113</span>    <span class="keyword">const</span> <span class="keyword">auto</span> input2_val_original = vld1q_s8(input2_data + i);</div>
<div class="line"><span class="lineno"> 1114</span>    <span class="keyword">const</span> <span class="keyword">auto</span> input2_val_s16_high = vmovl_s8(vget_high_s8(input2_val_original));</div>
<div class="line"><span class="lineno"> 1115</span>    <span class="keyword">const</span> <span class="keyword">auto</span> input2_val_s16_low = vmovl_s8(vget_low_s8(input2_val_original));</div>
<div class="line"><span class="lineno"> 1116</span> </div>
<div class="line"><span class="lineno"> 1117</span>    <span class="keyword">const</span> <span class="keyword">auto</span> input2_val_high = vaddq_s16(input2_val_s16_high, input2_offset_vector);</div>
<div class="line"><span class="lineno"> 1118</span>    <span class="keyword">const</span> <span class="keyword">auto</span> input2_val_low = vaddq_s16(input2_val_s16_low, input2_offset_vector);</div>
<div class="line"><span class="lineno"> 1119</span> </div>
<div class="line"><span class="lineno"> 1120</span>    <span class="keyword">const</span> <span class="keyword">auto</span> input2_val_low_low = vget_low_s16(input2_val_low);</div>
<div class="line"><span class="lineno"> 1121</span>    <span class="keyword">const</span> <span class="keyword">auto</span> input2_val_low_high = vget_high_s16(input2_val_low);</div>
<div class="line"><span class="lineno"> 1122</span>    <span class="keyword">const</span> <span class="keyword">auto</span> input2_val_high_low = vget_low_s16(input2_val_high);</div>
<div class="line"><span class="lineno"> 1123</span>    <span class="keyword">const</span> <span class="keyword">auto</span> input2_val_high_high = vget_high_s16(input2_val_high);</div>
<div class="line"><span class="lineno"> 1124</span> </div>
<div class="line"><span class="lineno"> 1125</span>    <span class="keyword">auto</span> p1 = vmull_n_s16(input2_val_high_high, input1_val);</div>
<div class="line"><span class="lineno"> 1126</span>    <span class="keyword">auto</span> p2 = vmull_n_s16(input2_val_high_low, input1_val);</div>
<div class="line"><span class="lineno"> 1127</span>    <span class="keyword">auto</span> p3 = vmull_n_s16(input2_val_low_high, input1_val);</div>
<div class="line"><span class="lineno"> 1128</span>    <span class="keyword">auto</span> p4 = vmull_n_s16(input2_val_low_low, input1_val);</div>
<div class="line"><span class="lineno"> 1129</span> </div>
<div class="line"><span class="lineno"> 1130</span>    p1 = vshlq_s32(p1, left_shift_vec);</div>
<div class="line"><span class="lineno"> 1131</span>    p2 = vshlq_s32(p2, left_shift_vec);</div>
<div class="line"><span class="lineno"> 1132</span>    p3 = vshlq_s32(p3, left_shift_vec);</div>
<div class="line"><span class="lineno"> 1133</span>    p4 = vshlq_s32(p4, left_shift_vec);</div>
<div class="line"><span class="lineno"> 1134</span> </div>
<div class="line"><span class="lineno"> 1135</span>    p1 = vqrdmulhq_n_s32(p1, params.<a class="code hl_variable" href="structnnfw_1_1cker_1_1_binary_arithmetic_op_param.html#adae6f3cd4f5808bde90f4dfd3d915b4c">output_multiplier</a>);</div>
<div class="line"><span class="lineno"> 1136</span>    p2 = vqrdmulhq_n_s32(p2, params.<a class="code hl_variable" href="structnnfw_1_1cker_1_1_binary_arithmetic_op_param.html#adae6f3cd4f5808bde90f4dfd3d915b4c">output_multiplier</a>);</div>
<div class="line"><span class="lineno"> 1137</span>    p3 = vqrdmulhq_n_s32(p3, params.<a class="code hl_variable" href="structnnfw_1_1cker_1_1_binary_arithmetic_op_param.html#adae6f3cd4f5808bde90f4dfd3d915b4c">output_multiplier</a>);</div>
<div class="line"><span class="lineno"> 1138</span>    p4 = vqrdmulhq_n_s32(p4, params.<a class="code hl_variable" href="structnnfw_1_1cker_1_1_binary_arithmetic_op_param.html#adae6f3cd4f5808bde90f4dfd3d915b4c">output_multiplier</a>);</div>
<div class="line"><span class="lineno"> 1139</span>    <span class="keyword">using </span>gemmlowp::RoundingDivideByPOT;</div>
<div class="line"><span class="lineno"> 1140</span>    p1 = RoundingDivideByPOT(p1, right_shift);</div>
<div class="line"><span class="lineno"> 1141</span>    p2 = RoundingDivideByPOT(p2, right_shift);</div>
<div class="line"><span class="lineno"> 1142</span>    p3 = RoundingDivideByPOT(p3, right_shift);</div>
<div class="line"><span class="lineno"> 1143</span>    p4 = RoundingDivideByPOT(p4, right_shift);</div>
<div class="line"><span class="lineno"> 1144</span> </div>
<div class="line"><span class="lineno"> 1145</span>    <span class="keyword">const</span> <span class="keyword">auto</span> p1_narrowed = vqmovn_s32(p1);</div>
<div class="line"><span class="lineno"> 1146</span>    <span class="keyword">const</span> <span class="keyword">auto</span> p2_narrowed = vqmovn_s32(p2);</div>
<div class="line"><span class="lineno"> 1147</span>    <span class="keyword">const</span> <span class="keyword">auto</span> p3_narrowed = vqmovn_s32(p3);</div>
<div class="line"><span class="lineno"> 1148</span>    <span class="keyword">const</span> <span class="keyword">auto</span> p4_narrowed = vqmovn_s32(p4);</div>
<div class="line"><span class="lineno"> 1149</span> </div>
<div class="line"><span class="lineno"> 1150</span>    <span class="keyword">const</span> int16x8_t p_part1 =</div>
<div class="line"><span class="lineno"> 1151</span>      vaddq_s16(vcombine_s16(p2_narrowed, p1_narrowed), output_offset_vector);</div>
<div class="line"><span class="lineno"> 1152</span>    <span class="keyword">const</span> int16x8_t p_part2 =</div>
<div class="line"><span class="lineno"> 1153</span>      vaddq_s16(vcombine_s16(p4_narrowed, p3_narrowed), output_offset_vector);</div>
<div class="line"><span class="lineno"> 1154</span>    <span class="keyword">const</span> int8x16_t p = vcombine_s8(vqmovn_s16(p_part2), vqmovn_s16(p_part1));</div>
<div class="line"><span class="lineno"> 1155</span> </div>
<div class="line"><span class="lineno"> 1156</span>    <span class="keyword">const</span> <span class="keyword">auto</span> clamped =</div>
<div class="line"><span class="lineno"> 1157</span>      vmaxq_s8(output_activation_min_vector, vminq_s8(output_activation_max_vector, p));</div>
<div class="line"><span class="lineno"> 1158</span>    vst1q_s8(output_data + i, clamped);</div>
<div class="line"><span class="lineno"> 1159</span>  }</div>
<div class="line"><span class="lineno"> 1160</span><span class="preprocessor">#endif </span><span class="comment">// NEON</span></div>
<div class="line"><span class="lineno"> 1161</span> </div>
<div class="line"><span class="lineno"> 1162</span>  <span class="keywordflow">for</span> (; i &lt; size; ++i)</div>
<div class="line"><span class="lineno"> 1163</span>  {</div>
<div class="line"><span class="lineno"> 1164</span>    <span class="keyword">const</span> int32_t input2_val = params.<a class="code hl_variable" href="structnnfw_1_1cker_1_1_binary_arithmetic_op_param.html#ac4d8a22f96208f1efd75047318475f03">input2_offset</a> + input2_data[i];</div>
<div class="line"><span class="lineno"> 1165</span>    <span class="keyword">const</span> int32_t unclamped_result =</div>
<div class="line"><span class="lineno"> 1166</span>      params.<a class="code hl_variable" href="structnnfw_1_1cker_1_1_binary_arithmetic_op_param.html#aa5bf6b7c6e3772bd04264f6729915729">output_offset</a> + <a class="code hl_function" href="namespacennfw_1_1cker.html#af6b33a8f0c1d60e578337bdb8dcb3673">MultiplyByQuantizedMultiplier</a>(input1_val * input2_val,</div>
<div class="line"><span class="lineno"> 1167</span>                                                           params.<a class="code hl_variable" href="structnnfw_1_1cker_1_1_binary_arithmetic_op_param.html#adae6f3cd4f5808bde90f4dfd3d915b4c">output_multiplier</a>,</div>
<div class="line"><span class="lineno"> 1168</span>                                                           params.<a class="code hl_variable" href="structnnfw_1_1cker_1_1_binary_arithmetic_op_param.html#a1c0adc1575a15790a35479c5fa5093a4">output_shift</a>);</div>
<div class="line"><span class="lineno"> 1169</span>    <span class="keyword">const</span> int32_t clamped_output = std::min(</div>
<div class="line"><span class="lineno"> 1170</span>      params.<a class="code hl_variable" href="structnnfw_1_1cker_1_1_binary_arithmetic_op_param.html#a20d919b6418bf302a97df426260bdec0">quantized_activation_max</a>, std::max(params.<a class="code hl_variable" href="structnnfw_1_1cker_1_1_binary_arithmetic_op_param.html#abb8ef3401fcae98626f1da50cda22b2f">quantized_activation_min</a>, unclamped_result));</div>
<div class="line"><span class="lineno"> 1171</span>    output_data[i] = <span class="keyword">static_cast&lt;</span>int8_t<span class="keyword">&gt;</span>(clamped_output);</div>
<div class="line"><span class="lineno"> 1172</span>  }</div>
<div class="line"><span class="lineno"> 1173</span>}</div>
</div><!-- fragment -->
<p class="reference">References <a class="el" href="compute_2cker_2include_2cker_2_types_8h_source.html#l00177">nnfw::cker::BinaryArithmeticOpParam::input1_offset</a>, <a class="el" href="compute_2cker_2include_2cker_2_types_8h_source.html#l00178">nnfw::cker::BinaryArithmeticOpParam::input2_offset</a>, <a class="el" href="compute_2cker_2include_2cker_2_utils_8h_source.html#l00093">nnfw::cker::MultiplyByQuantizedMultiplier()</a>, <a class="el" href="compute_2cker_2include_2cker_2_types_8h_source.html#l00180">nnfw::cker::BinaryArithmeticOpParam::output_multiplier</a>, <a class="el" href="compute_2cker_2include_2cker_2_types_8h_source.html#l00179">nnfw::cker::BinaryArithmeticOpParam::output_offset</a>, <a class="el" href="compute_2cker_2include_2cker_2_types_8h_source.html#l00181">nnfw::cker::BinaryArithmeticOpParam::output_shift</a>, <a class="el" href="compute_2cker_2include_2cker_2_types_8h_source.html#l00190">nnfw::cker::BinaryArithmeticOpParam::quantized_activation_max</a>, and <a class="el" href="compute_2cker_2include_2cker_2_types_8h_source.html#l00189">nnfw::cker::BinaryArithmeticOpParam::quantized_activation_min</a>.</p>

</div>
</div>
<a id="a12c1a56c7e65d8f294610e075c9ccc7c" name="a12c1a56c7e65d8f294610e075c9ccc7c"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a12c1a56c7e65d8f294610e075c9ccc7c">&#9670;&#160;</a></span>MulSimpleBroadcast() <span class="overload">[2/2]</span></h2>

<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">void nnfw::cker::optimized::MulSimpleBroadcast </td>
          <td>(</td>
          <td class="paramtype">int&#160;</td>
          <td class="paramname"><em>size</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const <a class="el" href="structnnfw_1_1cker_1_1_binary_arithmetic_op_param.html">BinaryArithmeticOpParam</a> &amp;&#160;</td>
          <td class="paramname"><em>params</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const uint8_t&#160;</td>
          <td class="paramname"><em>broadcast_value</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const uint8_t *&#160;</td>
          <td class="paramname"><em>input2_data</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">uint8_t *&#160;</td>
          <td class="paramname"><em>output_data</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">inline</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p class="definition">Definition at line <a class="el" href="optimized_2_binary_arithmetic_ops_8h_source.html#l01081">1081</a> of file <a class="el" href="optimized_2_binary_arithmetic_ops_8h_source.html">BinaryArithmeticOps.h</a>.</p>
<div class="fragment"><div class="line"><span class="lineno"> 1084</span>{</div>
<div class="line"><span class="lineno"> 1085</span>  <span class="keywordtype">int</span> i = 0;</div>
<div class="line"><span class="lineno"> 1086</span>  int32_t clamped_output;</div>
<div class="line"><span class="lineno"> 1087</span>  <span class="keywordflow">for</span> (; i &lt; size; ++i)</div>
<div class="line"><span class="lineno"> 1088</span>  {</div>
<div class="line"><span class="lineno"> 1089</span>    clamped_output = <a class="code hl_function" href="namespacennfw_1_1cker_1_1optimized.html#a19f1d857236b4b11c737cb4fe7cc7567">quant8_mul</a>(params, broadcast_value, input2_data[i]);</div>
<div class="line"><span class="lineno"> 1090</span>    output_data[i] = <span class="keyword">static_cast&lt;</span>uint8_t<span class="keyword">&gt;</span>(clamped_output);</div>
<div class="line"><span class="lineno"> 1091</span>  }</div>
<div class="line"><span class="lineno"> 1092</span>}</div>
</div><!-- fragment -->
<p class="reference">References <a class="el" href="optimized_2_binary_arithmetic_ops_8h_source.html#l00896">quant8_mul()</a>.</p>

<p class="reference">Referenced by <a class="el" href="optimized_2_binary_arithmetic_ops_8h_source.html#l01177">BroadcastMulDispatch()</a>.</p>

</div>
</div>
<a id="a19f1d857236b4b11c737cb4fe7cc7567" name="a19f1d857236b4b11c737cb4fe7cc7567"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a19f1d857236b4b11c737cb4fe7cc7567">&#9670;&#160;</a></span>quant8_mul()</h2>

<div class="memitem">
<div class="memproto">
<div class="memtemplate">
template&lt;typename T &gt; </div>
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">std::enable_if_t&lt; <a class="el" href="structnnfw_1_1cker_1_1is__quant8.html">is_quant8</a>&lt; T &gt;::value, int32_t &gt; nnfw::cker::optimized::quant8_mul </td>
          <td>(</td>
          <td class="paramtype">const <a class="el" href="structnnfw_1_1cker_1_1_binary_arithmetic_op_param.html">BinaryArithmeticOpParam</a> &amp;&#160;</td>
          <td class="paramname"><em>params</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const T&#160;</td>
          <td class="paramname"><em>input1_data</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const T&#160;</td>
          <td class="paramname"><em>input2_data</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">inline</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p class="definition">Definition at line <a class="el" href="optimized_2_binary_arithmetic_ops_8h_source.html#l00896">896</a> of file <a class="el" href="optimized_2_binary_arithmetic_ops_8h_source.html">BinaryArithmeticOps.h</a>.</p>
<div class="fragment"><div class="line"><span class="lineno">  897</span>{</div>
<div class="line"><span class="lineno">  898</span>  <span class="keyword">const</span> int32_t input1_val = params.<a class="code hl_variable" href="structnnfw_1_1cker_1_1_binary_arithmetic_op_param.html#a553e59d259a380f2da048553a23af0b0">input1_offset</a> + input1_data;</div>
<div class="line"><span class="lineno">  899</span>  <span class="keyword">const</span> int32_t input2_val = params.<a class="code hl_variable" href="structnnfw_1_1cker_1_1_binary_arithmetic_op_param.html#ac4d8a22f96208f1efd75047318475f03">input2_offset</a> + input2_data;</div>
<div class="line"><span class="lineno">  900</span>  <span class="keyword">const</span> int32_t unclamped_result =</div>
<div class="line"><span class="lineno">  901</span>    params.<a class="code hl_variable" href="structnnfw_1_1cker_1_1_binary_arithmetic_op_param.html#aa5bf6b7c6e3772bd04264f6729915729">output_offset</a> + <a class="code hl_function" href="namespacennfw_1_1cker.html#af6b33a8f0c1d60e578337bdb8dcb3673">MultiplyByQuantizedMultiplier</a>(input1_val * input2_val,</div>
<div class="line"><span class="lineno">  902</span>                                                         params.<a class="code hl_variable" href="structnnfw_1_1cker_1_1_binary_arithmetic_op_param.html#adae6f3cd4f5808bde90f4dfd3d915b4c">output_multiplier</a>,</div>
<div class="line"><span class="lineno">  903</span>                                                         params.<a class="code hl_variable" href="structnnfw_1_1cker_1_1_binary_arithmetic_op_param.html#a1c0adc1575a15790a35479c5fa5093a4">output_shift</a>);</div>
<div class="line"><span class="lineno">  904</span>  <span class="keyword">const</span> int32_t clamped_output = std::min(</div>
<div class="line"><span class="lineno">  905</span>    params.<a class="code hl_variable" href="structnnfw_1_1cker_1_1_binary_arithmetic_op_param.html#a20d919b6418bf302a97df426260bdec0">quantized_activation_max</a>, std::max(params.<a class="code hl_variable" href="structnnfw_1_1cker_1_1_binary_arithmetic_op_param.html#abb8ef3401fcae98626f1da50cda22b2f">quantized_activation_min</a>, unclamped_result));</div>
<div class="line"><span class="lineno">  906</span> </div>
<div class="line"><span class="lineno">  907</span>  <span class="keywordflow">return</span> clamped_output;</div>
<div class="line"><span class="lineno">  908</span>}</div>
</div><!-- fragment -->
<p class="reference">References <a class="el" href="compute_2cker_2include_2cker_2_types_8h_source.html#l00177">nnfw::cker::BinaryArithmeticOpParam::input1_offset</a>, <a class="el" href="compute_2cker_2include_2cker_2_types_8h_source.html#l00178">nnfw::cker::BinaryArithmeticOpParam::input2_offset</a>, <a class="el" href="compute_2cker_2include_2cker_2_utils_8h_source.html#l00093">nnfw::cker::MultiplyByQuantizedMultiplier()</a>, <a class="el" href="compute_2cker_2include_2cker_2_types_8h_source.html#l00180">nnfw::cker::BinaryArithmeticOpParam::output_multiplier</a>, <a class="el" href="compute_2cker_2include_2cker_2_types_8h_source.html#l00179">nnfw::cker::BinaryArithmeticOpParam::output_offset</a>, <a class="el" href="compute_2cker_2include_2cker_2_types_8h_source.html#l00181">nnfw::cker::BinaryArithmeticOpParam::output_shift</a>, <a class="el" href="compute_2cker_2include_2cker_2_types_8h_source.html#l00190">nnfw::cker::BinaryArithmeticOpParam::quantized_activation_max</a>, and <a class="el" href="compute_2cker_2include_2cker_2_types_8h_source.html#l00189">nnfw::cker::BinaryArithmeticOpParam::quantized_activation_min</a>.</p>

<p class="reference">Referenced by <a class="el" href="optimized_2_binary_arithmetic_ops_8h_source.html#l01177">BroadcastMulDispatch()</a>, and <a class="el" href="optimized_2_binary_arithmetic_ops_8h_source.html#l01081">MulSimpleBroadcast()</a>.</p>

</div>
</div>
<a id="a1b089b3d4d74b15a41c88996227a23e3" name="a1b089b3d4d74b15a41c88996227a23e3"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a1b089b3d4d74b15a41c88996227a23e3">&#9670;&#160;</a></span>quant8_sum()</h2>

<div class="memitem">
<div class="memproto">
<div class="memtemplate">
template&lt;typename T &gt; </div>
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">std::enable_if_t&lt; <a class="el" href="structnnfw_1_1cker_1_1is__quant8.html">is_quant8</a>&lt; T &gt;::value, int32_t &gt; nnfw::cker::optimized::quant8_sum </td>
          <td>(</td>
          <td class="paramtype">const <a class="el" href="structnnfw_1_1cker_1_1_binary_arithmetic_op_param.html">BinaryArithmeticOpParam</a> &amp;&#160;</td>
          <td class="paramname"><em>params</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const T&#160;</td>
          <td class="paramname"><em>input1_data</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const T&#160;</td>
          <td class="paramname"><em>input2_data</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">inline</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p class="definition">Definition at line <a class="el" href="optimized_2_binary_arithmetic_ops_8h_source.html#l00227">227</a> of file <a class="el" href="optimized_2_binary_arithmetic_ops_8h_source.html">BinaryArithmeticOps.h</a>.</p>
<div class="fragment"><div class="line"><span class="lineno">  228</span>{</div>
<div class="line"><span class="lineno">  229</span>  <span class="keyword">const</span> int32_t input1_val = params.<a class="code hl_variable" href="structnnfw_1_1cker_1_1_binary_arithmetic_op_param.html#a553e59d259a380f2da048553a23af0b0">input1_offset</a> + input1_data;</div>
<div class="line"><span class="lineno">  230</span>  <span class="keyword">const</span> int32_t input2_val = params.<a class="code hl_variable" href="structnnfw_1_1cker_1_1_binary_arithmetic_op_param.html#ac4d8a22f96208f1efd75047318475f03">input2_offset</a> + input2_data;</div>
<div class="line"><span class="lineno">  231</span>  <span class="keyword">const</span> int32_t shifted_input1_val = input1_val * (1 &lt;&lt; params.<a class="code hl_variable" href="structnnfw_1_1cker_1_1_binary_arithmetic_op_param.html#ae5b66510e715c8fb127887931de98dd9">left_shift</a>);</div>
<div class="line"><span class="lineno">  232</span>  <span class="keyword">const</span> int32_t shifted_input2_val = input2_val * (1 &lt;&lt; params.<a class="code hl_variable" href="structnnfw_1_1cker_1_1_binary_arithmetic_op_param.html#ae5b66510e715c8fb127887931de98dd9">left_shift</a>);</div>
<div class="line"><span class="lineno">  233</span>  <span class="keyword">const</span> int32_t scaled_input1_val = <a class="code hl_function" href="namespacennfw_1_1cker.html#afe95d2a91cb367a4b9be585d4ff4b90f">MultiplyByQuantizedMultiplierSmallerThanOneExp</a>(</div>
<div class="line"><span class="lineno">  234</span>    shifted_input1_val, params.<a class="code hl_variable" href="structnnfw_1_1cker_1_1_binary_arithmetic_op_param.html#ae26b1a5367ed8036f623f4116d22b703">input1_multiplier</a>, params.<a class="code hl_variable" href="structnnfw_1_1cker_1_1_binary_arithmetic_op_param.html#a473732b2b25fa319a787fd22411f83b8">input1_shift</a>);</div>
<div class="line"><span class="lineno">  235</span>  <span class="keyword">const</span> int32_t scaled_input2_val = <a class="code hl_function" href="namespacennfw_1_1cker.html#afe95d2a91cb367a4b9be585d4ff4b90f">MultiplyByQuantizedMultiplierSmallerThanOneExp</a>(</div>
<div class="line"><span class="lineno">  236</span>    shifted_input2_val, params.<a class="code hl_variable" href="structnnfw_1_1cker_1_1_binary_arithmetic_op_param.html#ab6b610a4f597712f6dcaf6576c04dc66">input2_multiplier</a>, params.<a class="code hl_variable" href="structnnfw_1_1cker_1_1_binary_arithmetic_op_param.html#aaf1857ba76a406a592402a13607421a5">input2_shift</a>);</div>
<div class="line"><span class="lineno">  237</span>  <span class="keyword">const</span> int32_t raw_sum = scaled_input1_val + scaled_input2_val;</div>
<div class="line"><span class="lineno">  238</span>  <span class="keyword">const</span> int32_t raw_output = <a class="code hl_function" href="namespacennfw_1_1cker.html#afe95d2a91cb367a4b9be585d4ff4b90f">MultiplyByQuantizedMultiplierSmallerThanOneExp</a>(</div>
<div class="line"><span class="lineno">  239</span>                               raw_sum, params.<a class="code hl_variable" href="structnnfw_1_1cker_1_1_binary_arithmetic_op_param.html#adae6f3cd4f5808bde90f4dfd3d915b4c">output_multiplier</a>, params.<a class="code hl_variable" href="structnnfw_1_1cker_1_1_binary_arithmetic_op_param.html#a1c0adc1575a15790a35479c5fa5093a4">output_shift</a>) +</div>
<div class="line"><span class="lineno">  240</span>                             params.<a class="code hl_variable" href="structnnfw_1_1cker_1_1_binary_arithmetic_op_param.html#aa5bf6b7c6e3772bd04264f6729915729">output_offset</a>;</div>
<div class="line"><span class="lineno">  241</span>  <span class="keyword">const</span> int32_t clamped_output = std::min(params.<a class="code hl_variable" href="structnnfw_1_1cker_1_1_binary_arithmetic_op_param.html#a20d919b6418bf302a97df426260bdec0">quantized_activation_max</a>,</div>
<div class="line"><span class="lineno">  242</span>                                          std::max(params.<a class="code hl_variable" href="structnnfw_1_1cker_1_1_binary_arithmetic_op_param.html#abb8ef3401fcae98626f1da50cda22b2f">quantized_activation_min</a>, raw_output));</div>
<div class="line"><span class="lineno">  243</span>  <span class="keywordflow">return</span> clamped_output;</div>
<div class="line"><span class="lineno">  244</span>}</div>
</div><!-- fragment -->
<p class="reference">References <a class="el" href="compute_2cker_2include_2cker_2_types_8h_source.html#l00184">nnfw::cker::BinaryArithmeticOpParam::input1_multiplier</a>, <a class="el" href="compute_2cker_2include_2cker_2_types_8h_source.html#l00177">nnfw::cker::BinaryArithmeticOpParam::input1_offset</a>, <a class="el" href="compute_2cker_2include_2cker_2_types_8h_source.html#l00185">nnfw::cker::BinaryArithmeticOpParam::input1_shift</a>, <a class="el" href="compute_2cker_2include_2cker_2_types_8h_source.html#l00186">nnfw::cker::BinaryArithmeticOpParam::input2_multiplier</a>, <a class="el" href="compute_2cker_2include_2cker_2_types_8h_source.html#l00178">nnfw::cker::BinaryArithmeticOpParam::input2_offset</a>, <a class="el" href="compute_2cker_2include_2cker_2_types_8h_source.html#l00187">nnfw::cker::BinaryArithmeticOpParam::input2_shift</a>, <a class="el" href="compute_2cker_2include_2cker_2_types_8h_source.html#l00183">nnfw::cker::BinaryArithmeticOpParam::left_shift</a>, <a class="el" href="compute_2cker_2include_2cker_2_utils_8h_source.html#l00108">nnfw::cker::MultiplyByQuantizedMultiplierSmallerThanOneExp()</a>, <a class="el" href="compute_2cker_2include_2cker_2_types_8h_source.html#l00180">nnfw::cker::BinaryArithmeticOpParam::output_multiplier</a>, <a class="el" href="compute_2cker_2include_2cker_2_types_8h_source.html#l00179">nnfw::cker::BinaryArithmeticOpParam::output_offset</a>, <a class="el" href="compute_2cker_2include_2cker_2_types_8h_source.html#l00181">nnfw::cker::BinaryArithmeticOpParam::output_shift</a>, <a class="el" href="compute_2cker_2include_2cker_2_types_8h_source.html#l00190">nnfw::cker::BinaryArithmeticOpParam::quantized_activation_max</a>, and <a class="el" href="compute_2cker_2include_2cker_2_types_8h_source.html#l00189">nnfw::cker::BinaryArithmeticOpParam::quantized_activation_min</a>.</p>

<p class="reference">Referenced by <a class="el" href="optimized_2_binary_arithmetic_ops_8h_source.html#l00711">AddScalarBroadcast()</a>, and <a class="el" href="optimized_2_binary_arithmetic_ops_8h_source.html#l00812">BroadcastAddDispatch()</a>.</p>

</div>
</div>
<a id="a2fe73b064c2531585c43abd6e9ee929d" name="a2fe73b064c2531585c43abd6e9ee929d"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a2fe73b064c2531585c43abd6e9ee929d">&#9670;&#160;</a></span>Sub()</h2>

<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">void nnfw::cker::optimized::Sub </td>
          <td>(</td>
          <td class="paramtype">const <a class="el" href="structnnfw_1_1cker_1_1_binary_arithmetic_op_param.html">BinaryArithmeticOpParam</a> &amp;&#160;</td>
          <td class="paramname"><em>params</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const <a class="el" href="classnnfw_1_1cker_1_1_shape.html">Shape</a> &amp;&#160;</td>
          <td class="paramname"><em>input1_shape</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const float *&#160;</td>
          <td class="paramname"><em>input1_data</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const <a class="el" href="classnnfw_1_1cker_1_1_shape.html">Shape</a> &amp;&#160;</td>
          <td class="paramname"><em>input2_shape</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const float *&#160;</td>
          <td class="paramname"><em>input2_data</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const <a class="el" href="classnnfw_1_1cker_1_1_shape.html">Shape</a> &amp;&#160;</td>
          <td class="paramname"><em>output_shape</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">float *&#160;</td>
          <td class="paramname"><em>output_data</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">inline</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p class="definition">Definition at line <a class="el" href="optimized_2_binary_arithmetic_ops_8h_source.html#l00858">858</a> of file <a class="el" href="optimized_2_binary_arithmetic_ops_8h_source.html">BinaryArithmeticOps.h</a>.</p>
<div class="fragment"><div class="line"><span class="lineno">  861</span>{</div>
<div class="line"><span class="lineno">  862</span>  <span class="keyword">const</span> <span class="keywordtype">int</span> flat_size = <a class="code hl_function" href="namespacennfw_1_1cker.html#ab86126de4e835f9499051e43b841abca">MatchingElementsSize</a>(input1_shape, input2_shape, output_shape);</div>
<div class="line"><span class="lineno">  863</span>  <span class="keyword">auto</span> implFuncs = getBinaryOpWithActivationImplFloat&lt;BinaryOpFuncSubFloat&gt;(params);</div>
<div class="line"><span class="lineno">  864</span>  (*implFuncs.first)(flat_size, params, input1_data, input2_data, output_data);</div>
<div class="line"><span class="lineno">  865</span>}</div>
</div><!-- fragment -->
<p class="reference">References <a class="el" href="compute_2cker_2include_2cker_2_shape_8h_source.html#l00337">nnfw::cker::MatchingElementsSize()</a>.</p>

<p class="reference">Referenced by <a class="el" href="_binary_arithmetic_ops_8h_source.html#l00228">nnfw::cker::BinaryArithmeticOp()</a>.</p>

</div>
</div>
<h2 class="groupheader">Variable Documentation</h2>
<a id="a4bac55e13be194606f4691fd24dae393" name="a4bac55e13be194606f4691fd24dae393"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a4bac55e13be194606f4691fd24dae393">&#9670;&#160;</a></span>_gemmlowp_mutex</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">std::mutex nnfw::cker::optimized::_gemmlowp_mutex</td>
        </tr>
      </table>
</div><div class="memdoc">

<p class="definition">Definition at line <a class="el" href="compute_2cker_2include_2cker_2operation_2optimized_2_conv_8h_source.html#l00045">45</a> of file <a class="el" href="compute_2cker_2include_2cker_2operation_2optimized_2_conv_8h_source.html">Conv.h</a>.</p>

<p class="reference">Referenced by <a class="el" href="compute_2cker_2include_2cker_2operation_2optimized_2_conv_8h_source.html#l00083">Conv()</a>.</p>

</div>
</div>
</div><!-- contents -->
</div><!-- doc-content -->
<!-- start footer part -->
<div id="nav-path" class="navpath"><!-- id is needed for treeview function! -->
  <ul>
    <li class="navelem"><a class="el" href="namespacennfw.html">nnfw</a></li><li class="navelem"><a class="el" href="namespacennfw_1_1cker.html">cker</a></li><li class="navelem"><a class="el" href="namespacennfw_1_1cker_1_1optimized.html">optimized</a></li>
    <li class="footer">Generated by <a href="https://www.doxygen.org/index.html"><img class="footer" src="doxygen.svg" width="104" height="31" alt="doxygen"/></a> 1.9.5 </li>
  </ul>
</div>
</body>
</html>
